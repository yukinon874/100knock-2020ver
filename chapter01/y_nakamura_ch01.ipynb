{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00.文字列の逆順"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desserts\n"
     ]
    }
   ],
   "source": [
    "text = \"stressed\"\n",
    "reversed_text = text[::-1]\n",
    "print(reversed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01.「パタトクカシーー」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パトカー\n"
     ]
    }
   ],
   "source": [
    "text = \"パタトクカシーー\"\n",
    "sliced_text = text[::2]\n",
    "print(sliced_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02.「パトカー」＋「タクシー」＝「パタトクカシーー」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パタトクカシーー\n",
      "パタトクカシーー\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "text_a = \"パトカー\"\n",
    "text_b = \"タクシー\"\n",
    "mearged_text = ''.join(char_a + char_b for (char_a, char_b) in zip(text_a, text_b))\n",
    "print(mearged_text)\n",
    "mearged_text = reduce(lambda word, char_ab: word + ''.join(char_ab), zip(text_a, text_b), '')\n",
    "print(mearged_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03.円周率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = 'Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.'\n",
    "word_size_list = [len(word) for word in re.split('[,. ]', text) if word != '']\n",
    "print(word_size_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04.元素記号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H': 1, 'He': 2, 'Li': 3, 'Be': 4, 'B': 5, 'C': 6, 'N': 7, 'O': 8, 'F': 9, 'Ne': 10, 'Na': 11, 'Mi': 12, 'Al': 13, 'Si': 14, 'P': 15, 'S': 16, 'Cl': 17, 'Ar': 18, 'K': 19, 'Ca': 20}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = 'Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.'\n",
    "num_list = {1, 5, 6, 7, 8, 9, 15, 16, 19}\n",
    "word_index = {word[:1] if i + 1 in num_list else word[:2]:i + 1 for (i, word) in enumerate([word for word in re.split('[. ]', text) if word != ''])}\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05.n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I ', ' a', 'am', 'm ', ' a', 'an', 'n ', ' N', 'NL', 'LP', 'Pe', 'er']\n",
      "[['I', 'am'], ['am', 'an'], ['an', 'NLPer']]\n"
     ]
    }
   ],
   "source": [
    "def n_gram(text, n):\n",
    "    return [text[i:i + n] for i in range(len(text) - n + 1)]\n",
    "\n",
    "text = 'I am an NLPer'\n",
    "print(n_gram(text, 2))\n",
    "print(n_gram(text.split(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06.集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ar', 'di', 'ag', 'gr', 'ph', 'ad', 'ap', 'is', 'se', 'ra', 'pa'}\n",
      "{'ra', 'ar', 'pa', 'ap'}\n",
      "{'is', 'di', 'se', 'ad'}\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "text_x = 'paraparaparadise'\n",
    "text_y = 'paragraph'\n",
    "x_n_gram_set = set(n_gram(text_x, 2))\n",
    "y_n_gram_set = set(n_gram(text_y, 2))\n",
    "print(x_n_gram_set | y_n_gram_set)\n",
    "print(x_n_gram_set & y_n_gram_set)\n",
    "print(x_n_gram_set - y_n_gram_set)\n",
    "print('se' in x_n_gram_set)\n",
    "print('se' in y_n_gram_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07.テンプレートによる文生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12時の気温は22.4\n"
     ]
    }
   ],
   "source": [
    "def make_text1(text_x, text_y, text_z):\n",
    "    return str(text_x) + '時の' + str(text_y) + 'は' + str(text_z)\n",
    "\n",
    "print(make_text1(12, '気温', 22.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12時の気温は22.4\n"
     ]
    }
   ],
   "source": [
    "def make_text2(x, y, z):\n",
    "    return f'{x}時の{y}は{z}'\n",
    "\n",
    "print(make_text2(12, '気温', 22.4)) #f文字列python3.6以降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08.暗号文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzBy-0:!\n",
      "AaBb-0:!\n"
     ]
    }
   ],
   "source": [
    "def chipher(text):\n",
    "    return ''.join(map(lambda x:chr(219 - ord(x)) if x.islower() else x, text))\n",
    "\n",
    "text = \"AaBb-0:!\"\n",
    "encode_text = chipher(text)\n",
    "print(encode_text)\n",
    "decode_text = chipher(encode_text)\n",
    "print(decode_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09.Typoglycemia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .\n",
      "I culnd’ot bevelie that I cluod aatcully unastedrnd what I was rdieang : the paehmonenl pweor of the hmuan mind .\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "from functools import reduce\n",
    "\n",
    "def shuffle_word(word):\n",
    "    return word[0] + ''.join(sample(word[1:-1], len(word) - 2)) + word[-1]\n",
    "\n",
    "def typoglycemia(text):\n",
    "    return reduce(lambda words, word: words + ' ' + (shuffle_word(word) if len(word) > 4 else word), text.split())\n",
    "\n",
    "text = 'I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .'\n",
    "print(text)\n",
    "print(typoglycemia(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
