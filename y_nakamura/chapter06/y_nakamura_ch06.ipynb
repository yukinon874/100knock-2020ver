{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50. データの入手・整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "出来ないが？\n",
    "'''\n",
    "col_names = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']\n",
    "df = pd.read_csv(\"./data/NewsAggregatorDataset/newsCorpora.csv\", encoding = 'UTF-8', names = col_names, sep = '\\t')\n",
    "extracted_data_frame = df[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail'])][['TITLE', 'CATEGORY']]\n",
    "row_size, _ = extracted_data_frame.shape\n",
    "data_list = [{} for _ in range(row_size)]\n",
    "\n",
    "for data_name in extracted_data_frame:\n",
    "    for idx, frame in enumerate(extracted_data_frame[data_name]):\n",
    "        data_list[idx][data_name] = frame\n",
    "\n",
    "\n",
    "shuffle(data_list)\n",
    "\n",
    "train_size = row_size // 10 * 8\n",
    "valid_size = row_size // 10\n",
    "test_size = row_size // 10\n",
    "\n",
    "path = './work/'\n",
    "file_names = ['train.txt', 'valid.txt', 'test.txt']\n",
    "data_names = ['train_data', 'valid_data', 'test_data']\n",
    "\n",
    "file_data = dict()\n",
    "file_data['train_data'] = data_list[:train_size]\n",
    "file_data['valid_data'] = data_list[train_size:train_size + valid_size]\n",
    "file_data['test_data'] = data_list[-test_size:]\n",
    "\n",
    "\n",
    "for idx, file_name in enumerate(file_names):\n",
    "    with open(path + file_name, mode = 'w') as f:\n",
    "        for data in file_data[data_names[idx]]:\n",
    "            f.write('\\t'.join([data['TITLE'], data['CATEGORY']]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = list()\n",
    "with open('./data/NewsAggregatorDataset/newsCorpora.csv', mode = 'r') as f:\n",
    "    for line in f:\n",
    "        splited_line = line.split('\\t')\n",
    "        if splited_line[3] in ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']:\n",
    "            data_list.append([splited_line[1], splited_line[4]])\n",
    "\n",
    "row_size = len(data_list)\n",
    "shuffle(data_list)\n",
    "\n",
    "train_size = row_size // 10 * 8\n",
    "valid_size = row_size // 10\n",
    "test_size = row_size // 10\n",
    "\n",
    "path = './work/'\n",
    "file_names = ['train.txt', 'valid.txt', 'test.txt']\n",
    "data_names = ['train_data', 'valid_data', 'test_data']\n",
    "\n",
    "file_data = dict()\n",
    "train_data = data_list[:train_size]\n",
    "valid_data = data_list[train_size:train_size + valid_size]\n",
    "test_data = data_list[-test_size:]\n",
    "\n",
    "file_data['train_data'] = train_data\n",
    "file_data['valid_data'] = valid_data \n",
    "file_data['test_data'] = test_data\n",
    "\n",
    "for idx, file_name in enumerate(file_names):\n",
    "    with open(path + file_name, mode = 'w') as f:\n",
    "        for data in file_data[data_names[idx]]:\n",
    "            f.write('\\t'.join([data[0], data[1]]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1335  15216  89224 ./work/test.txt\n",
      " 10680 122450 713624 ./work/train.txt\n",
      "  1335  15247  89063 ./work/valid.txt\n",
      " 13350 152913 891911 total\n"
     ]
    }
   ],
   "source": [
    "!wc ./work/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 51. 特徴量抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 : ビジネス  \n",
    "1 : 科学技術  \n",
    "2 : エンターテイメント  \n",
    "3 : 健康  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file_npy(dir_name, file_name, x):\n",
    "    np.save(f'{dir_name}{file_name}', x)\n",
    "        \n",
    "def load_file_npy(dir_name, file_name):\n",
    "    return np.load(f'{dir_name}{file_name}').astype(np.float32)\n",
    "\n",
    "def chr2num(y):\n",
    "    converter = dict()\n",
    "    converter['b'] = 0\n",
    "    converter['t'] = 1\n",
    "    converter['e'] = 2\n",
    "    converter['m'] = 3\n",
    "    return np.asarray([converter[article_type] for article_type in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_tfidf = vectorizer.fit_transform(list(map(lambda x: x[0], train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 12829\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size: {}'.format(train_tfidf.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>0ff</th>\n",
       "      <th>0ut</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "      <th>...</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zooey</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zynga</th>\n",
       "      <th>zâ</th>\n",
       "      <th>œf</th>\n",
       "      <th>œwaist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10678</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10680 rows × 12829 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        00   07   08   09  0ff  0ut   10  100  1000  10000  ...  zoe  zombie  \\\n",
       "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  ...  0.0     0.0   \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  ...  0.0     0.0   \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  ...  0.0     0.0   \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  ...  0.0     0.0   \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  ...  0.0     0.0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...   ...    ...  ...  ...     ...   \n",
       "10675  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  ...  0.0     0.0   \n",
       "10676  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  ...  0.0     0.0   \n",
       "10677  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  ...  0.0     0.0   \n",
       "10678  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  ...  0.0     0.0   \n",
       "10679  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    0.0  ...  0.0     0.0   \n",
       "\n",
       "       zombies  zone  zooey  zuckerberg  zynga   zâ   œf  œwaist  \n",
       "0          0.0   0.0    0.0         0.0    0.0  0.0  0.0     0.0  \n",
       "1          0.0   0.0    0.0         0.0    0.0  0.0  0.0     0.0  \n",
       "2          0.0   0.0    0.0         0.0    0.0  0.0  0.0     0.0  \n",
       "3          0.0   0.0    0.0         0.0    0.0  0.0  0.0     0.0  \n",
       "4          0.0   0.0    0.0         0.0    0.0  0.0  0.0     0.0  \n",
       "...        ...   ...    ...         ...    ...  ...  ...     ...  \n",
       "10675      0.0   0.0    0.0         0.0    0.0  0.0  0.0     0.0  \n",
       "10676      0.0   0.0    0.0         0.0    0.0  0.0  0.0     0.0  \n",
       "10677      0.0   0.0    0.0         0.0    0.0  0.0  0.0     0.0  \n",
       "10678      0.0   0.0    0.0         0.0    0.0  0.0  0.0     0.0  \n",
       "10679      0.0   0.0    0.0         0.0    0.0  0.0  0.0     0.0  \n",
       "\n",
       "[10680 rows x 12829 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_tfidf.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "valid_tfidf = vectorizer.transform(list(map(lambda x: x[0], valid_data)))\n",
    "test_tfidf = vectorizer.transform(list(map(lambda x: x[0], test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_npy('work/', 'train_feature', train_tfidf.toarray())\n",
    "save_file_npy('work/', 'valid_feature', valid_tfidf.toarray())\n",
    "save_file_npy('work/', 'test_feature', test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = chr2num(list(map(lambda x: x[1], train_data)))\n",
    "valid_y = chr2num(list(map(lambda x: x[1], valid_data)))\n",
    "test_y = chr2num(list(map(lambda x: x[1], test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_npy('work/', 'train_y', train_y)\n",
    "save_file_npy('work/', 'valid_y', valid_y)\n",
    "save_file_npy('work/', 'test_y', test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 52. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = load_file_npy('work/', 'train_feature.npy')\n",
    "train_y = load_file_npy('work/', 'train_y.npy')\n",
    "valid_x = load_file_npy('work/', 'valid_feature.npy')\n",
    "valid_y = load_file_npy('work/', 'valid_y.npy')\n",
    "test_x = load_file_npy('work/', 'test_feature.npy')\n",
    "test_y = load_file_npy('work/', 'test_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 53. 予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_y = lr.predict(train_x)\n",
    "test_pred_y = lr.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 0. ... 2. 2. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(test_pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 54. 正解率の計測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.9218164794007491\n",
      "test accuracy = 0.8816479400749063\n"
     ]
    }
   ],
   "source": [
    "print('train accuracy = {}'.format(accuracy_score(y_true = train_y, y_pred = train_pred_y)))\n",
    "print('test accuracy = {}'.format(accuracy_score(y_true = test_y, y_pred = test_pred_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 55. 混同行列の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train confusion matrix = \n",
      " [[4370   32   85    3]\n",
      " [ 223  805  170    2]\n",
      " [  24    4 4211    0]\n",
      " [ 105    4  183  459]]\n"
     ]
    }
   ],
   "source": [
    "print('train confusion matrix = \\n', confusion_matrix(y_true = train_y, y_pred = train_pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test confusion matrix = \n",
      " [[538   6  18   1]\n",
      " [ 39  92  32   2]\n",
      " [ 12   2 511   0]\n",
      " [ 23   1  22  36]]\n"
     ]
    }
   ],
   "source": [
    "print('test confusion matrix = \\n', confusion_matrix(y_true = test_y, y_pred = test_pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 56. 適合率，再現率，F1スコアの計測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これ自信ないぽよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision micro =  0.8816479400749063\n",
      "precision macro =  0.8973884592847572\n",
      "precision =  0.8895181996798318\n",
      "recall micro =  0.8816479400749063\n",
      "recall macro =  0.7313821269489943\n",
      "recall =  0.8065150335119503\n",
      "f1 score micro =  0.8816479400749062\n",
      "f1 score macro =  0.7812244994883822\n",
      "f1 score =  0.8314362197816443\n"
     ]
    }
   ],
   "source": [
    "precision_micro = precision_score(y_true = test_y, y_pred = test_pred_y, average = 'micro')\n",
    "print('precision micro = ', precision_micro)\n",
    "precision_macro = precision_score(y_true = test_y, y_pred = test_pred_y, average = 'macro')\n",
    "print('precision macro = ', precision_macro)\n",
    "recall_micro = recall_score(y_true = test_y, y_pred = test_pred_y, average = 'micro')\n",
    "print('precision = ', (precision_micro + precision_macro) / 2)\n",
    "print('recall micro = ', recall_micro)\n",
    "recall_macro = recall_score(y_true = test_y, y_pred = test_pred_y, average = 'macro')\n",
    "print('recall macro = ', recall_macro)\n",
    "print('recall = ', (recall_micro + recall_macro) / 2)\n",
    "f1_score_micro = f1_score(y_true = test_y, y_pred = test_pred_y, average = 'micro')\n",
    "print('f1 score micro = ', f1_score_micro)\n",
    "f1_score_macro = f1_score(y_true = test_y, y_pred = test_pred_y, average = 'macro')\n",
    "print('f1 score macro = ', f1_score_macro)\n",
    "print('f1 score = ', (f1_score_micro + f1_score_macro) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 57. 特徴量の重みの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.36767703 -3.18083715 -3.10623132 ...  4.40239917  4.57616196\n",
      "   4.62608361]\n",
      " [-0.03444174 -0.0186113  -0.01232489 ... -0.0317544  -0.03314261\n",
      "  -0.02191253]\n",
      " [-0.15160942 -0.06559459 -0.02614219 ...  0.04693901  0.14384818\n",
      "   0.0439173 ]\n",
      " [-0.0285663  -0.01598717 -0.01064402 ... -0.02231331 -0.01760548\n",
      "  -0.01495018]]\n"
     ]
    }
   ],
   "source": [
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '07' '08' '09' '0ff' '0ut' '10' '100' '1000' '10000']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "print(feature_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['zynga' 'zâ' 'œwaist' 'zooey' 'zombies' 'zone' 'œf' 'zuckerberg'\n",
      "  'zombie' 'zoe']\n",
      " ['facebook' 'apple' 'google' 'microsoft' 'tesla' 'gm' 'climate'\n",
      "  'samsung' 'comcast' 'nasa']\n",
      " ['she' 'kardashian' 'star' 'kim' 'chris' 'her' 'miley' 'cyrus' 'film'\n",
      "  'movie']\n",
      " ['study' 'mers' 'drug' 'ebola' 'cdc' 'health' 'cancer' 'fda' 'could'\n",
      "  'cases']]\n"
     ]
    }
   ],
   "source": [
    "#各カテゴリの重み最大の単語\n",
    "max_indices = np.argpartition(-lr.coef_, 10)[:,:10]\n",
    "print(feature_names[max_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['07' '00' '08' '09' '0ff' '0ut' '100' '10' '1000' '10000']\n",
      " ['kim' 'ecb' 'fed' 'bank' 'her' 'kardashian' 'euro' 'stocks' 'ukraine'\n",
      "  'thrones']\n",
      " ['billion' 'says' 'fed' 'bank' 'ceo' 'china' 'facebook' 'us' 'google'\n",
      "  'update']\n",
      " ['bank' 'google' 'fed' 'kim' 'kardashian' 'china' 'stocks' 'ecb' 'euro'\n",
      "  'gm']]\n"
     ]
    }
   ],
   "source": [
    "#各カテゴリの重み最小の単語\n",
    "min_indices = np.argpartition(lr.coef_, 10)[:,:10]\n",
    "print(feature_names[min_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 58. 正則化パラメータの変更"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 59. ハイパーパラメータの探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
