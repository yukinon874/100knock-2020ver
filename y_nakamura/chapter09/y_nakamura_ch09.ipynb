{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80. ID番号への変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from functools import reduce\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, mode='r') as f:\n",
    "        X = list()\n",
    "        Y = list()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            splited_line = line.split('\\t')\n",
    "            X.append(splited_line[0])\n",
    "            Y.append(splited_line[1])\n",
    "        return X, Y\n",
    "\n",
    "def save_file_json(path, data):\n",
    "    with open(path, mode='w') as out_file:\n",
    "        out_file.write(json.dumps(data)+'\\n')\n",
    "        \n",
    "def load_file_json(path):\n",
    "    with open(path, mode='r') as in_file:\n",
    "        data = json.load(in_file)\n",
    "    return data\n",
    "\n",
    "def chr2num(y):\n",
    "    converter = {'b':0, 't':1, 'e':2, 'm':3}\n",
    "    return [converter[article_type] for article_type in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessTools:\n",
    "    def __init__(self, vocab_path=None):\n",
    "        self.word_count = defaultdict(int)       \n",
    "        if vocab_path:\n",
    "            self.word_transformer = load_file_json(vocab_path)\n",
    "            self.vocab_size = len(self.word_transformer) + 1\n",
    "        else:\n",
    "            self.word_transformer = dict()\n",
    "            self.vocab_size = -1\n",
    "        \n",
    "    def tokenize(self, data):\n",
    "        return [[word for word in word_tokenize(txt)] for txt in data]\n",
    "\n",
    "    def make_word_transformar(self, train_data:list):\n",
    "        for data in train_data:\n",
    "            for word in data:\n",
    "                self.word_count[word] += 1\n",
    "        sorted_word_count = sorted(self.word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "        for idx, (word, count) in enumerate(sorted_word_count):\n",
    "            if count < 2:\n",
    "                break\n",
    "            else:\n",
    "                self.word_transformer[word] = idx + 1\n",
    "        self.vocab_size = len(self.word_transformer) + 1\n",
    "\n",
    "    def txt2ids(self, txt_list:list):\n",
    "        txt_ids = list()\n",
    "        for txt in txt_list:\n",
    "            ids = list()\n",
    "            for word in txt:\n",
    "                if word in self.word_transformer:\n",
    "                    ids.append(self.word_transformer[word])\n",
    "                else:\n",
    "                    ids.append(0)\n",
    "            txt_ids.append(ids)\n",
    "        return txt_ids\n",
    "\n",
    "\n",
    "    def ids2vec(self, txt_ids:list):\n",
    "        txt_vec = list()\n",
    "        identity = np.identity(self.vocab_size)\n",
    "        for ids in txt_ids:\n",
    "            txt_vec.append(identity[ids])\n",
    "        return txt_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = PreprocessTools()\n",
    "x_train, y_train = load_data('data/train.txt')\n",
    "x_valid, y_valid = load_data('data/valid.txt')\n",
    "x_test, y_test = load_data('data/test.txt')\n",
    "x_train = preprocess.tokenize(x_train)\n",
    "x_valid = preprocess.tokenize(x_valid)\n",
    "x_test = preprocess.tokenize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.make_word_transformar(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ids = preprocess.txt2ids(x_train)\n",
    "x_valid_ids = preprocess.txt2ids(x_valid)\n",
    "x_test_ids = preprocess.txt2ids(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kindred', 'Healthcare', 'to', 'buy', 'Gentiva', 'for', 'about', '$', '573', 'mln'] [5065, 3395, 2, 181, 3396, 13, 164, 19, 0, 220]\n",
      "['US', 'to', 'boost', 'ground', ',', 'naval', 'forces', 'in', 'NATO', 'countries'] [15, 2, 586, 3397, 1, 0, 4067, 6, 5066, 5067]\n",
      "['Robert', 'Pattinson', '-', 'Robert', 'Pattinson', 'Brushes', 'Off', 'Kristen', 'Stewart', \"'s\", 'Cheating', '...'] [237, 945, 11, 237, 945, 0, 385, 669, 1259, 4, 3398, 3]\n",
      "['Piers', 'Morgan', 'Delivers', 'One', 'Final', 'Blow', 'To', 'Gun', 'Violence', 'In', 'Last', 'Show'] [5068, 399, 6690, 185, 1074, 5069, 16, 3399, 2225, 20, 785, 161]\n",
      "['Here', 'We', 'Go', ':', \"'Star\", 'Wars', 'Episode', 'VII', \"'\", 'Kicks', 'Off', 'Filming', 'at', 'Pinewood'] [400, 196, 639, 7, 549, 210, 295, 587, 5, 5070, 385, 1371, 22, 0]\n",
      "['Amazon', 'gets', 'in', 'the', 'game', ':', 'Retailer', 'beats', 'Google', 'to', 'buy', 'hit', 'console', 'broadcasting', '...'] [169, 330, 6, 17, 1619, 7, 0, 609, 82, 2, 181, 245, 5071, 0, 3]\n",
      "['FOREX-Euro', 'retreats', 'as', 'ECB', 'steps', 'up', 'verbal', 'campaign'] [331, 5072, 9, 52, 588, 43, 6691, 2518]\n",
      "['UPDATE', '2-US', 'appeals', 'court', 'revives', 'Apple', 'patent', 'lawsuit', 'against', 'Google'] [10, 301, 6692, 332, 0, 109, 1783, 734, 232, 82]\n",
      "['Kendall', 'Jenner', 'shows', 'support', 'for', 'Kanye', 'West', 'in', 'Yeezus', 'tour', 'shirt'] [735, 333, 204, 786, 13, 87, 69, 6, 0, 702, 5073]\n",
      "['UPDATE', '1-S.Korea', \"'s\", 'state', 'health', 'insurer', 'tobacco', 'firms', 'for', 'damages'] [10, 0, 4, 3400, 880, 6693, 3401, 946, 13, 3402]\n"
     ]
    }
   ],
   "source": [
    "for word, ids in zip(x_train[:10], x_train_ids[:10]):\n",
    "    print(word, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9866\n"
     ]
    }
   ],
   "source": [
    "print(preprocess.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_json('work/vocab.json', preprocess.word_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 81. RNNによる予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hotにはしない\n",
    "#xtrain_vec = preprocess.ids2vec(xtrain_ids) \n",
    "#xvalid_vec = preprocess.ids2vec(xvalid_ids)\n",
    "#xtest_vec = preprocess.ids2vec(xtest_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = chr2num(y_train)\n",
    "y_valid = chr2num(y_valid)\n",
    "y_test = chr2num(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_json('work/train_x.json', {'data':x_train_ids})\n",
    "save_file_json('work/train_y.json', {'data': y_train})\n",
    "save_file_json('work/valid_x.json', {'data': x_valid_ids})\n",
    "save_file_json('work/valid_y.json', {'data': y_valid})\n",
    "save_file_json('work/test_x.json', {'data': x_test_ids})\n",
    "save_file_json('work/test_y.json', {'data': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = load_file_json('work/train_x.json')['data']\n",
    "y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "x_valid = load_file_json('work/valid_x.json')['data']\n",
    "y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "x_test = load_file_json('work/test_x.json')['data']\n",
    "y_test = np.asarray(load_file_json('work/test_y.json')['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs: input, h_0\n",
    "\n",
    "- input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details.\n",
    "\n",
    "- h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.\n",
    "\n",
    "## Outputs: output, h_n\n",
    "\n",
    "- output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.\n",
    "\n",
    "    For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.\n",
    "\n",
    "- h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n",
    "\n",
    "    Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "dw, dh = 300, 50\n",
    "torch.manual_seed(1234)\n",
    "embed = nn.Embedding(vocab_size, dw, padding_idx=0) #idx 0 は 0埋め\n",
    "rnn = nn.RNN(dw, dh, num_layers=2, bidirectional=False, batch_first=True)\n",
    "linear = nn.Linear(50, 4, bias=True)\n",
    "softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "input = torch.LongTensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 50])\n",
      "tensor([[ 0.3656,  0.1161, -0.2724,  0.4356],\n",
      "        [ 0.1910,  0.0066, -0.2242,  0.0248]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.2179, -1.4674, -1.8559, -1.1479],\n",
      "        [-1.2056, -1.3900, -1.6208, -1.3718]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "output, hn = rnn(embed(input))\n",
    "print(hn.shape)\n",
    "x = linear(hn[-1])\n",
    "print(x)\n",
    "print(softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 82. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False, rnn_bias=True, PATH=None):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.dw, self.dh = dw, dh\n",
    "        if PATH:\n",
    "            self.embed = nn.from_pretrained(PATH)\n",
    "        else:\n",
    "            m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "            nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "            nn.init.constant_(m.weight[0], 0)\n",
    "            self.embed = m\n",
    "        self.rnn = nn.RNN(dw, dh, bias=rnn_bias, num_layers=num_layers, bidirectional=bidirectional, batch_first=True, nonlinearity='relu')\n",
    "        if bidirectional:\n",
    "            self.linear = nn.Linear(2 * dh, L, bias=True)\n",
    "        else:\n",
    "            self.linear = nn.Linear(dh, L, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "    '''\n",
    "    x: ids (not one hot vector)\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        _, hidden = self.rnn(x)\n",
    "        hidden = hidden.view(self.num_layers, 2 if self.bidirectional else 1, -1, self.dh)\n",
    "        last_hidden = hidden[-1]\n",
    "        if self.bidirectional:\n",
    "            x = self.linear(torch.cat([last_hidden[0], last_hidden[1]], dim=1))\n",
    "        else:\n",
    "            x = self.linear(last_hidden[0])\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "    \n",
    "    def update_from_word2vec(self, w2v, transformer):\n",
    "        for word, idx in transformer.items():\n",
    "            with torch.no_grad():\n",
    "                if word in w2v:\n",
    "                    self.embed.weight[idx].copy_(torch.from_numpy(w2v[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSets(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = [torch.LongTensor(data) for data in x]\n",
    "        self.y = [torch.LongTensor([data]) for data in y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    x = [data[0] for data in batch]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = torch.LongTensor([data[1] for data in batch])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution(data_x, data_y, op, criterion, model, batch_size=1, is_train=True, use_gpu=False):\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "    ndata = len(data_x)\n",
    "    dataset = MyDataSets(data_x, data_y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    sum_loss, acc_score = 0, 0\n",
    "    for batch_x, batch_y in data_loader:\n",
    "        op.zero_grad()\n",
    "        out = model(batch_x)\n",
    "        loss = criterion(out, batch_y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        acc_score += np.sum((pred == batch_y).cpu().detach().numpy())\n",
    "    return sum_loss / ndata, acc_score / ndata * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = load_file_json('work/train_x.json')['data']\n",
    "y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "x_valid = load_file_json('work/valid_x.json')['data']\n",
    "y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "x_test = load_file_json('work/test_x.json')['data']\n",
    "y_test = np.asarray(load_file_json('work/test_y.json')['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyRNN(vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False)\n",
    "ntrain = len(x_train)\n",
    "nepoch = 10 \n",
    "batch_size = 1\n",
    "op = optim.SGD(model.parameters(), lr=0.1)\n",
    "#op = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRNN(\n",
      "  (embed): Embedding(9866, 300, padding_idx=0)\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (linear): Linear(in_features=50, out_features=4, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a7ebf435ca4fd99fdd3b2ea5185b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 1.1710304718339042, 'train_acc': 41.08614232209738, 'valid_loss': 1.161880460214079, 'valid_acc': 42.54681647940075}\n",
      "{'epoch': 1, 'train_loss': 1.1614205118422205, 'train_acc': 42.537453183520604, 'valid_loss': 1.163245714112614, 'valid_acc': 49.13857677902622}\n",
      "{'epoch': 2, 'train_loss': 1.1620618166548482, 'train_acc': 41.21722846441948, 'valid_loss': 1.1617270750945874, 'valid_acc': 42.54681647940075}\n",
      "{'epoch': 3, 'train_loss': 1.161140552710058, 'train_acc': 41.947565543071164, 'valid_loss': 1.1605979920326548, 'valid_acc': 43.37078651685393}\n",
      "{'epoch': 4, 'train_loss': 1.1606137023882919, 'train_acc': 42.79962546816479, 'valid_loss': 1.162265390492557, 'valid_acc': 47.041198501872664}\n",
      "{'epoch': 5, 'train_loss': 1.159734628352333, 'train_acc': 42.930711610486895, 'valid_loss': 1.1590327347709, 'valid_acc': 43.37078651685393}\n",
      "{'epoch': 6, 'train_loss': 1.1581479981597442, 'train_acc': 44.04494382022472, 'valid_loss': 1.1556194185764155, 'valid_acc': 48.68913857677903}\n",
      "{'epoch': 7, 'train_loss': 1.1539837438069034, 'train_acc': 45.140449438202246, 'valid_loss': 1.1528491068422124, 'valid_acc': 39.47565543071161}\n",
      "{'epoch': 8, 'train_loss': 1.1465490350116059, 'train_acc': 48.28651685393258, 'valid_loss': 1.1526607534858617, 'valid_acc': 48.68913857677903}\n",
      "{'epoch': 9, 'train_loss': 1.1290656312128131, 'train_acc': 50.08426966292134, 'valid_loss': 1.11433073009891, 'valid_acc': 52.50936329588015}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_writer = SummaryWriter(log_dir='./work/logs/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/valid')\n",
    "logger = list()\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 83. ミニバッチ化・GPU上での学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/gpu.py\n"
     ]
    }
   ],
   "source": [
    "%%file src/gpu.py\n",
    "\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class PreprocessTools:\n",
    "    def __init__(self, vocab_path=None):\n",
    "        self.word_count = defaultdict(int)       \n",
    "        if vocab_path:\n",
    "            self.word_transformer = load_file_json(vocab_path)\n",
    "            self.vocab_size = len(self.word_transformer) + 1\n",
    "        else:\n",
    "            self.word_transformer = defaultdict(int)\n",
    "            self.vocab_size = -1\n",
    "        \n",
    "    def tokenize(self, data):\n",
    "        return [[word for word in word_tokenize(txt)] for txt in data]\n",
    "\n",
    "    def make_word_transformar(self, train_data:list):\n",
    "        for data in train_data:\n",
    "            for word in data:\n",
    "                self.word_count[word] += 1\n",
    "        sorted_word_count = sorted(self.word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "        for idx, (word, count) in enumerate(sorted_word_count):\n",
    "            if count < 2:\n",
    "                break\n",
    "            else:\n",
    "                self.word_transformer[word] = idx + 1\n",
    "        self.vocab_size = len(self.word_transformer) + 1\n",
    "\n",
    "    def txt2ids(self, txt_list:list):\n",
    "        txt_ids = list()\n",
    "        for txt in txt_list:\n",
    "            ids = list()\n",
    "            for word in txt:\n",
    "                ids.append(self.word_transformer[word])\n",
    "            txt_ids.append(ids)\n",
    "        return txt_ids\n",
    "\n",
    "\n",
    "    def ids2vec(self, txt_ids:list):\n",
    "        txt_vec = list()\n",
    "        identity = np.identity(self.vocab_size)\n",
    "        for ids in txt_ids:\n",
    "            txt_vec.append(identity[ids])\n",
    "        return txt_vec\n",
    "    \n",
    "    \n",
    "def load_data(path):\n",
    "    with open(path, mode='r') as f:\n",
    "        X = list()\n",
    "        Y = list()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            splited_line = line.split('\\t')\n",
    "            X.append(splited_line[0])\n",
    "            Y.append(splited_line[1])\n",
    "        return X, Y\n",
    "\n",
    "def save_file_json(path, data):\n",
    "    with open(path, mode='w') as out_file:\n",
    "        out_file.write(json.dumps(data)+'\\n')\n",
    "        \n",
    "def load_file_json(path):\n",
    "    with open(path, mode='r') as in_file:\n",
    "        data = json.load(in_file)\n",
    "    return data\n",
    "\n",
    "def chr2num(y):\n",
    "    converter = {'b':0, 't':1, 'e':2, 'm':3}\n",
    "    return [converter[article_type] for article_type in y]\n",
    "\n",
    "class MyRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False, rnn_bias=True, PATH=None):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.dw, self.dh = dw, dh\n",
    "        if PATH:\n",
    "            self.embed = nn.from_pretrained(PATH)\n",
    "        else:\n",
    "            m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "            nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "            nn.init.constant_(m.weight[0], 0)\n",
    "            self.embed = m\n",
    "        self.rnn = nn.RNN(dw, dh, bias=rnn_bias, num_layers=num_layers, bidirectional=bidirectional, batch_first=True, nonlinearity='tanh')\n",
    "        if bidirectional:\n",
    "            self.linear = nn.Linear(2 * dh, L, bias=True)\n",
    "        else:\n",
    "            self.linear = nn.Linear(dh, L, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "    '''\n",
    "    x: ids (not one hot vector)\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        _, hidden = self.rnn(x)\n",
    "        hidden = hidden.view(self.num_layers, 2 if self.bidirectional else 1, -1, self.dh)\n",
    "        last_hidden = hidden[-1]\n",
    "        if self.bidirectional:\n",
    "            x = self.linear(torch.cat([last_hidden[0], last_hidden[1]], dim=1))\n",
    "        else:\n",
    "            x = self.linear(last_hidden[0])\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "\n",
    "class MyDataSets(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = [torch.LongTensor(data) for data in x]\n",
    "        self.y = [torch.LongTensor([data]) for data in y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    x = [data[0] for data in batch]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = torch.LongTensor([data[1] for data in batch])\n",
    "    return x, y\n",
    "\n",
    "    \n",
    "def execution(data_x, data_y, op, criterion, model, batch_size=1, is_train=True, use_gpu=False):\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "    ndata = len(data_x)\n",
    "    dataset = MyDataSets(data_x, data_y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    sum_loss, acc_score = 0, 0\n",
    "    for batch_x, batch_y in data_loader:\n",
    "        op.zero_grad()\n",
    "        if use_gpu:\n",
    "            batch_x = batch_x.cuda()\n",
    "            batch_y = batch_y.cuda()\n",
    "        out = model(batch_x)\n",
    "        loss = criterion(out, batch_y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        acc_score += np.sum((pred == batch_y).cpu().detach().numpy())\n",
    "    return sum_loss / ndata, acc_score / ndata * 100\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess = PreprocessTools('work/vocab.json')\n",
    "    \n",
    "    x_train = load_file_json('work/train_x.json')['data']\n",
    "    y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "    x_valid = load_file_json('work/valid_x.json')['data']\n",
    "    y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "    x_test = load_file_json('work/test_x.json')['data']\n",
    "    y_test = np.asarray(load_file_json('work/test_y.json')['data'])\n",
    "\n",
    "\n",
    "    vocab_size = preprocess.vocab_size\n",
    "    torch.manual_seed(1234)\n",
    "    model = MyRNN(vocab_size, dw=300, dh=50, L=4, num_layers=2, bidirectional=True)\n",
    "    ntrain = len(x_train)\n",
    "    nepoch = 10 \n",
    "    batch_size = 128 \n",
    "    op = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.NLLLoss() \n",
    "\n",
    "    train_writer = SummaryWriter(log_dir='./work/logs/train')\n",
    "    valid_writer = SummaryWriter(log_dir='./work/logs/valid')\n",
    "    logger = list()\n",
    "    for epoch in tqdm.tqdm(range(nepoch)):\n",
    "        train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "        train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "        train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "        with torch.no_grad():\n",
    "            valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "            valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "            valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "        logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "        print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    train_writer.close()\n",
    "    valid_writer.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_size:1\telapsed_time:361.9438319206238 [sec]\n",
    "batch_size:2\telapsed_time:195.748544216156 [sec]\n",
    "batch_size:4\telapsed_time:100.17491579055786 [sec]\n",
    "batch_size:8\telapsed_time:52.92460918426514 [sec]\n",
    "batch_size:16\telapsed_time:28.268762826919556 [sec]\n",
    "batch_size:32\telapsed_time:15.605499982833862 [sec]\n",
    "batch_size:64\telapsed_time:10.040234327316284 [sec]\n",
    "batch_size:128\telapsed_time:6.472395420074463 [sec]\n",
    "batch_size:256\telapsed_time:4.624249458312988 [sec]\n",
    "batch_size:512\telapsed_time:3.524317502975464 [sec]\n",
    "batch_size:1024\telapsed_time:3.101412773132324 [sec]\n",
    "batch_size:2048\telapsed_time:2.749919891357422 [sec]\n",
    "batch_size:4096\telapsed_time:2.6615805625915527 [sec]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_size:1\telapsed_time:1016.4217035770416 [sec]\n",
    "batch_size:2\telapsed_time:527.0541796684265 [sec]\n",
    "batch_size:4\telapsed_time:291.180860042572 [sec]\n",
    "batch_size:8\telapsed_time:147.82439351081848 [sec]\n",
    "batch_size:16\telapsed_time:82.42432832717896 [sec]\n",
    "batch_size:32\telapsed_time:44.86296892166138 [sec]\n",
    "batch_size:64\telapsed_time:28.106043815612793 [sec]\n",
    "batch_size:128\telapsed_time:17.97643756866455 [sec]\n",
    "batch_size:256\telapsed_time:13.771385669708252 [sec]\n",
    "batch_size:512\telapsed_time:11.08540654182434 [sec]\n",
    "batch_size:1024\telapsed_time:9.829963445663452 [sec]\n",
    "batch_size:2048\telapsed_time:15.359896421432495 [sec]\n",
    "batch_size:4096\telapsed_time:12.083300590515137 [sec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 84. 単語ベクトルの導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyRNN(vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False)\n",
    "model.update_from_word2vec(w2v, preprocess.word_transformer)\n",
    "ntrain = len(x_train)\n",
    "nepoch = 10 \n",
    "batch_size = 128 \n",
    "#op = optim.SGD(model.parameters(), lr=0.1)\n",
    "op = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRNN(\n",
      "  (embed): Embedding(9866, 300, padding_idx=0)\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (linear): Linear(in_features=50, out_features=4, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae01c67ef5742b8815efbf8244972b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 1.0841823437687164, 'train_acc': 50.468164794007485, 'valid_loss': 0.8035261861840437, 'valid_acc': 73.85767790262172}\n",
      "{'epoch': 1, 'train_loss': 0.5676666072245394, 'train_acc': 79.05430711610487, 'valid_loss': 0.46076675584253746, 'valid_acc': 83.07116104868913}\n",
      "{'epoch': 2, 'train_loss': 0.3240814438919896, 'train_acc': 87.34082397003745, 'valid_loss': 0.45489098219835805, 'valid_acc': 83.59550561797752}\n",
      "{'epoch': 3, 'train_loss': 0.2447720053490628, 'train_acc': 89.84082397003745, 'valid_loss': 0.41074375164196286, 'valid_acc': 85.0187265917603}\n",
      "{'epoch': 4, 'train_loss': 0.20364386380723354, 'train_acc': 90.53370786516854, 'valid_loss': 0.4525810466946734, 'valid_acc': 85.2434456928839}\n",
      "{'epoch': 5, 'train_loss': 0.16895339485634578, 'train_acc': 91.44194756554307, 'valid_loss': 0.39696553704443943, 'valid_acc': 85.76779026217228}\n",
      "{'epoch': 6, 'train_loss': 0.14188933785488542, 'train_acc': 92.61235955056179, 'valid_loss': 0.4342009868291433, 'valid_acc': 85.76779026217228}\n",
      "{'epoch': 7, 'train_loss': 0.13013480573557737, 'train_acc': 94.20411985018727, 'valid_loss': 0.4601046518440104, 'valid_acc': 86.29213483146067}\n",
      "{'epoch': 8, 'train_loss': 0.10268918731239404, 'train_acc': 96.21722846441948, 'valid_loss': 0.6388556601402912, 'valid_acc': 86.66666666666667}\n",
      "{'epoch': 9, 'train_loss': 0.09355827285890722, 'train_acc': 97.09737827715355, 'valid_loss': 0.6587920381335284, 'valid_acc': 87.11610486891385}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_writer = SummaryWriter(log_dir='./work/logs/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/valid')\n",
    "logger = list()\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 85. 双方向RNN・多層化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False, dropout=0.0, rnn_bias=True, PATH=None):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.dw, self.dh = dw, dh\n",
    "        if PATH:\n",
    "            self.embed = nn.from_pretrained(PATH)\n",
    "        else:\n",
    "            m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "            nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "            nn.init.constant_(m.weight[0], 0)\n",
    "            self.embed = m\n",
    "        self.rnn = nn.RNN(dw, dh, bias=rnn_bias, num_layers=num_layers, bidirectional=bidirectional, batch_first=True, nonlinearity='relu', dropout=dropout)\n",
    "        if bidirectional:\n",
    "            self.linear = nn.Linear(2 * dh, L, bias=True)\n",
    "        else:\n",
    "            self.linear = nn.Linear(dh, L, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "    '''\n",
    "    x: ids (not one hot vector)\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        _, hidden = self.rnn(x)\n",
    "        hidden = hidden.view(self.num_layers, 2 if self.bidirectional else 1, -1, self.dh)\n",
    "        last_hidden = hidden[-1]\n",
    "        if self.bidirectional:\n",
    "            x = self.linear(torch.cat([last_hidden[0], last_hidden[1]], dim=1))\n",
    "        else:\n",
    "            x = self.linear(last_hidden[0])\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "    \n",
    "    def update_from_word2vec(self, w2v, transformer):\n",
    "        for word, idx in transformer.items():\n",
    "            with torch.no_grad():\n",
    "                if word in w2v:\n",
    "                    self.embed.weight[idx].copy_(torch.from_numpy(w2v[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c16056c906495684ac7ac4d04c6c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 0.5214494584874714, 'train_acc': 80.32771535580524, 'valid_loss': 0.40309572698694934, 'valid_acc': 84.34456928838952}\n",
      "{'epoch': 1, 'train_loss': 0.2972070640392518, 'train_acc': 88.65168539325843, 'valid_loss': 0.3528627883182483, 'valid_acc': 86.81647940074907}\n",
      "{'epoch': 2, 'train_loss': 0.19832998494083962, 'train_acc': 93.04307116104869, 'valid_loss': 0.3388892986354756, 'valid_acc': 88.91385767790261}\n",
      "{'epoch': 3, 'train_loss': 0.129997069045399, 'train_acc': 95.76779026217228, 'valid_loss': 0.3542939303519574, 'valid_acc': 89.8876404494382}\n",
      "{'epoch': 4, 'train_loss': 0.08409301009071007, 'train_acc': 97.35955056179775, 'valid_loss': 0.3383731834450911, 'valid_acc': 90.56179775280899}\n",
      "{'epoch': 5, 'train_loss': 0.06061504503035367, 'train_acc': 98.14606741573034, 'valid_loss': 0.390652206640565, 'valid_acc': 91.08614232209737}\n",
      "{'epoch': 6, 'train_loss': 0.04411436755642686, 'train_acc': 98.70786516853933, 'valid_loss': 0.41162516456864745, 'valid_acc': 91.01123595505618}\n",
      "{'epoch': 7, 'train_loss': 0.036276442868362985, 'train_acc': 99.03558052434457, 'valid_loss': 0.447160574604063, 'valid_acc': 90.93632958801498}\n",
      "{'epoch': 8, 'train_loss': 0.02717887132929129, 'train_acc': 99.27902621722846, 'valid_loss': 0.47730230541711444, 'valid_acc': 90.78651685393258}\n",
      "{'epoch': 9, 'train_loss': 0.02305590404705092, 'train_acc': 99.34456928838952, 'valid_loss': 0.48604311941938005, 'valid_acc': 90.78651685393258}\n",
      "{'epoch': 10, 'train_loss': 0.020198295474722145, 'train_acc': 99.43820224719101, 'valid_loss': 0.4924429955107442, 'valid_acc': 91.01123595505618}\n",
      "{'epoch': 11, 'train_loss': 0.01793896552290391, 'train_acc': 99.52247191011236, 'valid_loss': 0.5146746636329965, 'valid_acc': 90.86142322097378}\n",
      "{'epoch': 12, 'train_loss': 0.01531273230800426, 'train_acc': 99.5692883895131, 'valid_loss': 0.5440415174327093, 'valid_acc': 90.86142322097378}\n",
      "{'epoch': 13, 'train_loss': 0.015666990899323916, 'train_acc': 99.54119850187266, 'valid_loss': 0.5216245273972272, 'valid_acc': 90.78651685393258}\n",
      "{'epoch': 14, 'train_loss': 0.01266861653237949, 'train_acc': 99.6067415730337, 'valid_loss': 0.5472780854960952, 'valid_acc': 90.93632958801498}\n",
      "{'epoch': 15, 'train_loss': 0.011386164935473144, 'train_acc': 99.6629213483146, 'valid_loss': 0.5608530048127478, 'valid_acc': 90.78651685393258}\n",
      "{'epoch': 16, 'train_loss': 0.010903896324245838, 'train_acc': 99.69101123595505, 'valid_loss': 0.5834014747919661, 'valid_acc': 90.78651685393258}\n",
      "{'epoch': 17, 'train_loss': 0.009617842179688203, 'train_acc': 99.70973782771536, 'valid_loss': 0.5890885730584462, 'valid_acc': 90.93632958801498}\n",
      "{'epoch': 18, 'train_loss': 0.008870455074474592, 'train_acc': 99.69101123595505, 'valid_loss': 0.618728354003992, 'valid_acc': 90.93632958801498}\n",
      "{'epoch': 19, 'train_loss': 0.008753099787042019, 'train_acc': 99.71910112359551, 'valid_loss': 0.6444673932000492, 'valid_acc': 90.78651685393258}\n",
      "{'epoch': 20, 'train_loss': 0.008094069706746953, 'train_acc': 99.6816479400749, 'valid_loss': 0.6301028210572089, 'valid_acc': 90.48689138576779}\n",
      "{'epoch': 21, 'train_loss': 0.008439318371774482, 'train_acc': 99.69101123595505, 'valid_loss': 0.6351963798651534, 'valid_acc': 90.63670411985018}\n",
      "{'epoch': 22, 'train_loss': 0.007968881078612074, 'train_acc': 99.71910112359551, 'valid_loss': 0.6493587417660581, 'valid_acc': 90.71161048689137}\n",
      "{'epoch': 23, 'train_loss': 0.005892335608460302, 'train_acc': 99.812734082397, 'valid_loss': 0.6620513841900486, 'valid_acc': 90.71161048689137}\n",
      "{'epoch': 24, 'train_loss': 0.007173564703110431, 'train_acc': 99.7940074906367, 'valid_loss': 0.658873180146521, 'valid_acc': 90.78651685393258}\n",
      "{'epoch': 25, 'train_loss': 0.007344340359594164, 'train_acc': 99.76591760299625, 'valid_loss': 0.6502218201589048, 'valid_acc': 90.71161048689137}\n",
      "{'epoch': 26, 'train_loss': 0.005910984864907535, 'train_acc': 99.78464419475655, 'valid_loss': 0.6683314778831567, 'valid_acc': 90.63670411985018}\n",
      "{'epoch': 27, 'train_loss': 0.006768058991379694, 'train_acc': 99.78464419475655, 'valid_loss': 0.6848623431577218, 'valid_acc': 90.71161048689137}\n",
      "{'epoch': 28, 'train_loss': 0.006116266653777976, 'train_acc': 99.812734082397, 'valid_loss': 0.6992925957794046, 'valid_acc': 90.56179775280899}\n",
      "{'epoch': 29, 'train_loss': 0.005799773345114524, 'train_acc': 99.812734082397, 'valid_loss': 0.6849708574095023, 'valid_acc': 90.86142322097378}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyRNN(vocab_size, dw=300, dh=50, L=4, num_layers=2, bidirectional=True, dropout=0.6)\n",
    "model.update_from_word2vec(w2v, preprocess.word_transformer)\n",
    "ntrain = len(x_train)\n",
    "nepoch = 30 \n",
    "batch_size = 64 \n",
    "#op = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "op = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0.001)\n",
    "#op = optim.Adam(model.parameters(), lr=0.001)\n",
    "#nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "criterion = nn.NLLLoss() \n",
    "train_writer = SummaryWriter(log_dir='./work/logs/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/valid')\n",
    "logger = list()\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 86. 畳み込みニューラルネットワーク (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "dw, dh = 300, 50\n",
    "torch.manual_seed(1234)\n",
    "embed = nn.Embedding(vocab_size, dw, padding_idx=0) #idx 0 は 0埋め\n",
    "kernel_size = [3 * dw, dh]\n",
    "cnn = nn.Conv2d(1, 1, kernel_size, padding=0, stride=1, padding_mode='replicate')\n",
    "linear = nn.Linear(50, 4, bias=True)\n",
    "softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "input = torch.LongTensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 900, 50])\n"
     ]
    }
   ],
   "source": [
    "print(cnn.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([600])"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = embed(input)\n",
    "torch.cat([x[0][0], x[0][1]]).shape\n",
    "#print(cnn(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, L=4, dropout=0.0, PATH=None):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.dw, self.dh = dw, dh\n",
    "        if PATH:\n",
    "            self.embed = nn.from_pretrained(PATH)\n",
    "        else:\n",
    "            m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "            nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "            nn.init.constant_(m.weight[0], 0)\n",
    "            self.embed = m\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "    '''\n",
    "    x: ids (not one hot vector)\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        _, hidden = self.rnn(x)\n",
    "        hidden = hidden.view(self.num_layers, 2 if self.bidirectional else 1, -1, self.dh)\n",
    "        last_hidden = hidden[-1]\n",
    "        if self.bidirectional:\n",
    "            x = self.linear(torch.cat([last_hidden[0], last_hidden[1]], dim=1))\n",
    "        else:\n",
    "            x = self.linear(last_hidden[0])\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "    \n",
    "    def update_from_word2vec(self, w2v, transformer):\n",
    "        for word, idx in transformer.items():\n",
    "            with torch.no_grad():\n",
    "                if word in w2v:\n",
    "                    self.embed.weight[idx].copy_(torch.from_numpy(w2v[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 87. 確率的勾配降下法によるCNNの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 88. パラメータチューニング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 89. 事前学習済み言語モデルからの転移学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
