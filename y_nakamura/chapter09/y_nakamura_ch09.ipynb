{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80. ID番号への変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from functools import reduce\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, mode='r') as f:\n",
    "        X = list()\n",
    "        Y = list()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            splited_line = line.split('\\t')\n",
    "            X.append(splited_line[0])\n",
    "            Y.append(splited_line[1])\n",
    "        return X, Y\n",
    "\n",
    "def save_file_json(path, data):\n",
    "    with open(path, mode='w') as out_file:\n",
    "        out_file.write(json.dumps(data)+'\\n')\n",
    "        \n",
    "def load_file_json(path):\n",
    "    with open(path, mode='r') as in_file:\n",
    "        data = json.load(in_file)\n",
    "    return data\n",
    "\n",
    "def chr2num(y):\n",
    "    converter = {'b':0, 't':1, 'e':2, 'm':3}\n",
    "    return [converter[article_type] for article_type in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessTools:\n",
    "    def __init__(self, vocab_path=None):\n",
    "        self.word_count = defaultdict(int)       \n",
    "        if vocab_path:\n",
    "            self.word_transformer = load_file_json(vocab_path)\n",
    "            self.vocab_size = len(self.word_transformer) + 1\n",
    "        else:\n",
    "            self.word_transformer = dict()\n",
    "            self.vocab_size = -1\n",
    "        \n",
    "    def tokenize(self, data):\n",
    "        return [[word for word in word_tokenize(txt)] for txt in data]\n",
    "\n",
    "    def make_word_transformar(self, train_data:list):\n",
    "        for data in train_data:\n",
    "            for word in data:\n",
    "                self.word_count[word] += 1\n",
    "        sorted_word_count = sorted(self.word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "        for idx, (word, count) in enumerate(sorted_word_count):\n",
    "            if count < 2:\n",
    "                break\n",
    "            else:\n",
    "                self.word_transformer[word] = idx + 1\n",
    "        self.vocab_size = len(self.word_transformer) + 1\n",
    "\n",
    "    def txt2ids(self, txt_list:list):\n",
    "        txt_ids = list()\n",
    "        for txt in txt_list:\n",
    "            ids = list()\n",
    "            for word in txt:\n",
    "                if word in self.word_transformer:\n",
    "                    ids.append(self.word_transformer[word])\n",
    "                else:\n",
    "                    ids.append(0)\n",
    "            txt_ids.append(ids)\n",
    "        return txt_ids\n",
    "\n",
    "\n",
    "    def ids2vec(self, txt_ids:list):\n",
    "        txt_vec = list()\n",
    "        identity = np.identity(self.vocab_size)\n",
    "        for ids in txt_ids:\n",
    "            txt_vec.append(identity[ids])\n",
    "        return txt_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = PreprocessTools()\n",
    "x_train, y_train = load_data('data/train.txt')\n",
    "x_valid, y_valid = load_data('data/valid.txt')\n",
    "x_test, y_test = load_data('data/test.txt')\n",
    "x_train = preprocess.tokenize(x_train)\n",
    "x_valid = preprocess.tokenize(x_valid)\n",
    "x_test = preprocess.tokenize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.make_word_transformar(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ids = preprocess.txt2ids(x_train)\n",
    "x_valid_ids = preprocess.txt2ids(x_valid)\n",
    "x_test_ids = preprocess.txt2ids(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kindred', 'Healthcare', 'to', 'buy', 'Gentiva', 'for', 'about', '$', '573', 'mln'] [5065, 3395, 2, 181, 3396, 13, 164, 19, 0, 220]\n",
      "['US', 'to', 'boost', 'ground', ',', 'naval', 'forces', 'in', 'NATO', 'countries'] [15, 2, 586, 3397, 1, 0, 4067, 6, 5066, 5067]\n",
      "['Robert', 'Pattinson', '-', 'Robert', 'Pattinson', 'Brushes', 'Off', 'Kristen', 'Stewart', \"'s\", 'Cheating', '...'] [237, 945, 11, 237, 945, 0, 385, 669, 1259, 4, 3398, 3]\n",
      "['Piers', 'Morgan', 'Delivers', 'One', 'Final', 'Blow', 'To', 'Gun', 'Violence', 'In', 'Last', 'Show'] [5068, 399, 6690, 185, 1074, 5069, 16, 3399, 2225, 20, 785, 161]\n",
      "['Here', 'We', 'Go', ':', \"'Star\", 'Wars', 'Episode', 'VII', \"'\", 'Kicks', 'Off', 'Filming', 'at', 'Pinewood'] [400, 196, 639, 7, 549, 210, 295, 587, 5, 5070, 385, 1371, 22, 0]\n",
      "['Amazon', 'gets', 'in', 'the', 'game', ':', 'Retailer', 'beats', 'Google', 'to', 'buy', 'hit', 'console', 'broadcasting', '...'] [169, 330, 6, 17, 1619, 7, 0, 609, 82, 2, 181, 245, 5071, 0, 3]\n",
      "['FOREX-Euro', 'retreats', 'as', 'ECB', 'steps', 'up', 'verbal', 'campaign'] [331, 5072, 9, 52, 588, 43, 6691, 2518]\n",
      "['UPDATE', '2-US', 'appeals', 'court', 'revives', 'Apple', 'patent', 'lawsuit', 'against', 'Google'] [10, 301, 6692, 332, 0, 109, 1783, 734, 232, 82]\n",
      "['Kendall', 'Jenner', 'shows', 'support', 'for', 'Kanye', 'West', 'in', 'Yeezus', 'tour', 'shirt'] [735, 333, 204, 786, 13, 87, 69, 6, 0, 702, 5073]\n",
      "['UPDATE', '1-S.Korea', \"'s\", 'state', 'health', 'insurer', 'tobacco', 'firms', 'for', 'damages'] [10, 0, 4, 3400, 880, 6693, 3401, 946, 13, 3402]\n"
     ]
    }
   ],
   "source": [
    "for word, ids in zip(x_train[:10], x_train_ids[:10]):\n",
    "    print(word, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9866\n"
     ]
    }
   ],
   "source": [
    "print(preprocess.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_json('work/vocab.json', preprocess.word_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 81. RNNによる予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hotにはしない\n",
    "#xtrain_vec = preprocess.ids2vec(xtrain_ids) \n",
    "#xvalid_vec = preprocess.ids2vec(xvalid_ids)\n",
    "#xtest_vec = preprocess.ids2vec(xtest_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = chr2num(y_train)\n",
    "y_valid = chr2num(y_valid)\n",
    "y_test = chr2num(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_json('work/train_x.json', {'data':x_train_ids})\n",
    "save_file_json('work/train_y.json', {'data': y_train})\n",
    "save_file_json('work/valid_x.json', {'data': x_valid_ids})\n",
    "save_file_json('work/valid_y.json', {'data': y_valid})\n",
    "save_file_json('work/test_x.json', {'data': x_test_ids})\n",
    "save_file_json('work/test_y.json', {'data': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = load_file_json('work/train_x.json')['data']\n",
    "y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "x_valid = load_file_json('work/valid_x.json')['data']\n",
    "y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "x_test = load_file_json('work/test_x.json')['data']\n",
    "y_test = np.asarray(load_file_json('work/test_y.json')['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs: input, h_0\n",
    "\n",
    "- input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details.\n",
    "\n",
    "- h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.\n",
    "\n",
    "## Outputs: output, h_n\n",
    "\n",
    "- output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.\n",
    "\n",
    "    For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.\n",
    "\n",
    "- h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n",
    "\n",
    "    Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "dw, dh = 300, 50\n",
    "torch.manual_seed(1234)\n",
    "embed = nn.Embedding(vocab_size, dw, padding_idx=0) #idx 0 は 0埋め\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "rnn = nn.RNN(dw, dh, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "input = torch.LongTensor([[0, 1, 2, 3, 4]])\n",
    "linear = nn.Linear(50, 4, bias=True)\n",
    "softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.9588,  0.7610, -0.9565,  0.5900, -0.5844,  0.9816,  0.9628,\n",
      "          -0.5733, -0.9226, -0.8944,  0.1460,  0.4036,  0.9598,  0.8089,\n",
      "           0.9755, -0.9994,  0.9671,  0.1083,  0.9839, -0.7825,  0.9294,\n",
      "           0.9097, -0.8783, -0.7987, -0.9839, -0.9838,  0.7480, -0.9795,\n",
      "          -0.9946,  0.8387, -0.4521, -0.9794, -0.3257,  0.4931, -0.9273,\n",
      "          -0.8774, -0.4283, -0.9119, -0.9689, -0.8910,  0.6624,  0.9643,\n",
      "          -0.2032, -0.2346,  0.0353,  0.8976, -0.2835, -0.9466, -0.8547,\n",
      "          -0.7617]],\n",
      "\n",
      "        [[ 0.6467,  0.4122,  0.2868,  0.2072,  0.1445, -0.1215,  0.5725,\n",
      "           0.0861,  0.1255, -0.3936,  0.6288,  0.1832, -0.1401,  0.2997,\n",
      "           0.1401, -0.3450, -0.6161, -0.1151, -0.4304,  0.0187, -0.7646,\n",
      "           0.2113,  0.8125, -0.4816, -0.5212,  0.2364,  0.0617, -0.3991,\n",
      "          -0.4392, -0.6603,  0.2891, -0.1834,  0.2687,  0.2681, -0.4468,\n",
      "          -0.3341,  0.1834,  0.1835, -0.4578,  0.2792,  0.6250,  0.0511,\n",
      "          -0.1108, -0.4308, -0.7459, -0.5074,  0.0465, -0.0198,  0.5051,\n",
      "           0.4699]],\n",
      "\n",
      "        [[ 0.1849, -0.3214,  0.7265, -0.9820,  0.0688,  0.5299,  0.9316,\n",
      "          -0.1264,  0.8790,  0.2429, -0.3675, -0.3819,  0.8729,  0.2429,\n",
      "          -0.1085,  0.2291, -0.1721, -0.2097, -0.5037, -0.0138,  0.2188,\n",
      "           0.1799,  0.7399, -0.0382, -0.2318, -0.8965, -0.7706, -0.8238,\n",
      "          -0.5875,  0.6266,  0.0176, -0.3838, -0.1236, -0.0666,  0.8076,\n",
      "           0.1164,  0.0104,  0.1598,  0.4741,  0.3584,  0.8805,  0.5828,\n",
      "          -0.7260,  0.9095,  0.7137,  0.5898, -0.0039,  0.6323, -0.8684,\n",
      "          -0.5871]],\n",
      "\n",
      "        [[ 0.1481,  0.4623,  0.2626, -0.1558, -0.5080,  0.2642,  0.2887,\n",
      "          -0.1443, -0.5105, -0.5715, -0.6857, -0.2533,  0.4301,  0.3039,\n",
      "          -0.0092, -0.7680, -0.5048, -0.1967,  0.3575,  0.3964, -0.2472,\n",
      "          -0.0245, -0.4628,  0.1140, -0.4245, -0.0258,  0.5089,  0.1486,\n",
      "          -0.0030,  0.0629, -0.5284, -0.2572,  0.5957, -0.1044, -0.1753,\n",
      "          -0.5407,  0.5071,  0.0366,  0.5268, -0.2456,  0.0632, -0.1939,\n",
      "          -0.2886, -0.1329, -0.3405,  0.2228, -0.1557,  0.1135, -0.6398,\n",
      "          -0.3822]]], grad_fn=<StackBackward>)\n",
      "tensor([[[ 0.1849, -0.3214,  0.7265, -0.9820,  0.0688,  0.5299,  0.9316,\n",
      "          -0.1264,  0.8790,  0.2429, -0.3675, -0.3819,  0.8729,  0.2429,\n",
      "          -0.1085,  0.2291, -0.1721, -0.2097, -0.5037, -0.0138,  0.2188,\n",
      "           0.1799,  0.7399, -0.0382, -0.2318, -0.8965, -0.7706, -0.8238,\n",
      "          -0.5875,  0.6266,  0.0176, -0.3838, -0.1236, -0.0666,  0.8076,\n",
      "           0.1164,  0.0104,  0.1598,  0.4741,  0.3584,  0.8805,  0.5828,\n",
      "          -0.7260,  0.9095,  0.7137,  0.5898, -0.0039,  0.6323, -0.8684,\n",
      "          -0.5871]],\n",
      "\n",
      "        [[ 0.1481,  0.4623,  0.2626, -0.1558, -0.5080,  0.2642,  0.2887,\n",
      "          -0.1443, -0.5105, -0.5715, -0.6857, -0.2533,  0.4301,  0.3039,\n",
      "          -0.0092, -0.7680, -0.5048, -0.1967,  0.3575,  0.3964, -0.2472,\n",
      "          -0.0245, -0.4628,  0.1140, -0.4245, -0.0258,  0.5089,  0.1486,\n",
      "          -0.0030,  0.0629, -0.5284, -0.2572,  0.5957, -0.1044, -0.1753,\n",
      "          -0.5407,  0.5071,  0.0366,  0.5268, -0.2456,  0.0632, -0.1939,\n",
      "          -0.2886, -0.1329, -0.3405,  0.2228, -0.1557,  0.1135, -0.6398,\n",
      "          -0.3822]]], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "output, hidden = rnn(embed(input))\n",
    "print(hidden)\n",
    "hidden = hidden.view(num_layers, 2 if bidirectional else 1, -1, dh)\n",
    "#print(hidden)\n",
    "last_hidden = hidden[-1]\n",
    "print(last_hidden)\n",
    "#x = linear(hidden[-1])\n",
    "#print(x)\n",
    "#print(softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 50])\n",
      "tensor([[[[-0.9588,  0.7610, -0.9565,  0.5900, -0.5844,  0.9816,  0.9628,\n",
      "           -0.5733, -0.9226, -0.8944,  0.1460,  0.4036,  0.9598,  0.8089,\n",
      "            0.9755, -0.9994,  0.9671,  0.1083,  0.9839, -0.7825,  0.9294,\n",
      "            0.9097, -0.8783, -0.7987, -0.9839, -0.9838,  0.7480, -0.9795,\n",
      "           -0.9946,  0.8387, -0.4521, -0.9794, -0.3257,  0.4931, -0.9273,\n",
      "           -0.8774, -0.4283, -0.9119, -0.9689, -0.8910,  0.6624,  0.9643,\n",
      "           -0.2032, -0.2346,  0.0353,  0.8976, -0.2835, -0.9466, -0.8547,\n",
      "           -0.7617],\n",
      "          [ 0.8426, -0.0087,  0.9774,  0.8538,  0.4782, -0.6959,  0.4698,\n",
      "            0.6362,  0.2218,  0.9951,  0.9583,  0.9620,  0.6493, -0.8816,\n",
      "           -0.3704, -0.9910,  0.7911, -0.6577,  0.8689, -0.5620,  0.9889,\n",
      "            0.4303, -0.9841, -0.6123, -0.6164, -0.8283,  0.5802,  0.9361,\n",
      "            0.5248, -0.9203,  0.7948,  0.9841, -0.4951,  0.9653,  0.3215,\n",
      "            0.0211, -0.8799,  0.8807,  0.8267, -0.6232,  1.0000,  0.1017,\n",
      "            0.3106,  0.9512, -0.6253, -0.4531, -0.9522,  0.6767, -0.5615,\n",
      "           -0.6872]]],\n",
      "\n",
      "\n",
      "        [[[-0.3830,  0.3538, -0.4110, -0.5545,  0.4338,  0.1584,  0.7069,\n",
      "           -0.5637, -0.0689, -0.2600, -0.8683,  0.8483, -0.3127, -0.1350,\n",
      "           -0.3615, -0.5036,  0.3629,  0.9161,  0.4008, -0.4728,  0.1985,\n",
      "            0.2731,  0.7614,  0.1421,  0.4071,  0.2477,  0.3459, -0.4124,\n",
      "           -0.5912, -0.1848,  0.1775,  0.7608, -0.5969, -0.0939, -0.4988,\n",
      "            0.3173,  0.0281, -0.0270, -0.5084,  0.2242,  0.3207,  0.1414,\n",
      "           -0.1770, -0.2583,  0.7680,  0.3466,  0.2475, -0.3726,  0.4511,\n",
      "           -0.2360],\n",
      "          [-0.6264,  0.3932, -0.2255, -0.3236, -0.3958,  0.6881,  0.2437,\n",
      "           -0.8589,  0.7434,  0.6048,  0.3585,  0.0416,  0.0809, -0.2589,\n",
      "           -0.7223, -0.2905,  0.1789,  0.8684,  0.2572,  0.0121,  0.6689,\n",
      "            0.1254,  0.5855,  0.3966,  0.5681, -0.2039,  0.4120, -0.0111,\n",
      "            0.7642,  0.5110,  0.0892,  0.1279,  0.2740,  0.0628, -0.4232,\n",
      "           -0.4149, -0.5136, -0.6611, -0.4580,  0.3846, -0.3276,  0.2606,\n",
      "            0.7238, -0.3607, -0.3181,  0.0359, -0.7637,  0.2464,  0.0602,\n",
      "           -0.1428]]]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "output, hidden = rnn(embed(input))\n",
    "print(hidden.shape)\n",
    "hidden = hidden.view(num_layers, 2 if bidirectional else 1, -1, dh)\n",
    "print(hidden)\n",
    "last_hidden = hidden[-1]\n",
    "#x = linear(hidden[-1])\n",
    "#print(x)\n",
    "#print(softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 82. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False, rnn_bias=True, PATH=None):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.dw, self.dh = dw, dh\n",
    "        if PATH:\n",
    "            self.embed = nn.from_pretrained(PATH)\n",
    "        else:\n",
    "            m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "            nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "            nn.init.constant_(m.weight[0], 0)\n",
    "            self.embed = m\n",
    "        self.rnn = nn.RNN(dw, dh, bias=rnn_bias, num_layers=num_layers, bidirectional=bidirectional, batch_first=True, nonlinearity='relu')\n",
    "        if bidirectional:\n",
    "            self.linear = nn.Linear(2 * dh, L, bias=True)\n",
    "        else:\n",
    "            self.linear = nn.Linear(dh, L, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "    '''\n",
    "    x: ids (not one hot vector)\n",
    "    '''\n",
    "    def forward(self, x, x_lengths):\n",
    "        x = self.embed(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, hidden = self.rnn(packed)\n",
    "        hidden = hidden.view(self.num_layers, 2 if self.bidirectional else 1, -1, self.dh)\n",
    "        last_hidden = hidden[-1]\n",
    "        if self.bidirectional:\n",
    "            x = self.linear(torch.cat([last_hidden[0], last_hidden[1]], dim=1))\n",
    "        else:\n",
    "            x = self.linear(last_hidden[0])\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "    \n",
    "    def update_from_word2vec(self, w2v, transformer):\n",
    "        for word, idx in transformer.items():\n",
    "            with torch.no_grad():\n",
    "                if word in w2v:\n",
    "                    self.embed.weight[idx].copy_(torch.from_numpy(w2v[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSets(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = [torch.LongTensor(data) for data in x]\n",
    "        self.y = [torch.LongTensor([data]) for data in y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    x = [data[0] for data in batch]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = torch.LongTensor([data[1] for data in batch])\n",
    "    lengths = torch.LongTensor([len(data[0]) for data in batch])\n",
    "    return x, y, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution(data_x, data_y, op, criterion, model, batch_size=1, is_train=True, use_gpu=False):\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "    ndata = len(data_x)\n",
    "    dataset = MyDataSets(data_x, data_y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    sum_loss, acc_score = 0, 0\n",
    "    for batch_x, batch_y, batch_lengths in data_loader:\n",
    "        op.zero_grad()\n",
    "        out = model(batch_x, batch_lengths)\n",
    "        loss = criterion(out, batch_y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        acc_score += np.sum((pred == batch_y).cpu().detach().numpy())\n",
    "    return sum_loss / ndata, acc_score / ndata * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = load_file_json('work/train_x.json')['data']\n",
    "y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "x_valid = load_file_json('work/valid_x.json')['data']\n",
    "y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "x_test = load_file_json('work/test_x.json')['data']\n",
    "y_test = np.asarray(load_file_json('work/test_y.json')['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyRNN(vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False)\n",
    "nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "ntrain = len(x_train)\n",
    "nepoch = 10 \n",
    "batch_size = 1\n",
    "op = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.NLLLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRNN(\n",
      "  (embed): Embedding(9866, 300, padding_idx=0)\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (linear): Linear(in_features=50, out_features=4, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd189293fec44314bef7d8eb31dd0c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 1.0372647139848967, 'train_acc': 59.21348314606741, 'valid_loss': 0.887252983872252, 'valid_acc': 69.06367041198503}\n",
      "{'epoch': 1, 'train_loss': 0.8374215004742438, 'train_acc': 70.91760299625469, 'valid_loss': 0.6876947393141969, 'valid_acc': 76.70411985018727}\n",
      "{'epoch': 2, 'train_loss': 0.7466804328128738, 'train_acc': 74.11048689138576, 'valid_loss': 0.7361666740501925, 'valid_acc': 72.65917602996255}\n",
      "{'epoch': 3, 'train_loss': 0.735462963356367, 'train_acc': 74.6067415730337, 'valid_loss': 0.6611646328217783, 'valid_acc': 76.17977528089888}\n",
      "{'epoch': 4, 'train_loss': 0.6526657013838769, 'train_acc': 77.4812734082397, 'valid_loss': 0.9591464588454356, 'valid_acc': 63.97003745318352}\n",
      "{'epoch': 5, 'train_loss': nan, 'train_acc': 70.78651685393258, 'valid_loss': nan, 'valid_acc': 42.54681647940075}\n",
      "{'epoch': 6, 'train_loss': nan, 'train_acc': 42.041198501872664, 'valid_loss': nan, 'valid_acc': 42.54681647940075}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-df7023a10c87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-c37af45fcf40>\u001b[0m in \u001b[0;36mexecution\u001b[0;34m(data_x, data_y, op, criterion, model, batch_size, is_train, use_gpu)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0msum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_writer = SummaryWriter(log_dir='./work/logs/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/valid')\n",
    "logger = list()\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 83. ミニバッチ化・GPU上での学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/gpu.py\n"
     ]
    }
   ],
   "source": [
    "%%file src/gpu.py\n",
    "\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class PreprocessTools:\n",
    "    def __init__(self, vocab_path=None):\n",
    "        self.word_count = defaultdict(int)       \n",
    "        if vocab_path:\n",
    "            self.word_transformer = load_file_json(vocab_path)\n",
    "            self.vocab_size = len(self.word_transformer) + 1\n",
    "        else:\n",
    "            self.word_transformer = defaultdict(int)\n",
    "            self.vocab_size = -1\n",
    "        \n",
    "    def tokenize(self, data):\n",
    "        return [[word for word in word_tokenize(txt)] for txt in data]\n",
    "\n",
    "    def make_word_transformar(self, train_data:list):\n",
    "        for data in train_data:\n",
    "            for word in data:\n",
    "                self.word_count[word] += 1\n",
    "        sorted_word_count = sorted(self.word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "        for idx, (word, count) in enumerate(sorted_word_count):\n",
    "            if count < 2:\n",
    "                break\n",
    "            else:\n",
    "                self.word_transformer[word] = idx + 1\n",
    "        self.vocab_size = len(self.word_transformer) + 1\n",
    "\n",
    "    def txt2ids(self, txt_list:list):\n",
    "        txt_ids = list()\n",
    "        for txt in txt_list:\n",
    "            ids = list()\n",
    "            for word in txt:\n",
    "                ids.append(self.word_transformer[word])\n",
    "            txt_ids.append(ids)\n",
    "        return txt_ids\n",
    "\n",
    "\n",
    "    def ids2vec(self, txt_ids:list):\n",
    "        txt_vec = list()\n",
    "        identity = np.identity(self.vocab_size)\n",
    "        for ids in txt_ids:\n",
    "            txt_vec.append(identity[ids])\n",
    "        return txt_vec\n",
    "    \n",
    "    \n",
    "def load_data(path):\n",
    "    with open(path, mode='r') as f:\n",
    "        X = list()\n",
    "        Y = list()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            splited_line = line.split('\\t')\n",
    "            X.append(splited_line[0])\n",
    "            Y.append(splited_line[1])\n",
    "        return X, Y\n",
    "\n",
    "def save_file_json(path, data):\n",
    "    with open(path, mode='w') as out_file:\n",
    "        out_file.write(json.dumps(data)+'\\n')\n",
    "        \n",
    "def load_file_json(path):\n",
    "    with open(path, mode='r') as in_file:\n",
    "        data = json.load(in_file)\n",
    "    return data\n",
    "\n",
    "def chr2num(y):\n",
    "    converter = {'b':0, 't':1, 'e':2, 'm':3}\n",
    "    return [converter[article_type] for article_type in y]\n",
    "\n",
    "class MyRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False, rnn_bias=True, PATH=None):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.dw, self.dh = dw, dh\n",
    "        if PATH:\n",
    "            self.embed = nn.from_pretrained(PATH)\n",
    "        else:\n",
    "            m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "            nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "            nn.init.constant_(m.weight[0], 0)\n",
    "            self.embed = m\n",
    "        self.rnn = nn.RNN(dw, dh, bias=rnn_bias, num_layers=num_layers, bidirectional=bidirectional, batch_first=True, nonlinearity='tanh')\n",
    "        if bidirectional:\n",
    "            self.linear = nn.Linear(2 * dh, L, bias=True)\n",
    "        else:\n",
    "            self.linear = nn.Linear(dh, L, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "    '''\n",
    "    x: ids (not one hot vector)\n",
    "    '''\n",
    "    def forward(self, x, x_lengths):\n",
    "        x = self.embed(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, hidden = self.rnn(packed)\n",
    "        hidden = hidden.view(self.num_layers, 2 if self.bidirectional else 1, -1, self.dh)\n",
    "        last_hidden = hidden[-1]\n",
    "        if self.bidirectional:\n",
    "            x = self.linear(torch.cat([last_hidden[0], last_hidden[1]], dim=1))\n",
    "        else:\n",
    "            x = self.linear(last_hidden[0])\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "\n",
    "class MyDataSets(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = [torch.LongTensor(data) for data in x]\n",
    "        self.y = [torch.LongTensor([data]) for data in y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    x = [data[0] for data in batch]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = torch.LongTensor([data[1] for data in batch])\n",
    "    lengths = torch.LongTensor([len(data[0]) for data in batch])\n",
    "    return x, y, lengths\n",
    "    \n",
    "def execution(data_x, data_y, op, criterion, model, batch_size=1, is_train=True, use_gpu=False):\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "    ndata = len(data_x)\n",
    "    dataset = MyDataSets(data_x, data_y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    sum_loss, acc_score = 0, 0\n",
    "    for batch_x, batch_y, batch_lengths in data_loader:\n",
    "        op.zero_grad()\n",
    "        out = model(batch_x, batch_lengths)\n",
    "        loss = criterion(out, batch_y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        acc_score += np.sum((pred == batch_y).cpu().detach().numpy())\n",
    "    return sum_loss / ndata, acc_score / ndata * 100\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess = PreprocessTools('work/vocab.json')\n",
    "    \n",
    "    x_train = load_file_json('work/train_x.json')['data']\n",
    "    y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "    x_valid = load_file_json('work/valid_x.json')['data']\n",
    "    y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "    x_test = load_file_json('work/test_x.json')['data']\n",
    "    y_test = np.asarray(load_file_json('work/test_y.json')['data'])\n",
    "\n",
    "\n",
    "    vocab_size = preprocess.vocab_size\n",
    "    torch.manual_seed(1234)\n",
    "    model = MyRNN(vocab_size, dw=300, dh=50, L=4, num_layers=2, bidirectional=True)\n",
    "    ntrain = len(x_train)\n",
    "    nepoch = 10 \n",
    "    batch_size = 128 \n",
    "    op = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.NLLLoss() \n",
    "\n",
    "    train_writer = SummaryWriter(log_dir='./work/logs/train')\n",
    "    valid_writer = SummaryWriter(log_dir='./work/logs/valid')\n",
    "    logger = list()\n",
    "    for epoch in tqdm.tqdm(range(nepoch)):\n",
    "        train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "        train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "        train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "        with torch.no_grad():\n",
    "            valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "            valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "            valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "        logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "        print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    train_writer.close()\n",
    "    valid_writer.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_size:1\telapsed_time:361.9438319206238 [sec]\n",
    "batch_size:2\telapsed_time:195.748544216156 [sec]\n",
    "batch_size:4\telapsed_time:100.17491579055786 [sec]\n",
    "batch_size:8\telapsed_time:52.92460918426514 [sec]\n",
    "batch_size:16\telapsed_time:28.268762826919556 [sec]\n",
    "batch_size:32\telapsed_time:15.605499982833862 [sec]\n",
    "batch_size:64\telapsed_time:10.040234327316284 [sec]\n",
    "batch_size:128\telapsed_time:6.472395420074463 [sec]\n",
    "batch_size:256\telapsed_time:4.624249458312988 [sec]\n",
    "batch_size:512\telapsed_time:3.524317502975464 [sec]\n",
    "batch_size:1024\telapsed_time:3.101412773132324 [sec]\n",
    "batch_size:2048\telapsed_time:2.749919891357422 [sec]\n",
    "batch_size:4096\telapsed_time:2.6615805625915527 [sec]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_size:1\telapsed_time:1016.4217035770416 [sec]\n",
    "batch_size:2\telapsed_time:527.0541796684265 [sec]\n",
    "batch_size:4\telapsed_time:291.180860042572 [sec]\n",
    "batch_size:8\telapsed_time:147.82439351081848 [sec]\n",
    "batch_size:16\telapsed_time:82.42432832717896 [sec]\n",
    "batch_size:32\telapsed_time:44.86296892166138 [sec]\n",
    "batch_size:64\telapsed_time:28.106043815612793 [sec]\n",
    "batch_size:128\telapsed_time:17.97643756866455 [sec]\n",
    "batch_size:256\telapsed_time:13.771385669708252 [sec]\n",
    "batch_size:512\telapsed_time:11.08540654182434 [sec]\n",
    "batch_size:1024\telapsed_time:9.829963445663452 [sec]\n",
    "batch_size:2048\telapsed_time:15.359896421432495 [sec]\n",
    "batch_size:4096\telapsed_time:12.083300590515137 [sec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 84. 単語ベクトルの導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyRNN(vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False)\n",
    "model.update_from_word2vec(w2v, preprocess.word_transformer)\n",
    "ntrain = len(x_train)\n",
    "nepoch = 10 \n",
    "batch_size = 128 \n",
    "#op = optim.SGD(model.parameters(), lr=0.1)\n",
    "op = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRNN(\n",
      "  (embed): Embedding(9866, 300, padding_idx=0)\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (linear): Linear(in_features=50, out_features=4, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae01c67ef5742b8815efbf8244972b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 1.0841823437687164, 'train_acc': 50.468164794007485, 'valid_loss': 0.8035261861840437, 'valid_acc': 73.85767790262172}\n",
      "{'epoch': 1, 'train_loss': 0.5676666072245394, 'train_acc': 79.05430711610487, 'valid_loss': 0.46076675584253746, 'valid_acc': 83.07116104868913}\n",
      "{'epoch': 2, 'train_loss': 0.3240814438919896, 'train_acc': 87.34082397003745, 'valid_loss': 0.45489098219835805, 'valid_acc': 83.59550561797752}\n",
      "{'epoch': 3, 'train_loss': 0.2447720053490628, 'train_acc': 89.84082397003745, 'valid_loss': 0.41074375164196286, 'valid_acc': 85.0187265917603}\n",
      "{'epoch': 4, 'train_loss': 0.20364386380723354, 'train_acc': 90.53370786516854, 'valid_loss': 0.4525810466946734, 'valid_acc': 85.2434456928839}\n",
      "{'epoch': 5, 'train_loss': 0.16895339485634578, 'train_acc': 91.44194756554307, 'valid_loss': 0.39696553704443943, 'valid_acc': 85.76779026217228}\n",
      "{'epoch': 6, 'train_loss': 0.14188933785488542, 'train_acc': 92.61235955056179, 'valid_loss': 0.4342009868291433, 'valid_acc': 85.76779026217228}\n",
      "{'epoch': 7, 'train_loss': 0.13013480573557737, 'train_acc': 94.20411985018727, 'valid_loss': 0.4601046518440104, 'valid_acc': 86.29213483146067}\n",
      "{'epoch': 8, 'train_loss': 0.10268918731239404, 'train_acc': 96.21722846441948, 'valid_loss': 0.6388556601402912, 'valid_acc': 86.66666666666667}\n",
      "{'epoch': 9, 'train_loss': 0.09355827285890722, 'train_acc': 97.09737827715355, 'valid_loss': 0.6587920381335284, 'valid_acc': 87.11610486891385}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_writer = SummaryWriter(log_dir='./work/logs/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/valid')\n",
    "logger = list()\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 85. 双方向RNN・多層化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False, dropout=0.0, rnn_bias=True, PATH=None):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.dw, self.dh = dw, dh\n",
    "        if PATH:\n",
    "            self.embed = nn.from_pretrained(PATH)\n",
    "        else:\n",
    "            m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "            nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "            nn.init.constant_(m.weight[0], 0)\n",
    "            self.embed = m\n",
    "        self.rnn = nn.RNN(dw, dh, bias=rnn_bias, num_layers=num_layers, bidirectional=bidirectional, batch_first=True, nonlinearity='relu', dropout=dropout)\n",
    "        if bidirectional:\n",
    "            self.linear = nn.Linear(2 * dh, L, bias=True)\n",
    "        else:\n",
    "            self.linear = nn.Linear(dh, L, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "    '''\n",
    "    x: ids (not one hot vector)\n",
    "    '''\n",
    "    def forward(self, x, x_lengths):\n",
    "        x = self.embed(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, hidden = self.rnn(packed)\n",
    "        hidden = hidden.view(self.num_layers, 2 if self.bidirectional else 1, -1, self.dh)\n",
    "        last_hidden = hidden[-1]\n",
    "        if self.bidirectional:\n",
    "            x = self.linear(torch.cat([last_hidden[0], last_hidden[1]], dim=1))\n",
    "        else:\n",
    "            x = self.linear(last_hidden[0])\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "    \n",
    "    def update_from_word2vec(self, w2v, transformer):\n",
    "        for word, idx in transformer.items():\n",
    "            with torch.no_grad():\n",
    "                if word in w2v:\n",
    "                    self.embed.weight[idx].copy_(torch.from_numpy(w2v[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169d89be6b054088bc48ceaaf605a221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 0.5621581544813592, 'train_acc': 78.89513108614233, 'valid_loss': 0.4073159295521425, 'valid_acc': 84.49438202247191}\n",
      "{'epoch': 1, 'train_loss': 0.2761372826965561, 'train_acc': 90.54307116104869, 'valid_loss': 0.32408394719777484, 'valid_acc': 89.21348314606742}\n",
      "{'epoch': 2, 'train_loss': 0.15865980392314968, 'train_acc': 94.85955056179776, 'valid_loss': 0.38206236023134954, 'valid_acc': 89.58801498127342}\n",
      "{'epoch': 3, 'train_loss': 0.09868653422214566, 'train_acc': 96.76029962546816, 'valid_loss': 0.31603187080402945, 'valid_acc': 90.0374531835206}\n",
      "{'epoch': 4, 'train_loss': 0.06191470492352149, 'train_acc': 98.01498127340824, 'valid_loss': 0.354854448614049, 'valid_acc': 90.187265917603}\n",
      "{'epoch': 5, 'train_loss': 0.04207206202970908, 'train_acc': 98.76404494382022, 'valid_loss': 0.423645007476378, 'valid_acc': 90.11235955056179}\n",
      "{'epoch': 6, 'train_loss': 0.029736371623694004, 'train_acc': 99.14794007490637, 'valid_loss': 0.43740582595603744, 'valid_acc': 90.48689138576779}\n",
      "{'epoch': 7, 'train_loss': 0.024143751769253378, 'train_acc': 99.26029962546816, 'valid_loss': 0.47447261707613086, 'valid_acc': 90.2621722846442}\n",
      "{'epoch': 8, 'train_loss': 0.016981003151627514, 'train_acc': 99.50374531835206, 'valid_loss': 0.5276704940679815, 'valid_acc': 90.187265917603}\n",
      "{'epoch': 9, 'train_loss': 0.014470773713300458, 'train_acc': 99.5692883895131, 'valid_loss': 0.578131686792838, 'valid_acc': 90.11235955056179}\n",
      "{'epoch': 10, 'train_loss': 0.012267955881805065, 'train_acc': 99.61610486891385, 'valid_loss': 0.609489129321852, 'valid_acc': 89.9625468164794}\n",
      "{'epoch': 11, 'train_loss': 0.011763852185915025, 'train_acc': 99.6441947565543, 'valid_loss': 0.6221801364354873, 'valid_acc': 90.56179775280899}\n",
      "{'epoch': 12, 'train_loss': 0.009190920337752094, 'train_acc': 99.7565543071161, 'valid_loss': 0.6349943097164569, 'valid_acc': 90.4119850187266}\n",
      "{'epoch': 13, 'train_loss': 0.007475100706369801, 'train_acc': 99.7940074906367, 'valid_loss': 0.6437471562110529, 'valid_acc': 90.56179775280899}\n",
      "{'epoch': 14, 'train_loss': 0.006996589585831772, 'train_acc': 99.76591760299625, 'valid_loss': 0.6853665305434095, 'valid_acc': 90.4119850187266}\n",
      "{'epoch': 15, 'train_loss': 0.006662463872041204, 'train_acc': 99.76591760299625, 'valid_loss': 0.7463206511088525, 'valid_acc': 90.2621722846442}\n",
      "{'epoch': 16, 'train_loss': 0.006095683014465929, 'train_acc': 99.82209737827715, 'valid_loss': 0.7572968491007773, 'valid_acc': 90.187265917603}\n",
      "{'epoch': 17, 'train_loss': 0.005496314409352867, 'train_acc': 99.8501872659176, 'valid_loss': 0.7394630722338787, 'valid_acc': 90.2621722846442}\n",
      "{'epoch': 18, 'train_loss': 0.005264996301163902, 'train_acc': 99.8501872659176, 'valid_loss': 0.7731378175122907, 'valid_acc': 90.187265917603}\n",
      "{'epoch': 19, 'train_loss': 0.005062298988945231, 'train_acc': 99.8689138576779, 'valid_loss': 0.775320291653108, 'valid_acc': 90.48689138576779}\n",
      "{'epoch': 20, 'train_loss': 0.0042176578875826425, 'train_acc': 99.8689138576779, 'valid_loss': 0.8054808324642395, 'valid_acc': 89.9625468164794}\n",
      "{'epoch': 21, 'train_loss': 0.0036168538353995383, 'train_acc': 99.8876404494382, 'valid_loss': 0.8166512896282396, 'valid_acc': 90.3370786516854}\n",
      "{'epoch': 22, 'train_loss': 0.003712566942216354, 'train_acc': 99.87827715355805, 'valid_loss': 0.8189672736639387, 'valid_acc': 90.11235955056179}\n",
      "{'epoch': 23, 'train_loss': 0.004103519654682792, 'train_acc': 99.8689138576779, 'valid_loss': 0.8435167791021897, 'valid_acc': 90.3370786516854}\n",
      "{'epoch': 24, 'train_loss': 0.004758185066966417, 'train_acc': 99.8689138576779, 'valid_loss': 0.8165045719914669, 'valid_acc': 90.4119850187266}\n",
      "{'epoch': 25, 'train_loss': 0.0038886942660910414, 'train_acc': 99.8876404494382, 'valid_loss': 0.7920553676867753, 'valid_acc': 90.2621722846442}\n",
      "{'epoch': 26, 'train_loss': 0.003254001973936341, 'train_acc': 99.91573033707866, 'valid_loss': 0.8587557230995836, 'valid_acc': 90.0374531835206}\n",
      "{'epoch': 27, 'train_loss': 0.0031458503124565723, 'train_acc': 99.8876404494382, 'valid_loss': 0.8222025812788403, 'valid_acc': 90.2621722846442}\n",
      "{'epoch': 28, 'train_loss': 0.003482625587260846, 'train_acc': 99.90636704119851, 'valid_loss': 0.8781661164894533, 'valid_acc': 89.9625468164794}\n",
      "{'epoch': 29, 'train_loss': 0.003453359592571673, 'train_acc': 99.90636704119851, 'valid_loss': 0.8485437124855956, 'valid_acc': 90.3370786516854}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyRNN(vocab_size, dw=300, dh=50, L=4, num_layers=2, bidirectional=True, dropout=0.6)\n",
    "model.update_from_word2vec(w2v, preprocess.word_transformer)\n",
    "ntrain = len(x_train)\n",
    "nepoch = 30 \n",
    "batch_size = 128 \n",
    "op = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0.001)\n",
    "criterion = nn.NLLLoss() \n",
    "train_writer = SummaryWriter(log_dir='./work/logs/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/valid')\n",
    "logger = list()\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 86. 畳み込みニューラルネットワーク (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = load_file_json('work/train_x.json')['data']\n",
    "y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "x_valid = load_file_json('work/valid_x.json')['data']\n",
    "y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "x_test = load_file_json('work/test_x.json')['data']\n",
    "y_test = np.asarray(load_file_json('work/test_y.json')['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "dw, dh, w_sz = 300, 50, 3\n",
    "torch.manual_seed(1234)\n",
    "embed = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "kernel_size = [w_sz, dw]\n",
    "cnn = nn.Conv2d(in_channels=1, out_channels=dh, kernel_size=kernel_size, padding=(w_sz-2, 0), stride=1)\n",
    "g = nn.ReLU()\n",
    "pool = torch.max\n",
    "linear = nn.Linear(dh, 4, bias=True)\n",
    "softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "input = torch.LongTensor([[[0, 1, 2, 3, 4]], [[5, 6, 7, 8, 9]]])\n",
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 3, 300])\n"
     ]
    }
   ],
   "source": [
    "print(cnn.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0314, -1.1941, -1.7095, -1.8355],\n",
      "        [-1.1559, -0.9727, -1.7407, -2.0267]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.FloatTensor([[[1,1,1,0,0]], [[1,1,1,1,0]]])\n",
    "x = embed(input)\n",
    "x = cnn(x)\n",
    "x = x.view(x.shape[:3])\n",
    "x = x * mask\n",
    "x = g(x)\n",
    "x = pool(x, dim=2).values\n",
    "x = linear(x)\n",
    "x = softmax(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 87. 確率的勾配降下法によるCNNの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, w_sz=3, L=4, dropout=0.0):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.dw, self.dh, self.w_sz = dw, dh, w_sz\n",
    "        \n",
    "        m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "        nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "        nn.init.constant_(m.weight[0], 0)\n",
    "        self.embed = m\n",
    "        \n",
    "        self.cnn = nn.Conv2d(in_channels=1, out_channels=dh, kernel_size=[w_sz, dw], padding=(w_sz-2, 0), stride=1)\n",
    "        self.g = nn.ReLU()\n",
    "        self.pool = torch.max\n",
    "        self.linear = nn.Linear(dh, 4, bias=True)\n",
    "        self.dropout = nn.Dropout2d(p=dropout)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, x_lengths):\n",
    "        x = self.embed(x)\n",
    "        x = self.cnn(x)\n",
    "        x = self.g(x)\n",
    "        x = x.view(x.shape[:3])\n",
    "        x = pool(x, dim=2).values\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "    \n",
    "    def update_from_word2vec(self, w2v, transformer):\n",
    "        for word, idx in transformer.items():\n",
    "            with torch.no_grad():\n",
    "                if word in w2v:\n",
    "                    self.embed.weight[idx].copy_(torch.from_numpy(w2v[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSets(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = [torch.LongTensor(data) for data in x]\n",
    "        self.y = [torch.LongTensor([data]) for data in y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x = [data[0] for data in batch]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = torch.LongTensor([data[1] for data in batch])\n",
    "    lengths = torch.LongTensor([len(data[0]) for data in batch])\n",
    "    x = x.view(-1, 1, torch.max(lengths))\n",
    "    return x, y, lengths\n",
    "\n",
    "def execution(data_x, data_y, op, criterion, model, batch_size=1, is_train=True, use_gpu=False):\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "    ndata = len(data_x)\n",
    "    dataset = MyDataSets(data_x, data_y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    sum_loss, acc_score = 0, 0\n",
    "    for batch_x, batch_y, batch_lengths in data_loader:\n",
    "        op.zero_grad()\n",
    "        out = model(batch_x, batch_lengths)\n",
    "        loss = criterion(out, batch_y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        acc_score += np.sum((pred == batch_y).cpu().detach().numpy())\n",
    "    return sum_loss / ndata, acc_score / ndata * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cead52d44c4b8994725797e6e615f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d(): argument 'input' (position 1) must be Tensor, not PackedSequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-266-95aa7e6a5dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-260-180835fdfc5c>\u001b[0m in \u001b[0;36mexecution\u001b[0;34m(data_x, data_y, op, criterion, model, batch_size, is_train, use_gpu)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_lengths\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-264-9f8fd4071316>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, x_lengths)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d(): argument 'input' (position 1) must be Tensor, not PackedSequence"
     ]
    }
   ],
   "source": [
    "x_train = load_file_json('work/train_x.json')['data']\n",
    "y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "x_valid = load_file_json('work/valid_x.json')['data']\n",
    "y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "x_test = load_file_json('work/test_x.json')['data']\n",
    "y_test = np.asarray(load_file_json('work/test_y.json')['data'])\n",
    "\n",
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyCNN(vocab_size, dw=300, dh=50, w_sz=3, L=4)\n",
    "nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "ntrain = len(x_train)\n",
    "nepoch = 10 \n",
    "batch_size = 1\n",
    "op = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.NLLLoss() \n",
    "\n",
    "train_writer = SummaryWriter(log_dir='./work/logs/cnn/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/cnn/valid')\n",
    "logger = list()\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 88. パラメータチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, w_sz=3, L=4, dropout=0.0):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.dw, self.dh, self.w_sz = dw, dh, w_sz\n",
    "        \n",
    "        m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "        nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "        nn.init.constant_(m.weight[0], 0)\n",
    "        self.embed = m\n",
    "        \n",
    "        self.cnn = nn.Conv2d(in_channels=1, out_channels=dh, kernel_size=[w_sz, dw], padding=(w_sz-2, 0), stride=1)\n",
    "        self.g = nn.ReLU()\n",
    "        self.pool = torch.max\n",
    "        self.linear = nn.Linear(dh, 4, bias=True)\n",
    "        self.dropout = nn.Dropout2d(p=0.2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "    def forward(self, x, x_lengths):\n",
    "        max_len = x_lengths.max()\n",
    "        mask = torch.FloatTensor([[[ 1.0 if i < seq_len else 0.0 for i in range(max_len)]] for seq_len in x_lengths])\n",
    "        x = self.embed(x)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.shape[:3])\n",
    "        x = x * mask\n",
    "        \n",
    "        \n",
    "        x = self.g(x)\n",
    "        x = self.dropout(x)\n",
    "        x = pool(x, dim=2).values\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "    \n",
    "    def update_from_word2vec(self, w2v, transformer):\n",
    "        for word, idx in transformer.items():\n",
    "            with torch.no_grad():\n",
    "                if word in w2v:\n",
    "                    self.embed.weight[idx].copy_(torch.from_numpy(w2v[word]))\n",
    "                    \n",
    "class MyDataSets(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = [torch.LongTensor(data) for data in x]\n",
    "        self.y = [torch.LongTensor([data]) for data in y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x = [data[0] for data in batch]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = torch.LongTensor([data[1] for data in batch])\n",
    "    lengths = torch.LongTensor([len(data[0]) for data in batch])\n",
    "    x = x.view(-1, 1, torch.max(lengths))\n",
    "    return x, y, lengths\n",
    "\n",
    "def execution(data_x, data_y, op, criterion, model, batch_size=1, is_train=True, use_gpu=False):\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "    ndata = len(data_x)\n",
    "    dataset = MyDataSets(data_x, data_y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    sum_loss, acc_score = 0, 0\n",
    "    for batch_x, batch_y, batch_lengths in data_loader:\n",
    "        op.zero_grad()\n",
    "        out = model(batch_x, batch_lengths)\n",
    "        loss = criterion(out, batch_y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        acc_score += np.sum((pred == batch_y).cpu().detach().numpy())\n",
    "    return sum_loss / ndata, acc_score / ndata * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53a42f3a1aa4159b47554430f5b99b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 0.5757558084605785, 'train_acc': 77.11610486891387, 'valid_loss': 0.3003921103834659, 'valid_acc': 91.01123595505618}\n",
      "{'epoch': 1, 'train_loss': 0.3131626569376456, 'train_acc': 86.86329588014982, 'valid_loss': 0.24776309576820346, 'valid_acc': 92.20973782771536}\n",
      "{'epoch': 2, 'train_loss': 0.24898011490200342, 'train_acc': 88.82022471910112, 'valid_loss': 0.2316487668605333, 'valid_acc': 92.35955056179776}\n",
      "{'epoch': 3, 'train_loss': 0.20590350836627047, 'train_acc': 90.25280898876404, 'valid_loss': 0.22737948575269865, 'valid_acc': 92.43445692883896}\n",
      "{'epoch': 4, 'train_loss': 0.18683609850844193, 'train_acc': 90.87078651685393, 'valid_loss': 0.22482745501767384, 'valid_acc': 92.73408239700375}\n",
      "{'epoch': 5, 'train_loss': 0.16657901802536254, 'train_acc': 91.03932584269663, 'valid_loss': 0.22434093769123492, 'valid_acc': 92.58426966292134}\n",
      "{'epoch': 6, 'train_loss': 0.14319769961110662, 'train_acc': 92.43445692883896, 'valid_loss': 0.22556115971522384, 'valid_acc': 92.88389513108615}\n",
      "{'epoch': 7, 'train_loss': 0.1409806786739871, 'train_acc': 92.09737827715357, 'valid_loss': 0.2287044880318731, 'valid_acc': 92.73408239700375}\n",
      "{'epoch': 8, 'train_loss': 0.13260559091407262, 'train_acc': 92.02247191011236, 'valid_loss': 0.2317976978722583, 'valid_acc': 92.73408239700375}\n",
      "{'epoch': 9, 'train_loss': 0.12465392889154985, 'train_acc': 92.64044943820224, 'valid_loss': 0.23459397989712405, 'valid_acc': 92.73408239700375}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train = load_file_json('work/train_x.json')['data']\n",
    "y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "x_valid = load_file_json('work/valid_x.json')['data']\n",
    "y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "x_test = load_file_json('work/test_x.json')['data']\n",
    "y_test = np.asarray(load_file_json('work/test_y.json')['data'])\n",
    "\n",
    "\n",
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyCNN(vocab_size, dw=300, dh=50, w_sz=3, L=4)\n",
    "model.update_from_word2vec(w2v, preprocess.word_transformer)\n",
    "nepoch = 10 \n",
    "batch_size = 256 \n",
    "op = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0.001)\n",
    "criterion = nn.NLLLoss() \n",
    "\n",
    "\n",
    "train_writer = SummaryWriter(log_dir='./work/logs/cnn/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/cnn/valid')\n",
    "logger = list()\n",
    "max_valid = -1\n",
    "max_model_param = None\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "        \n",
    "    if max_valid < valid_acc:\n",
    "        max_valid = valid_acc\n",
    "        max_model_param = model.state_dict()\n",
    "        \n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.53558052434457\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(max_model_param)\n",
    "with torch.no_grad():\n",
    "    test_loss, test_acc = execution(x_test, y_test, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "    print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCl(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, w_sz=3, L=4, num_layers=1, bidirectional=False, dropout=0.2):\n",
    "        super(MyCl, self).__init__()\n",
    "        self.dw, self.dh, self.w_sz = dw, dh, w_sz\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(dw, dh, num_layers=num_layers, bidirectional=bidirectional, batch_first=True, nonlinearity='relu', dropout=dropout)\n",
    "        \n",
    "        m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "        nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "        nn.init.constant_(m.weight[0], 0)\n",
    "        self.embed = m\n",
    "        \n",
    "        self.cnn = nn.Conv2d(in_channels=1, out_channels=dh, kernel_size=[w_sz, 2 * dh], padding=(w_sz-2, 0), stride=1)\n",
    "        self.g = nn.ReLU()\n",
    "        self.pool = torch.max\n",
    "        self.linear = nn.Linear(dh, 4, bias=True)\n",
    "        self.dropout = nn.Dropout2d(p=dropout)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "        \n",
    "    def forward(self, x, x_lengths):\n",
    "        x = self.embed(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "        x, _ = self.rnn(packed)\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        max_len = x_lengths.max()\n",
    "        mask = torch.FloatTensor([[ [1.0] if i < seq_len else [0.0] for i in range(max_len)] for seq_len in x_lengths])\n",
    "        x = x * mask\n",
    "        x = x.view(-1, 1, max_len, 2 * dh)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.shape[:3])\n",
    "        x = self.g(x)\n",
    "        x = self.dropout(x)\n",
    "        x = pool(x, dim=2).values\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "    \n",
    "    def update_from_word2vec(self, w2v, transformer):\n",
    "        for word, idx in transformer.items():\n",
    "            with torch.no_grad():\n",
    "                if word in w2v:\n",
    "                    self.embed.weight[idx].copy_(torch.from_numpy(w2v[word]))\n",
    "\n",
    "                    \n",
    "class MyDataSets(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = [torch.LongTensor(data) for data in x]\n",
    "        self.y = [torch.LongTensor([data]) for data in y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x = [data[0] for data in batch]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = torch.LongTensor([data[1] for data in batch])\n",
    "    lengths = torch.LongTensor([len(data[0]) for data in batch])\n",
    "    return x, y, lengths\n",
    "\n",
    "def execution(data_x, data_y, op, criterion, model, batch_size=1, is_train=True, use_gpu=False):\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "    ndata = len(data_x)\n",
    "    dataset = MyDataSets(data_x, data_y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    sum_loss, acc_score = 0, 0\n",
    "    for batch_x, batch_y, batch_lengths in data_loader:\n",
    "        op.zero_grad()\n",
    "        out = model(batch_x, batch_lengths)\n",
    "        loss = criterion(out, batch_y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        acc_score += np.sum((pred == batch_y).cpu().detach().numpy())\n",
    "    return sum_loss / ndata, acc_score / ndata * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70c19affbe44b17a631fcbc56609ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 0.7617954755990246, 'train_acc': 64.78464419475655, 'valid_loss': 0.45093154049991224, 'valid_acc': 82.47191011235955}\n",
      "{'epoch': 1, 'train_loss': 0.5248867922731106, 'train_acc': 74.05430711610487, 'valid_loss': 0.386541759141822, 'valid_acc': 84.49438202247191}\n",
      "{'epoch': 2, 'train_loss': 0.43306670436698397, 'train_acc': 76.45131086142321, 'valid_loss': 0.3535459771584929, 'valid_acc': 85.76779026217228}\n",
      "{'epoch': 3, 'train_loss': 0.35876306201634783, 'train_acc': 79.70973782771536, 'valid_loss': 0.29925455965576103, 'valid_acc': 89.9625468164794}\n",
      "{'epoch': 4, 'train_loss': 0.2839015465997132, 'train_acc': 82.7621722846442, 'valid_loss': 0.31662483724315515, 'valid_acc': 90.86142322097378}\n",
      "{'epoch': 5, 'train_loss': 0.23849713913956833, 'train_acc': 84.70973782771536, 'valid_loss': 0.4425045225839043, 'valid_acc': 90.56179775280899}\n",
      "{'epoch': 6, 'train_loss': 0.21623792997683478, 'train_acc': 85.07490636704121, 'valid_loss': 0.41337351468618444, 'valid_acc': 91.16104868913858}\n",
      "{'epoch': 7, 'train_loss': 0.20459032019649107, 'train_acc': 85.62734082397003, 'valid_loss': 0.503964547226938, 'valid_acc': 91.31086142322098}\n",
      "{'epoch': 8, 'train_loss': 0.1931200065974439, 'train_acc': 85.86142322097379, 'valid_loss': 0.49794238486540005, 'valid_acc': 91.08614232209737}\n",
      "{'epoch': 9, 'train_loss': 0.1955018829540367, 'train_acc': 85.77715355805243, 'valid_loss': 0.5288919447513108, 'valid_acc': 91.23595505617978}\n",
      "{'epoch': 10, 'train_loss': 0.18726633752553204, 'train_acc': 85.72097378277154, 'valid_loss': 0.5845789286081264, 'valid_acc': 91.23595505617978}\n",
      "{'epoch': 11, 'train_loss': 0.1802293274063296, 'train_acc': 86.34831460674157, 'valid_loss': 0.67468501410904, 'valid_acc': 90.93632958801498}\n",
      "{'epoch': 12, 'train_loss': 0.18166697998618364, 'train_acc': 86.01123595505618, 'valid_loss': 0.6607835679018542, 'valid_acc': 91.16104868913858}\n",
      "{'epoch': 13, 'train_loss': 0.184954794817203, 'train_acc': 85.73970037453184, 'valid_loss': 0.7500759056891395, 'valid_acc': 91.16104868913858}\n",
      "{'epoch': 14, 'train_loss': 0.1766696358776271, 'train_acc': 86.47003745318352, 'valid_loss': 0.7440189006846496, 'valid_acc': 91.16104868913858}\n",
      "{'epoch': 15, 'train_loss': 0.1744447665491354, 'train_acc': 86.41385767790261, 'valid_loss': 0.7751740882012728, 'valid_acc': 91.01123595505618}\n",
      "{'epoch': 16, 'train_loss': 0.1704878854171167, 'train_acc': 86.72284644194757, 'valid_loss': 0.8303259199478207, 'valid_acc': 90.86142322097378}\n",
      "{'epoch': 17, 'train_loss': 0.16836574651328812, 'train_acc': 86.45131086142322, 'valid_loss': 0.8989630131239301, 'valid_acc': 91.08614232209737}\n",
      "{'epoch': 18, 'train_loss': 0.1731133687139004, 'train_acc': 86.20786516853933, 'valid_loss': 0.9404611070727588, 'valid_acc': 90.93632958801498}\n",
      "{'epoch': 19, 'train_loss': 0.17129373890295457, 'train_acc': 86.12359550561798, 'valid_loss': 1.0660272668809927, 'valid_acc': 91.01123595505618}\n",
      "{'epoch': 20, 'train_loss': 0.16814394048537207, 'train_acc': 86.76966292134831, 'valid_loss': 0.859165332804012, 'valid_acc': 91.16104868913858}\n",
      "{'epoch': 21, 'train_loss': 0.17151649962650256, 'train_acc': 86.17041198501873, 'valid_loss': 0.9326133778032739, 'valid_acc': 91.08614232209737}\n",
      "{'epoch': 22, 'train_loss': 0.17227362345220446, 'train_acc': 86.14232209737828, 'valid_loss': 0.9722493578432204, 'valid_acc': 91.46067415730337}\n",
      "{'epoch': 23, 'train_loss': 0.17383089810051722, 'train_acc': 85.86142322097379, 'valid_loss': 0.9818114571803518, 'valid_acc': 91.08614232209737}\n",
      "{'epoch': 24, 'train_loss': 0.1739653533875719, 'train_acc': 85.87078651685394, 'valid_loss': 1.0070087960596834, 'valid_acc': 91.01123595505618}\n",
      "{'epoch': 25, 'train_loss': 0.16850095216031377, 'train_acc': 86.34831460674157, 'valid_loss': 1.0320392784107937, 'valid_acc': 91.38576779026218}\n",
      "{'epoch': 26, 'train_loss': 0.167805774749888, 'train_acc': 86.17041198501873, 'valid_loss': 1.032887939031651, 'valid_acc': 91.16104868913858}\n",
      "{'epoch': 27, 'train_loss': 0.17548278173927065, 'train_acc': 86.02059925093633, 'valid_loss': 1.0469414906555348, 'valid_acc': 91.16104868913858}\n",
      "{'epoch': 28, 'train_loss': 0.16723449256982698, 'train_acc': 86.52621722846442, 'valid_loss': 1.1431751954477378, 'valid_acc': 91.08614232209737}\n",
      "{'epoch': 29, 'train_loss': 0.16668684780597687, 'train_acc': 86.59176029962546, 'valid_loss': 1.1043180092443687, 'valid_acc': 91.16104868913858}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train = load_file_json('work/train_x.json')['data']\n",
    "y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "x_valid = load_file_json('work/valid_x.json')['data']\n",
    "y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "x_test = load_file_json('work/test_x.json')['data']\n",
    "y_test = np.asarray(load_file_json('work/test_y.json')['data'])\n",
    "\n",
    "\n",
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyCl(vocab_size, dw=300, dh=50, w_sz=3, L=4, num_layers=2, bidirectional=True, dropout=0.3)\n",
    "model.update_from_word2vec(w2v, preprocess.word_transformer)\n",
    "nepoch = 30 \n",
    "batch_size = 256\n",
    "op = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0.001)\n",
    "criterion = nn.NLLLoss() \n",
    "\n",
    "\n",
    "train_writer = SummaryWriter(log_dir='./work/logs/cnn/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/cnn/valid')\n",
    "logger = list()\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 89. 事前学習済み言語モデルからの転移学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
