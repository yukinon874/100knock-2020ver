{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80. ID番号への変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from functools import reduce\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, mode='r') as f:\n",
    "        X = list()\n",
    "        Y = list()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            splited_line = line.split('\\t')\n",
    "            X.append(splited_line[0])\n",
    "            Y.append(splited_line[1])\n",
    "        return X, Y\n",
    "\n",
    "def save_file_json(path, data):\n",
    "    with open(path, mode='w') as out_file:\n",
    "        out_file.write(json.dumps(data)+'\\n')\n",
    "        \n",
    "def load_file_json(path):\n",
    "    with open(path, mode='r') as in_file:\n",
    "        data = json.load(in_file)\n",
    "    return data\n",
    "\n",
    "def chr2num(y):\n",
    "    converter = {'b':0, 't':1, 'e':2, 'm':3}\n",
    "    return [converter[article_type] for article_type in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessTools:\n",
    "    def __init__(self, vocab_path=None):\n",
    "        self.word_count = defaultdict(int)       \n",
    "        if vocab_path:\n",
    "            self.word_transformer = load_file_json(vocab_path)\n",
    "            self.vocab_size = len(self.word_transformer) + 1\n",
    "        else:\n",
    "            self.word_transformer = dict()\n",
    "            self.vocab_size = -1\n",
    "        \n",
    "    def tokenize(self, data):\n",
    "        return [[word for word in word_tokenize(txt)] for txt in data]\n",
    "\n",
    "    def make_word_transformar(self, train_data:list):\n",
    "        for data in train_data:\n",
    "            for word in data:\n",
    "                self.word_count[word] += 1\n",
    "        sorted_word_count = sorted(self.word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "        for idx, (word, count) in enumerate(sorted_word_count):\n",
    "            if count < 2:\n",
    "                break\n",
    "            else:\n",
    "                self.word_transformer[word] = idx + 1\n",
    "        self.vocab_size = len(self.word_transformer) + 1\n",
    "\n",
    "    def txt2ids(self, txt_list:list):\n",
    "        txt_ids = list()\n",
    "        for txt in txt_list:\n",
    "            ids = list()\n",
    "            for word in txt:\n",
    "                if word in self.word_transformer:\n",
    "                    ids.append(self.word_transformer[word])\n",
    "                else:\n",
    "                    ids.append(0)\n",
    "            txt_ids.append(ids)\n",
    "        return txt_ids\n",
    "\n",
    "\n",
    "    def ids2vec(self, txt_ids:list):\n",
    "        txt_vec = list()\n",
    "        identity = np.identity(self.vocab_size)\n",
    "        for ids in txt_ids:\n",
    "            txt_vec.append(identity[ids])\n",
    "        return txt_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = PreprocessTools()\n",
    "x_train, y_train = load_data('data/train.txt')\n",
    "x_valid, y_valid = load_data('data/valid.txt')\n",
    "x_test, y_test = load_data('data/test.txt')\n",
    "x_train = preprocess.tokenize(x_train)\n",
    "x_valid = preprocess.tokenize(x_valid)\n",
    "x_test = preprocess.tokenize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.make_word_transformar(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ids = preprocess.txt2ids(x_train)\n",
    "x_valid_ids = preprocess.txt2ids(x_valid)\n",
    "x_test_ids = preprocess.txt2ids(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kindred', 'Healthcare', 'to', 'buy', 'Gentiva', 'for', 'about', '$', '573', 'mln'] [5065, 3395, 2, 181, 3396, 13, 164, 19, 0, 220]\n",
      "['US', 'to', 'boost', 'ground', ',', 'naval', 'forces', 'in', 'NATO', 'countries'] [15, 2, 586, 3397, 1, 0, 4067, 6, 5066, 5067]\n",
      "['Robert', 'Pattinson', '-', 'Robert', 'Pattinson', 'Brushes', 'Off', 'Kristen', 'Stewart', \"'s\", 'Cheating', '...'] [237, 945, 11, 237, 945, 0, 385, 669, 1259, 4, 3398, 3]\n",
      "['Piers', 'Morgan', 'Delivers', 'One', 'Final', 'Blow', 'To', 'Gun', 'Violence', 'In', 'Last', 'Show'] [5068, 399, 6690, 185, 1074, 5069, 16, 3399, 2225, 20, 785, 161]\n",
      "['Here', 'We', 'Go', ':', \"'Star\", 'Wars', 'Episode', 'VII', \"'\", 'Kicks', 'Off', 'Filming', 'at', 'Pinewood'] [400, 196, 639, 7, 549, 210, 295, 587, 5, 5070, 385, 1371, 22, 0]\n",
      "['Amazon', 'gets', 'in', 'the', 'game', ':', 'Retailer', 'beats', 'Google', 'to', 'buy', 'hit', 'console', 'broadcasting', '...'] [169, 330, 6, 17, 1619, 7, 0, 609, 82, 2, 181, 245, 5071, 0, 3]\n",
      "['FOREX-Euro', 'retreats', 'as', 'ECB', 'steps', 'up', 'verbal', 'campaign'] [331, 5072, 9, 52, 588, 43, 6691, 2518]\n",
      "['UPDATE', '2-US', 'appeals', 'court', 'revives', 'Apple', 'patent', 'lawsuit', 'against', 'Google'] [10, 301, 6692, 332, 0, 109, 1783, 734, 232, 82]\n",
      "['Kendall', 'Jenner', 'shows', 'support', 'for', 'Kanye', 'West', 'in', 'Yeezus', 'tour', 'shirt'] [735, 333, 204, 786, 13, 87, 69, 6, 0, 702, 5073]\n",
      "['UPDATE', '1-S.Korea', \"'s\", 'state', 'health', 'insurer', 'tobacco', 'firms', 'for', 'damages'] [10, 0, 4, 3400, 880, 6693, 3401, 946, 13, 3402]\n"
     ]
    }
   ],
   "source": [
    "for word, ids in zip(x_train[:10], x_train_ids[:10]):\n",
    "    print(word, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9866\n"
     ]
    }
   ],
   "source": [
    "print(preprocess.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_json('work/vocab.json', preprocess.word_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 81. RNNによる予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hotにはしない\n",
    "#xtrain_vec = preprocess.ids2vec(xtrain_ids) \n",
    "#xvalid_vec = preprocess.ids2vec(xvalid_ids)\n",
    "#xtest_vec = preprocess.ids2vec(xtest_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = chr2num(y_train)\n",
    "y_valid = chr2num(y_valid)\n",
    "y_test = chr2num(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_json('work/train_x.json', {'data':x_train_ids})\n",
    "save_file_json('work/train_y.json', {'data': y_train})\n",
    "save_file_json('work/valid_x.json', {'data': x_valid_ids})\n",
    "save_file_json('work/valid_y.json', {'data': y_valid})\n",
    "save_file_json('work/test_x.json', {'data': x_test_ids})\n",
    "save_file_json('work/test_y.json', {'data': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = load_file_json('work/train_x.json')['data']\n",
    "y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "x_valid = load_file_json('work/valid_x.json')['data']\n",
    "y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "x_test = load_file_json('work/test_x.json')['data']\n",
    "y_test = np.asarray(load_file_json('work/test_y.json')['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs: input, h_0\n",
    "\n",
    "- input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details.\n",
    "\n",
    "- h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.\n",
    "\n",
    "## Outputs: output, h_n\n",
    "\n",
    "- output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.\n",
    "\n",
    "    For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.\n",
    "\n",
    "- h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n",
    "\n",
    "    Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "dw, dh = 300, 50\n",
    "torch.manual_seed(1234)\n",
    "embed = nn.Embedding(vocab_size, dw, padding_idx=0) #idx 0 は 0埋め\n",
    "rnn = nn.RNN(dw, dh, num_layers=2, bidirectional=False, batch_first=True)\n",
    "linear = nn.Linear(50, 4, bias=True)\n",
    "softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "input = torch.LongTensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 50])\n",
      "tensor([[ 0.0390,  0.4236,  0.2686, -0.6118],\n",
      "        [-0.0630, -0.0920,  0.1053,  0.0488]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.4466, -1.0620, -1.2170, -2.0974],\n",
      "        [-1.4523, -1.4813, -1.2840, -1.3405]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "output, hn = rnn(embed(input))\n",
    "print(hn.shape)\n",
    "x = linear(hn[-1])\n",
    "print(x)\n",
    "print(softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 82. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False, rnn_bias=True, PATH=None):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.dw, self.dh = dw, dh\n",
    "        if PATH:\n",
    "            self.embed = nn.from_pretrained(PATH)\n",
    "        else:\n",
    "            m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "            nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "            nn.init.constant_(m.weight[0], 0)\n",
    "            self.embed = m\n",
    "        self.rnn = nn.RNN(dw, dh, bias=rnn_bias, num_layers=num_layers, bidirectional=bidirectional, batch_first=True, nonlinearity='relu')\n",
    "        if bidirectional:\n",
    "            self.linear = nn.Linear(2 * dh, L, bias=True)\n",
    "        else:\n",
    "            self.linear = nn.Linear(dh, L, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "    '''\n",
    "    x: ids (not one hot vector)\n",
    "    '''\n",
    "    def forward(self, x, x_lengths):\n",
    "        x = self.embed(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, hidden = self.rnn(packed)\n",
    "        hidden = hidden.view(self.num_layers, 2 if self.bidirectional else 1, -1, self.dh)\n",
    "        last_hidden = hidden[-1]\n",
    "        if self.bidirectional:\n",
    "            x = self.linear(torch.cat([last_hidden[0], last_hidden[1]], dim=1))\n",
    "        else:\n",
    "            x = self.linear(last_hidden[0])\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "    \n",
    "    def update_from_word2vec(self, w2v, transformer):\n",
    "        for word, idx in transformer.items():\n",
    "            with torch.no_grad():\n",
    "                if word in w2v:\n",
    "                    self.embed.weight[idx].copy_(torch.from_numpy(w2v[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSets(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = [torch.LongTensor(data) for data in x]\n",
    "        self.y = [torch.LongTensor([data]) for data in y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    x = [data[0] for data in batch]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = torch.LongTensor([data[1] for data in batch])\n",
    "    lengths = torch.LongTensor([len(data[0]) for data in batch])\n",
    "    return x, y, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution(data_x, data_y, op, criterion, model, batch_size=1, is_train=True, use_gpu=False):\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "    ndata = len(data_x)\n",
    "    dataset = MyDataSets(data_x, data_y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    sum_loss, acc_score = 0, 0\n",
    "    for batch_x, batch_y, batch_lengths in data_loader:\n",
    "        op.zero_grad()\n",
    "        out = model(batch_x, batch_lengths)\n",
    "        loss = criterion(out, batch_y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        acc_score += np.sum((pred == batch_y).cpu().detach().numpy())\n",
    "    return sum_loss / ndata, acc_score / ndata * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = load_file_json('work/train_x.json')['data']\n",
    "y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "x_valid = load_file_json('work/valid_x.json')['data']\n",
    "y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "x_test = load_file_json('work/test_x.json')['data']\n",
    "y_test = np.asarray(load_file_json('work/test_y.json')['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyRNN(vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False)\n",
    "nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "ntrain = len(x_train)\n",
    "nepoch = 10 \n",
    "batch_size = 1\n",
    "op = optim.SGD(model.parameters(), lr=0.1)\n",
    "criterion = nn.NLLLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRNN(\n",
      "  (embed): Embedding(9866, 300, padding_idx=0)\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (linear): Linear(in_features=50, out_features=4, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b59464de05142ed854ae55cb40e29fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 1.067292882755822, 'train_acc': 57.3876404494382, 'valid_loss': 1.0038792444382574, 'valid_acc': 58.876404494382015}\n"
     ]
    }
   ],
   "source": [
    "train_writer = SummaryWriter(log_dir='./work/logs/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/valid')\n",
    "logger = list()\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 83. ミニバッチ化・GPU上での学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/gpu.py\n"
     ]
    }
   ],
   "source": [
    "%%file src/gpu.py\n",
    "\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class PreprocessTools:\n",
    "    def __init__(self, vocab_path=None):\n",
    "        self.word_count = defaultdict(int)       \n",
    "        if vocab_path:\n",
    "            self.word_transformer = load_file_json(vocab_path)\n",
    "            self.vocab_size = len(self.word_transformer) + 1\n",
    "        else:\n",
    "            self.word_transformer = defaultdict(int)\n",
    "            self.vocab_size = -1\n",
    "        \n",
    "    def tokenize(self, data):\n",
    "        return [[word for word in word_tokenize(txt)] for txt in data]\n",
    "\n",
    "    def make_word_transformar(self, train_data:list):\n",
    "        for data in train_data:\n",
    "            for word in data:\n",
    "                self.word_count[word] += 1\n",
    "        sorted_word_count = sorted(self.word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "        for idx, (word, count) in enumerate(sorted_word_count):\n",
    "            if count < 2:\n",
    "                break\n",
    "            else:\n",
    "                self.word_transformer[word] = idx + 1\n",
    "        self.vocab_size = len(self.word_transformer) + 1\n",
    "\n",
    "    def txt2ids(self, txt_list:list):\n",
    "        txt_ids = list()\n",
    "        for txt in txt_list:\n",
    "            ids = list()\n",
    "            for word in txt:\n",
    "                ids.append(self.word_transformer[word])\n",
    "            txt_ids.append(ids)\n",
    "        return txt_ids\n",
    "\n",
    "\n",
    "    def ids2vec(self, txt_ids:list):\n",
    "        txt_vec = list()\n",
    "        identity = np.identity(self.vocab_size)\n",
    "        for ids in txt_ids:\n",
    "            txt_vec.append(identity[ids])\n",
    "        return txt_vec\n",
    "    \n",
    "    \n",
    "def load_data(path):\n",
    "    with open(path, mode='r') as f:\n",
    "        X = list()\n",
    "        Y = list()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            splited_line = line.split('\\t')\n",
    "            X.append(splited_line[0])\n",
    "            Y.append(splited_line[1])\n",
    "        return X, Y\n",
    "\n",
    "def save_file_json(path, data):\n",
    "    with open(path, mode='w') as out_file:\n",
    "        out_file.write(json.dumps(data)+'\\n')\n",
    "        \n",
    "def load_file_json(path):\n",
    "    with open(path, mode='r') as in_file:\n",
    "        data = json.load(in_file)\n",
    "    return data\n",
    "\n",
    "def chr2num(y):\n",
    "    converter = {'b':0, 't':1, 'e':2, 'm':3}\n",
    "    return [converter[article_type] for article_type in y]\n",
    "\n",
    "class MyRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False, rnn_bias=True, PATH=None):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.dw, self.dh = dw, dh\n",
    "        if PATH:\n",
    "            self.embed = nn.from_pretrained(PATH)\n",
    "        else:\n",
    "            m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "            nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "            nn.init.constant_(m.weight[0], 0)\n",
    "            self.embed = m\n",
    "        self.rnn = nn.RNN(dw, dh, bias=rnn_bias, num_layers=num_layers, bidirectional=bidirectional, batch_first=True, nonlinearity='tanh')\n",
    "        if bidirectional:\n",
    "            self.linear = nn.Linear(2 * dh, L, bias=True)\n",
    "        else:\n",
    "            self.linear = nn.Linear(dh, L, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "    '''\n",
    "    x: ids (not one hot vector)\n",
    "    '''\n",
    "    def forward(self, x, x_lengths):\n",
    "        x = self.embed(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, hidden = self.rnn(packed)\n",
    "        hidden = hidden.view(self.num_layers, 2 if self.bidirectional else 1, -1, self.dh)\n",
    "        last_hidden = hidden[-1]\n",
    "        if self.bidirectional:\n",
    "            x = self.linear(torch.cat([last_hidden[0], last_hidden[1]], dim=1))\n",
    "        else:\n",
    "            x = self.linear(last_hidden[0])\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "\n",
    "class MyDataSets(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = [torch.LongTensor(data) for data in x]\n",
    "        self.y = [torch.LongTensor([data]) for data in y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    x = [data[0] for data in batch]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = torch.LongTensor([data[1] for data in batch])\n",
    "    lengths = torch.LongTensor([len(data[0]) for data in batch])\n",
    "    return x, y, lengths\n",
    "    \n",
    "def execution(data_x, data_y, op, criterion, model, batch_size=1, is_train=True, use_gpu=False):\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "    ndata = len(data_x)\n",
    "    dataset = MyDataSets(data_x, data_y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    sum_loss, acc_score = 0, 0\n",
    "    for batch_x, batch_y, batch_lengths in data_loader:\n",
    "        op.zero_grad()\n",
    "        out = model(batch_x, batch_lengths)\n",
    "        loss = criterion(out, batch_y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        acc_score += np.sum((pred == batch_y).cpu().detach().numpy())\n",
    "    return sum_loss / ndata, acc_score / ndata * 100\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess = PreprocessTools('work/vocab.json')\n",
    "    \n",
    "    x_train = load_file_json('work/train_x.json')['data']\n",
    "    y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "    x_valid = load_file_json('work/valid_x.json')['data']\n",
    "    y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "    x_test = load_file_json('work/test_x.json')['data']\n",
    "    y_test = np.asarray(load_file_json('work/test_y.json')['data'])\n",
    "\n",
    "\n",
    "    vocab_size = preprocess.vocab_size\n",
    "    torch.manual_seed(1234)\n",
    "    model = MyRNN(vocab_size, dw=300, dh=50, L=4, num_layers=2, bidirectional=True)\n",
    "    ntrain = len(x_train)\n",
    "    nepoch = 10 \n",
    "    batch_size = 128 \n",
    "    op = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.NLLLoss() \n",
    "\n",
    "    train_writer = SummaryWriter(log_dir='./work/logs/train')\n",
    "    valid_writer = SummaryWriter(log_dir='./work/logs/valid')\n",
    "    logger = list()\n",
    "    for epoch in tqdm.tqdm(range(nepoch)):\n",
    "        train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "        train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "        train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "        with torch.no_grad():\n",
    "            valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "            valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "            valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "        logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "        print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    train_writer.close()\n",
    "    valid_writer.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_size:1\telapsed_time:361.9438319206238 [sec]\n",
    "batch_size:2\telapsed_time:195.748544216156 [sec]\n",
    "batch_size:4\telapsed_time:100.17491579055786 [sec]\n",
    "batch_size:8\telapsed_time:52.92460918426514 [sec]\n",
    "batch_size:16\telapsed_time:28.268762826919556 [sec]\n",
    "batch_size:32\telapsed_time:15.605499982833862 [sec]\n",
    "batch_size:64\telapsed_time:10.040234327316284 [sec]\n",
    "batch_size:128\telapsed_time:6.472395420074463 [sec]\n",
    "batch_size:256\telapsed_time:4.624249458312988 [sec]\n",
    "batch_size:512\telapsed_time:3.524317502975464 [sec]\n",
    "batch_size:1024\telapsed_time:3.101412773132324 [sec]\n",
    "batch_size:2048\telapsed_time:2.749919891357422 [sec]\n",
    "batch_size:4096\telapsed_time:2.6615805625915527 [sec]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_size:1\telapsed_time:1016.4217035770416 [sec]\n",
    "batch_size:2\telapsed_time:527.0541796684265 [sec]\n",
    "batch_size:4\telapsed_time:291.180860042572 [sec]\n",
    "batch_size:8\telapsed_time:147.82439351081848 [sec]\n",
    "batch_size:16\telapsed_time:82.42432832717896 [sec]\n",
    "batch_size:32\telapsed_time:44.86296892166138 [sec]\n",
    "batch_size:64\telapsed_time:28.106043815612793 [sec]\n",
    "batch_size:128\telapsed_time:17.97643756866455 [sec]\n",
    "batch_size:256\telapsed_time:13.771385669708252 [sec]\n",
    "batch_size:512\telapsed_time:11.08540654182434 [sec]\n",
    "batch_size:1024\telapsed_time:9.829963445663452 [sec]\n",
    "batch_size:2048\telapsed_time:15.359896421432495 [sec]\n",
    "batch_size:4096\telapsed_time:12.083300590515137 [sec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 84. 単語ベクトルの導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyRNN(vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False)\n",
    "model.update_from_word2vec(w2v, preprocess.word_transformer)\n",
    "ntrain = len(x_train)\n",
    "nepoch = 10 \n",
    "batch_size = 128 \n",
    "#op = optim.SGD(model.parameters(), lr=0.1)\n",
    "op = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRNN(\n",
      "  (embed): Embedding(9866, 300, padding_idx=0)\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (linear): Linear(in_features=50, out_features=4, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae01c67ef5742b8815efbf8244972b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 1.0841823437687164, 'train_acc': 50.468164794007485, 'valid_loss': 0.8035261861840437, 'valid_acc': 73.85767790262172}\n",
      "{'epoch': 1, 'train_loss': 0.5676666072245394, 'train_acc': 79.05430711610487, 'valid_loss': 0.46076675584253746, 'valid_acc': 83.07116104868913}\n",
      "{'epoch': 2, 'train_loss': 0.3240814438919896, 'train_acc': 87.34082397003745, 'valid_loss': 0.45489098219835805, 'valid_acc': 83.59550561797752}\n",
      "{'epoch': 3, 'train_loss': 0.2447720053490628, 'train_acc': 89.84082397003745, 'valid_loss': 0.41074375164196286, 'valid_acc': 85.0187265917603}\n",
      "{'epoch': 4, 'train_loss': 0.20364386380723354, 'train_acc': 90.53370786516854, 'valid_loss': 0.4525810466946734, 'valid_acc': 85.2434456928839}\n",
      "{'epoch': 5, 'train_loss': 0.16895339485634578, 'train_acc': 91.44194756554307, 'valid_loss': 0.39696553704443943, 'valid_acc': 85.76779026217228}\n",
      "{'epoch': 6, 'train_loss': 0.14188933785488542, 'train_acc': 92.61235955056179, 'valid_loss': 0.4342009868291433, 'valid_acc': 85.76779026217228}\n",
      "{'epoch': 7, 'train_loss': 0.13013480573557737, 'train_acc': 94.20411985018727, 'valid_loss': 0.4601046518440104, 'valid_acc': 86.29213483146067}\n",
      "{'epoch': 8, 'train_loss': 0.10268918731239404, 'train_acc': 96.21722846441948, 'valid_loss': 0.6388556601402912, 'valid_acc': 86.66666666666667}\n",
      "{'epoch': 9, 'train_loss': 0.09355827285890722, 'train_acc': 97.09737827715355, 'valid_loss': 0.6587920381335284, 'valid_acc': 87.11610486891385}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_writer = SummaryWriter(log_dir='./work/logs/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/valid')\n",
    "logger = list()\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 85. 双方向RNN・多層化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False, dropout=0.0, rnn_bias=True, PATH=None):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.dw, self.dh = dw, dh\n",
    "        if PATH:\n",
    "            self.embed = nn.from_pretrained(PATH)\n",
    "        else:\n",
    "            m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "            nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "            nn.init.constant_(m.weight[0], 0)\n",
    "            self.embed = m\n",
    "        self.rnn = nn.RNN(dw, dh, bias=rnn_bias, num_layers=num_layers, bidirectional=bidirectional, batch_first=True, nonlinearity='relu', dropout=dropout)\n",
    "        if bidirectional:\n",
    "            self.linear = nn.Linear(2 * dh, L, bias=True)\n",
    "        else:\n",
    "            self.linear = nn.Linear(dh, L, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "    '''\n",
    "    x: ids (not one hot vector)\n",
    "    '''\n",
    "    def forward(self, x, x_lengths):\n",
    "        x = self.embed(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, hidden = self.rnn(packed)\n",
    "        hidden = hidden.view(self.num_layers, 2 if self.bidirectional else 1, -1, self.dh)\n",
    "        last_hidden = hidden[-1]\n",
    "        if self.bidirectional:\n",
    "            x = self.linear(torch.cat([last_hidden[0], last_hidden[1]], dim=1))\n",
    "        else:\n",
    "            x = self.linear(last_hidden[0])\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "    \n",
    "    def update_from_word2vec(self, w2v, transformer):\n",
    "        for word, idx in transformer.items():\n",
    "            with torch.no_grad():\n",
    "                if word in w2v:\n",
    "                    self.embed.weight[idx].copy_(torch.from_numpy(w2v[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169d89be6b054088bc48ceaaf605a221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 0.5621581544813592, 'train_acc': 78.89513108614233, 'valid_loss': 0.4073159295521425, 'valid_acc': 84.49438202247191}\n",
      "{'epoch': 1, 'train_loss': 0.2761372826965561, 'train_acc': 90.54307116104869, 'valid_loss': 0.32408394719777484, 'valid_acc': 89.21348314606742}\n",
      "{'epoch': 2, 'train_loss': 0.15865980392314968, 'train_acc': 94.85955056179776, 'valid_loss': 0.38206236023134954, 'valid_acc': 89.58801498127342}\n",
      "{'epoch': 3, 'train_loss': 0.09868653422214566, 'train_acc': 96.76029962546816, 'valid_loss': 0.31603187080402945, 'valid_acc': 90.0374531835206}\n",
      "{'epoch': 4, 'train_loss': 0.06191470492352149, 'train_acc': 98.01498127340824, 'valid_loss': 0.354854448614049, 'valid_acc': 90.187265917603}\n",
      "{'epoch': 5, 'train_loss': 0.04207206202970908, 'train_acc': 98.76404494382022, 'valid_loss': 0.423645007476378, 'valid_acc': 90.11235955056179}\n",
      "{'epoch': 6, 'train_loss': 0.029736371623694004, 'train_acc': 99.14794007490637, 'valid_loss': 0.43740582595603744, 'valid_acc': 90.48689138576779}\n",
      "{'epoch': 7, 'train_loss': 0.024143751769253378, 'train_acc': 99.26029962546816, 'valid_loss': 0.47447261707613086, 'valid_acc': 90.2621722846442}\n",
      "{'epoch': 8, 'train_loss': 0.016981003151627514, 'train_acc': 99.50374531835206, 'valid_loss': 0.5276704940679815, 'valid_acc': 90.187265917603}\n",
      "{'epoch': 9, 'train_loss': 0.014470773713300458, 'train_acc': 99.5692883895131, 'valid_loss': 0.578131686792838, 'valid_acc': 90.11235955056179}\n",
      "{'epoch': 10, 'train_loss': 0.012267955881805065, 'train_acc': 99.61610486891385, 'valid_loss': 0.609489129321852, 'valid_acc': 89.9625468164794}\n",
      "{'epoch': 11, 'train_loss': 0.011763852185915025, 'train_acc': 99.6441947565543, 'valid_loss': 0.6221801364354873, 'valid_acc': 90.56179775280899}\n",
      "{'epoch': 12, 'train_loss': 0.009190920337752094, 'train_acc': 99.7565543071161, 'valid_loss': 0.6349943097164569, 'valid_acc': 90.4119850187266}\n",
      "{'epoch': 13, 'train_loss': 0.007475100706369801, 'train_acc': 99.7940074906367, 'valid_loss': 0.6437471562110529, 'valid_acc': 90.56179775280899}\n",
      "{'epoch': 14, 'train_loss': 0.006996589585831772, 'train_acc': 99.76591760299625, 'valid_loss': 0.6853665305434095, 'valid_acc': 90.4119850187266}\n",
      "{'epoch': 15, 'train_loss': 0.006662463872041204, 'train_acc': 99.76591760299625, 'valid_loss': 0.7463206511088525, 'valid_acc': 90.2621722846442}\n",
      "{'epoch': 16, 'train_loss': 0.006095683014465929, 'train_acc': 99.82209737827715, 'valid_loss': 0.7572968491007773, 'valid_acc': 90.187265917603}\n",
      "{'epoch': 17, 'train_loss': 0.005496314409352867, 'train_acc': 99.8501872659176, 'valid_loss': 0.7394630722338787, 'valid_acc': 90.2621722846442}\n",
      "{'epoch': 18, 'train_loss': 0.005264996301163902, 'train_acc': 99.8501872659176, 'valid_loss': 0.7731378175122907, 'valid_acc': 90.187265917603}\n",
      "{'epoch': 19, 'train_loss': 0.005062298988945231, 'train_acc': 99.8689138576779, 'valid_loss': 0.775320291653108, 'valid_acc': 90.48689138576779}\n",
      "{'epoch': 20, 'train_loss': 0.0042176578875826425, 'train_acc': 99.8689138576779, 'valid_loss': 0.8054808324642395, 'valid_acc': 89.9625468164794}\n",
      "{'epoch': 21, 'train_loss': 0.0036168538353995383, 'train_acc': 99.8876404494382, 'valid_loss': 0.8166512896282396, 'valid_acc': 90.3370786516854}\n",
      "{'epoch': 22, 'train_loss': 0.003712566942216354, 'train_acc': 99.87827715355805, 'valid_loss': 0.8189672736639387, 'valid_acc': 90.11235955056179}\n",
      "{'epoch': 23, 'train_loss': 0.004103519654682792, 'train_acc': 99.8689138576779, 'valid_loss': 0.8435167791021897, 'valid_acc': 90.3370786516854}\n",
      "{'epoch': 24, 'train_loss': 0.004758185066966417, 'train_acc': 99.8689138576779, 'valid_loss': 0.8165045719914669, 'valid_acc': 90.4119850187266}\n",
      "{'epoch': 25, 'train_loss': 0.0038886942660910414, 'train_acc': 99.8876404494382, 'valid_loss': 0.7920553676867753, 'valid_acc': 90.2621722846442}\n",
      "{'epoch': 26, 'train_loss': 0.003254001973936341, 'train_acc': 99.91573033707866, 'valid_loss': 0.8587557230995836, 'valid_acc': 90.0374531835206}\n",
      "{'epoch': 27, 'train_loss': 0.0031458503124565723, 'train_acc': 99.8876404494382, 'valid_loss': 0.8222025812788403, 'valid_acc': 90.2621722846442}\n",
      "{'epoch': 28, 'train_loss': 0.003482625587260846, 'train_acc': 99.90636704119851, 'valid_loss': 0.8781661164894533, 'valid_acc': 89.9625468164794}\n",
      "{'epoch': 29, 'train_loss': 0.003453359592571673, 'train_acc': 99.90636704119851, 'valid_loss': 0.8485437124855956, 'valid_acc': 90.3370786516854}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyRNN(vocab_size, dw=300, dh=50, L=4, num_layers=2, bidirectional=True, dropout=0.6)\n",
    "model.update_from_word2vec(w2v, preprocess.word_transformer)\n",
    "ntrain = len(x_train)\n",
    "nepoch = 30 \n",
    "batch_size = 128 \n",
    "op = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0.001)\n",
    "criterion = nn.NLLLoss() \n",
    "train_writer = SummaryWriter(log_dir='./work/logs/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/valid')\n",
    "logger = list()\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 86. 畳み込みニューラルネットワーク (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "dw, dh = 300, 50\n",
    "torch.manual_seed(1234)\n",
    "embed = nn.Embedding(vocab_size, dw, padding_idx=0) #idx 0 は 0埋め\n",
    "kernel_size = [3 * dw, dh]\n",
    "cnn = nn.Conv2d(1, 1, kernel_size, padding=0, stride=1, padding_mode='replicate')\n",
    "linear = nn.Linear(50, 4, bias=True)\n",
    "softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "input = torch.LongTensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 900, 50])\n"
     ]
    }
   ],
   "source": [
    "print(cnn.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([600])"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = embed(input)\n",
    "torch.cat([x[0][0], x[0][1]]).shape\n",
    "#print(cnn(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, L=4, dropout=0.0, PATH=None):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.dw, self.dh = dw, dh\n",
    "        if PATH:\n",
    "            self.embed = nn.from_pretrained(PATH)\n",
    "        else:\n",
    "            m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "            nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "            nn.init.constant_(m.weight[0], 0)\n",
    "            self.embed = m\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "    '''\n",
    "    x: ids (not one hot vector)\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        _, hidden = self.rnn(x)\n",
    "        hidden = hidden.view(self.num_layers, 2 if self.bidirectional else 1, -1, self.dh)\n",
    "        last_hidden = hidden[-1]\n",
    "        if self.bidirectional:\n",
    "            x = self.linear(torch.cat([last_hidden[0], last_hidden[1]], dim=1))\n",
    "        else:\n",
    "            x = self.linear(last_hidden[0])\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "    \n",
    "    def update_from_word2vec(self, w2v, transformer):\n",
    "        for word, idx in transformer.items():\n",
    "            with torch.no_grad():\n",
    "                if word in w2v:\n",
    "                    self.embed.weight[idx].copy_(torch.from_numpy(w2v[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 87. 確率的勾配降下法によるCNNの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 88. パラメータチューニング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 89. 事前学習済み言語モデルからの転移学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
