{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80. ID番号への変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from functools import reduce\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, mode='r') as f:\n",
    "        X = list()\n",
    "        Y = list()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            splited_line = line.split('\\t')\n",
    "            X.append(splited_line[0])\n",
    "            Y.append(splited_line[1])\n",
    "        return X, Y\n",
    "\n",
    "def save_file_json(path, data):\n",
    "    with open(path, mode='w') as out_file:\n",
    "        out_file.write(json.dumps(data)+'\\n')\n",
    "        \n",
    "def load_file_json(path):\n",
    "    with open(path, mode='r') as in_file:\n",
    "        data = json.load(in_file)\n",
    "    return data\n",
    "\n",
    "def chr2num(y):\n",
    "    converter = {'b':0, 't':1, 'e':2, 'm':3}\n",
    "    return [converter[article_type] for article_type in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessTools:\n",
    "    def __init__(self, vocab_path=None):\n",
    "        self.word_count = defaultdict(int)       \n",
    "        if vocab_path:\n",
    "            self.word_transformer = load_file_json(vocab_path)\n",
    "            self.vocab_size = len(self.word_transformer) + 1\n",
    "        else:\n",
    "            self.word_transformer = dict()\n",
    "            self.vocab_size = -1\n",
    "        \n",
    "    def tokenize(self, data):\n",
    "        return [[word for word in word_tokenize(txt)] for txt in data]\n",
    "\n",
    "    def make_word_transformar(self, train_data:list):\n",
    "        for data in train_data:\n",
    "            for word in data:\n",
    "                self.word_count[word] += 1\n",
    "        sorted_word_count = sorted(self.word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "        for idx, (word, count) in enumerate(sorted_word_count):\n",
    "            if count < 2:\n",
    "                break\n",
    "            else:\n",
    "                self.word_transformer[word] = idx + 1\n",
    "        self.vocab_size = len(self.word_transformer) + 1\n",
    "\n",
    "    def txt2ids(self, txt_list:list):\n",
    "        txt_ids = list()\n",
    "        for txt in txt_list:\n",
    "            ids = list()\n",
    "            for word in txt:\n",
    "                if word in self.word_transformer:\n",
    "                    ids.append(self.word_transformer[word])\n",
    "                else:\n",
    "                    ids.append(0)\n",
    "            txt_ids.append(ids)\n",
    "        return txt_ids\n",
    "\n",
    "\n",
    "    def ids2vec(self, txt_ids:list):\n",
    "        txt_vec = list()\n",
    "        identity = np.identity(self.vocab_size)\n",
    "        for ids in txt_ids:\n",
    "            txt_vec.append(identity[ids])\n",
    "        return txt_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = PreprocessTools()\n",
    "x_train, y_train = load_data('data/train.txt')\n",
    "x_valid, y_valid = load_data('data/valid.txt')\n",
    "x_test, y_test = load_data('data/test.txt')\n",
    "x_train = preprocess.tokenize(x_train)\n",
    "x_valid = preprocess.tokenize(x_valid)\n",
    "x_test = preprocess.tokenize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.make_word_transformar(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ids = preprocess.txt2ids(x_train)\n",
    "x_valid_ids = preprocess.txt2ids(x_valid)\n",
    "x_test_ids = preprocess.txt2ids(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kindred', 'Healthcare', 'to', 'buy', 'Gentiva', 'for', 'about', '$', '573', 'mln'] [5065, 3395, 2, 181, 3396, 13, 164, 19, 0, 220]\n",
      "['US', 'to', 'boost', 'ground', ',', 'naval', 'forces', 'in', 'NATO', 'countries'] [15, 2, 586, 3397, 1, 0, 4067, 6, 5066, 5067]\n",
      "['Robert', 'Pattinson', '-', 'Robert', 'Pattinson', 'Brushes', 'Off', 'Kristen', 'Stewart', \"'s\", 'Cheating', '...'] [237, 945, 11, 237, 945, 0, 385, 669, 1259, 4, 3398, 3]\n",
      "['Piers', 'Morgan', 'Delivers', 'One', 'Final', 'Blow', 'To', 'Gun', 'Violence', 'In', 'Last', 'Show'] [5068, 399, 6690, 185, 1074, 5069, 16, 3399, 2225, 20, 785, 161]\n",
      "['Here', 'We', 'Go', ':', \"'Star\", 'Wars', 'Episode', 'VII', \"'\", 'Kicks', 'Off', 'Filming', 'at', 'Pinewood'] [400, 196, 639, 7, 549, 210, 295, 587, 5, 5070, 385, 1371, 22, 0]\n",
      "['Amazon', 'gets', 'in', 'the', 'game', ':', 'Retailer', 'beats', 'Google', 'to', 'buy', 'hit', 'console', 'broadcasting', '...'] [169, 330, 6, 17, 1619, 7, 0, 609, 82, 2, 181, 245, 5071, 0, 3]\n",
      "['FOREX-Euro', 'retreats', 'as', 'ECB', 'steps', 'up', 'verbal', 'campaign'] [331, 5072, 9, 52, 588, 43, 6691, 2518]\n",
      "['UPDATE', '2-US', 'appeals', 'court', 'revives', 'Apple', 'patent', 'lawsuit', 'against', 'Google'] [10, 301, 6692, 332, 0, 109, 1783, 734, 232, 82]\n",
      "['Kendall', 'Jenner', 'shows', 'support', 'for', 'Kanye', 'West', 'in', 'Yeezus', 'tour', 'shirt'] [735, 333, 204, 786, 13, 87, 69, 6, 0, 702, 5073]\n",
      "['UPDATE', '1-S.Korea', \"'s\", 'state', 'health', 'insurer', 'tobacco', 'firms', 'for', 'damages'] [10, 0, 4, 3400, 880, 6693, 3401, 946, 13, 3402]\n"
     ]
    }
   ],
   "source": [
    "for word, ids in zip(x_train[:10], x_train_ids[:10]):\n",
    "    print(word, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9866\n"
     ]
    }
   ],
   "source": [
    "print(preprocess.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_json('work/vocab.json', preprocess.word_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 81. RNNによる予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hotにはしない\n",
    "#xtrain_vec = preprocess.ids2vec(xtrain_ids) \n",
    "#xvalid_vec = preprocess.ids2vec(xvalid_ids)\n",
    "#xtest_vec = preprocess.ids2vec(xtest_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = chr2num(y_train)\n",
    "y_valid = chr2num(y_valid)\n",
    "y_test = chr2num(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_json('work/train_x.json', {'data':x_train_ids})\n",
    "save_file_json('work/train_y.json', {'data': y_train})\n",
    "save_file_json('work/valid_x.json', {'data': x_valid_ids})\n",
    "save_file_json('work/valid_y.json', {'data': y_valid})\n",
    "save_file_json('work/test_x.json', {'data': x_test_ids})\n",
    "save_file_json('work/test_y.json', {'data': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = load_file_json('work/train_x.json')['data']\n",
    "y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "x_valid = load_file_json('work/valid_x.json')['data']\n",
    "y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "x_test = load_file_json('work/test_x.json')['data']\n",
    "y_test = np.asarray(load_file_json('work/test_y.json')['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs: input, h_0\n",
    "\n",
    "- input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details.\n",
    "\n",
    "- h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.\n",
    "\n",
    "## Outputs: output, h_n\n",
    "\n",
    "- output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.\n",
    "\n",
    "    For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.\n",
    "\n",
    "- h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n",
    "\n",
    "    Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "dw, dh = 300, 50\n",
    "torch.manual_seed(1234)\n",
    "embed = nn.Embedding(vocab_size, dw, padding_idx=0) #idx 0 は 0埋め\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "rnn = nn.RNN(dw, dh, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "input = torch.LongTensor([[0, 1, 2, 3, 4]])\n",
    "linear = nn.Linear(50, 4, bias=True)\n",
    "softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.9588,  0.7610, -0.9565,  0.5900, -0.5844,  0.9816,  0.9628,\n",
      "          -0.5733, -0.9226, -0.8944,  0.1460,  0.4036,  0.9598,  0.8089,\n",
      "           0.9755, -0.9994,  0.9671,  0.1083,  0.9839, -0.7825,  0.9294,\n",
      "           0.9097, -0.8783, -0.7987, -0.9839, -0.9838,  0.7480, -0.9795,\n",
      "          -0.9946,  0.8387, -0.4521, -0.9794, -0.3257,  0.4931, -0.9273,\n",
      "          -0.8774, -0.4283, -0.9119, -0.9689, -0.8910,  0.6624,  0.9643,\n",
      "          -0.2032, -0.2346,  0.0353,  0.8976, -0.2835, -0.9466, -0.8547,\n",
      "          -0.7617]],\n",
      "\n",
      "        [[ 0.6467,  0.4122,  0.2868,  0.2072,  0.1445, -0.1215,  0.5725,\n",
      "           0.0861,  0.1255, -0.3936,  0.6288,  0.1832, -0.1401,  0.2997,\n",
      "           0.1401, -0.3450, -0.6161, -0.1151, -0.4304,  0.0187, -0.7646,\n",
      "           0.2113,  0.8125, -0.4816, -0.5212,  0.2364,  0.0617, -0.3991,\n",
      "          -0.4392, -0.6603,  0.2891, -0.1834,  0.2687,  0.2681, -0.4468,\n",
      "          -0.3341,  0.1834,  0.1835, -0.4578,  0.2792,  0.6250,  0.0511,\n",
      "          -0.1108, -0.4308, -0.7459, -0.5074,  0.0465, -0.0198,  0.5051,\n",
      "           0.4699]],\n",
      "\n",
      "        [[ 0.1849, -0.3214,  0.7265, -0.9820,  0.0688,  0.5299,  0.9316,\n",
      "          -0.1264,  0.8790,  0.2429, -0.3675, -0.3819,  0.8729,  0.2429,\n",
      "          -0.1085,  0.2291, -0.1721, -0.2097, -0.5037, -0.0138,  0.2188,\n",
      "           0.1799,  0.7399, -0.0382, -0.2318, -0.8965, -0.7706, -0.8238,\n",
      "          -0.5875,  0.6266,  0.0176, -0.3838, -0.1236, -0.0666,  0.8076,\n",
      "           0.1164,  0.0104,  0.1598,  0.4741,  0.3584,  0.8805,  0.5828,\n",
      "          -0.7260,  0.9095,  0.7137,  0.5898, -0.0039,  0.6323, -0.8684,\n",
      "          -0.5871]],\n",
      "\n",
      "        [[ 0.1481,  0.4623,  0.2626, -0.1558, -0.5080,  0.2642,  0.2887,\n",
      "          -0.1443, -0.5105, -0.5715, -0.6857, -0.2533,  0.4301,  0.3039,\n",
      "          -0.0092, -0.7680, -0.5048, -0.1967,  0.3575,  0.3964, -0.2472,\n",
      "          -0.0245, -0.4628,  0.1140, -0.4245, -0.0258,  0.5089,  0.1486,\n",
      "          -0.0030,  0.0629, -0.5284, -0.2572,  0.5957, -0.1044, -0.1753,\n",
      "          -0.5407,  0.5071,  0.0366,  0.5268, -0.2456,  0.0632, -0.1939,\n",
      "          -0.2886, -0.1329, -0.3405,  0.2228, -0.1557,  0.1135, -0.6398,\n",
      "          -0.3822]]], grad_fn=<StackBackward>)\n",
      "tensor([[[ 0.1849, -0.3214,  0.7265, -0.9820,  0.0688,  0.5299,  0.9316,\n",
      "          -0.1264,  0.8790,  0.2429, -0.3675, -0.3819,  0.8729,  0.2429,\n",
      "          -0.1085,  0.2291, -0.1721, -0.2097, -0.5037, -0.0138,  0.2188,\n",
      "           0.1799,  0.7399, -0.0382, -0.2318, -0.8965, -0.7706, -0.8238,\n",
      "          -0.5875,  0.6266,  0.0176, -0.3838, -0.1236, -0.0666,  0.8076,\n",
      "           0.1164,  0.0104,  0.1598,  0.4741,  0.3584,  0.8805,  0.5828,\n",
      "          -0.7260,  0.9095,  0.7137,  0.5898, -0.0039,  0.6323, -0.8684,\n",
      "          -0.5871]],\n",
      "\n",
      "        [[ 0.1481,  0.4623,  0.2626, -0.1558, -0.5080,  0.2642,  0.2887,\n",
      "          -0.1443, -0.5105, -0.5715, -0.6857, -0.2533,  0.4301,  0.3039,\n",
      "          -0.0092, -0.7680, -0.5048, -0.1967,  0.3575,  0.3964, -0.2472,\n",
      "          -0.0245, -0.4628,  0.1140, -0.4245, -0.0258,  0.5089,  0.1486,\n",
      "          -0.0030,  0.0629, -0.5284, -0.2572,  0.5957, -0.1044, -0.1753,\n",
      "          -0.5407,  0.5071,  0.0366,  0.5268, -0.2456,  0.0632, -0.1939,\n",
      "          -0.2886, -0.1329, -0.3405,  0.2228, -0.1557,  0.1135, -0.6398,\n",
      "          -0.3822]]], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "output, hidden = rnn(embed(input))\n",
    "print(hidden)\n",
    "hidden = hidden.view(num_layers, 2 if bidirectional else 1, -1, dh)\n",
    "#print(hidden)\n",
    "last_hidden = hidden[-1]\n",
    "print(last_hidden)\n",
    "#x = linear(hidden[-1])\n",
    "#print(x)\n",
    "#print(softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 50])\n",
      "tensor([[[[-0.9588,  0.7610, -0.9565,  0.5900, -0.5844,  0.9816,  0.9628,\n",
      "           -0.5733, -0.9226, -0.8944,  0.1460,  0.4036,  0.9598,  0.8089,\n",
      "            0.9755, -0.9994,  0.9671,  0.1083,  0.9839, -0.7825,  0.9294,\n",
      "            0.9097, -0.8783, -0.7987, -0.9839, -0.9838,  0.7480, -0.9795,\n",
      "           -0.9946,  0.8387, -0.4521, -0.9794, -0.3257,  0.4931, -0.9273,\n",
      "           -0.8774, -0.4283, -0.9119, -0.9689, -0.8910,  0.6624,  0.9643,\n",
      "           -0.2032, -0.2346,  0.0353,  0.8976, -0.2835, -0.9466, -0.8547,\n",
      "           -0.7617],\n",
      "          [ 0.8426, -0.0087,  0.9774,  0.8538,  0.4782, -0.6959,  0.4698,\n",
      "            0.6362,  0.2218,  0.9951,  0.9583,  0.9620,  0.6493, -0.8816,\n",
      "           -0.3704, -0.9910,  0.7911, -0.6577,  0.8689, -0.5620,  0.9889,\n",
      "            0.4303, -0.9841, -0.6123, -0.6164, -0.8283,  0.5802,  0.9361,\n",
      "            0.5248, -0.9203,  0.7948,  0.9841, -0.4951,  0.9653,  0.3215,\n",
      "            0.0211, -0.8799,  0.8807,  0.8267, -0.6232,  1.0000,  0.1017,\n",
      "            0.3106,  0.9512, -0.6253, -0.4531, -0.9522,  0.6767, -0.5615,\n",
      "           -0.6872]]],\n",
      "\n",
      "\n",
      "        [[[-0.3830,  0.3538, -0.4110, -0.5545,  0.4338,  0.1584,  0.7069,\n",
      "           -0.5637, -0.0689, -0.2600, -0.8683,  0.8483, -0.3127, -0.1350,\n",
      "           -0.3615, -0.5036,  0.3629,  0.9161,  0.4008, -0.4728,  0.1985,\n",
      "            0.2731,  0.7614,  0.1421,  0.4071,  0.2477,  0.3459, -0.4124,\n",
      "           -0.5912, -0.1848,  0.1775,  0.7608, -0.5969, -0.0939, -0.4988,\n",
      "            0.3173,  0.0281, -0.0270, -0.5084,  0.2242,  0.3207,  0.1414,\n",
      "           -0.1770, -0.2583,  0.7680,  0.3466,  0.2475, -0.3726,  0.4511,\n",
      "           -0.2360],\n",
      "          [-0.6264,  0.3932, -0.2255, -0.3236, -0.3958,  0.6881,  0.2437,\n",
      "           -0.8589,  0.7434,  0.6048,  0.3585,  0.0416,  0.0809, -0.2589,\n",
      "           -0.7223, -0.2905,  0.1789,  0.8684,  0.2572,  0.0121,  0.6689,\n",
      "            0.1254,  0.5855,  0.3966,  0.5681, -0.2039,  0.4120, -0.0111,\n",
      "            0.7642,  0.5110,  0.0892,  0.1279,  0.2740,  0.0628, -0.4232,\n",
      "           -0.4149, -0.5136, -0.6611, -0.4580,  0.3846, -0.3276,  0.2606,\n",
      "            0.7238, -0.3607, -0.3181,  0.0359, -0.7637,  0.2464,  0.0602,\n",
      "           -0.1428]]]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "output, hidden = rnn(embed(input))\n",
    "print(hidden.shape)\n",
    "hidden = hidden.view(num_layers, 2 if bidirectional else 1, -1, dh)\n",
    "print(hidden)\n",
    "last_hidden = hidden[-1]\n",
    "#x = linear(hidden[-1])\n",
    "#print(x)\n",
    "#print(softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 82. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False, rnn_bias=True, PATH=None):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.dw, self.dh = dw, dh\n",
    "        if PATH:\n",
    "            self.embed = nn.from_pretrained(PATH)\n",
    "        else:\n",
    "            m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "            nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "            nn.init.constant_(m.weight[0], 0)\n",
    "            self.embed = m\n",
    "        self.rnn = nn.RNN(dw, dh, bias=rnn_bias, num_layers=num_layers, bidirectional=bidirectional, batch_first=True, nonlinearity='relu')\n",
    "        if bidirectional:\n",
    "            self.linear = nn.Linear(2 * dh, L, bias=True)\n",
    "        else:\n",
    "            self.linear = nn.Linear(dh, L, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "    '''\n",
    "    x: ids (not one hot vector)\n",
    "    '''\n",
    "    def forward(self, x, x_lengths):\n",
    "        x = self.embed(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, hidden = self.rnn(packed)\n",
    "        hidden = hidden.view(self.num_layers, 2 if self.bidirectional else 1, -1, self.dh)\n",
    "        last_hidden = hidden[-1]\n",
    "        if self.bidirectional:\n",
    "            x = self.linear(torch.cat([last_hidden[0], last_hidden[1]], dim=1))\n",
    "        else:\n",
    "            x = self.linear(last_hidden[0])\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "    \n",
    "    def update_from_word2vec(self, w2v, transformer):\n",
    "        for word, idx in transformer.items():\n",
    "            with torch.no_grad():\n",
    "                if word in w2v:\n",
    "                    self.embed.weight[idx].copy_(torch.from_numpy(w2v[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSets(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = [torch.LongTensor(data) for data in x]\n",
    "        self.y = [torch.LongTensor([data]) for data in y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    x = [data[0] for data in batch]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = torch.LongTensor([data[1] for data in batch])\n",
    "    lengths = torch.LongTensor([len(data[0]) for data in batch])\n",
    "    return x, y, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution(data_x, data_y, op, criterion, model, batch_size=1, is_train=True, use_gpu=False):\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "    ndata = len(data_x)\n",
    "    dataset = MyDataSets(data_x, data_y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    sum_loss, acc_score = 0, 0\n",
    "    for batch_x, batch_y, batch_lengths in data_loader:\n",
    "        op.zero_grad()\n",
    "        out = model(batch_x, batch_lengths)\n",
    "        loss = criterion(out, batch_y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        acc_score += np.sum((pred == batch_y).cpu().detach().numpy())\n",
    "    return sum_loss / ndata, acc_score / ndata * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = load_file_json('work/train_x.json')['data']\n",
    "y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "x_valid = load_file_json('work/valid_x.json')['data']\n",
    "y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "x_test = load_file_json('work/test_x.json')['data']\n",
    "y_test = np.asarray(load_file_json('work/test_y.json')['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyRNN(vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False)\n",
    "nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "ntrain = len(x_train)\n",
    "nepoch = 10 \n",
    "batch_size = 1\n",
    "op = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.NLLLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRNN(\n",
      "  (embed): Embedding(9866, 300, padding_idx=0)\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (linear): Linear(in_features=50, out_features=4, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd189293fec44314bef7d8eb31dd0c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 1.0372647139848967, 'train_acc': 59.21348314606741, 'valid_loss': 0.887252983872252, 'valid_acc': 69.06367041198503}\n",
      "{'epoch': 1, 'train_loss': 0.8374215004742438, 'train_acc': 70.91760299625469, 'valid_loss': 0.6876947393141969, 'valid_acc': 76.70411985018727}\n",
      "{'epoch': 2, 'train_loss': 0.7466804328128738, 'train_acc': 74.11048689138576, 'valid_loss': 0.7361666740501925, 'valid_acc': 72.65917602996255}\n",
      "{'epoch': 3, 'train_loss': 0.735462963356367, 'train_acc': 74.6067415730337, 'valid_loss': 0.6611646328217783, 'valid_acc': 76.17977528089888}\n",
      "{'epoch': 4, 'train_loss': 0.6526657013838769, 'train_acc': 77.4812734082397, 'valid_loss': 0.9591464588454356, 'valid_acc': 63.97003745318352}\n",
      "{'epoch': 5, 'train_loss': nan, 'train_acc': 70.78651685393258, 'valid_loss': nan, 'valid_acc': 42.54681647940075}\n",
      "{'epoch': 6, 'train_loss': nan, 'train_acc': 42.041198501872664, 'valid_loss': nan, 'valid_acc': 42.54681647940075}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-df7023a10c87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-c37af45fcf40>\u001b[0m in \u001b[0;36mexecution\u001b[0;34m(data_x, data_y, op, criterion, model, batch_size, is_train, use_gpu)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0msum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_writer = SummaryWriter(log_dir='./work/logs/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/valid')\n",
    "logger = list()\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 83. ミニバッチ化・GPU上での学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/gpu.py\n"
     ]
    }
   ],
   "source": [
    "%%file src/gpu.py\n",
    "\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class PreprocessTools:\n",
    "    def __init__(self, vocab_path=None):\n",
    "        self.word_count = defaultdict(int)       \n",
    "        if vocab_path:\n",
    "            self.word_transformer = load_file_json(vocab_path)\n",
    "            self.vocab_size = len(self.word_transformer) + 1\n",
    "        else:\n",
    "            self.word_transformer = defaultdict(int)\n",
    "            self.vocab_size = -1\n",
    "        \n",
    "    def tokenize(self, data):\n",
    "        return [[word for word in word_tokenize(txt)] for txt in data]\n",
    "\n",
    "    def make_word_transformar(self, train_data:list):\n",
    "        for data in train_data:\n",
    "            for word in data:\n",
    "                self.word_count[word] += 1\n",
    "        sorted_word_count = sorted(self.word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "        for idx, (word, count) in enumerate(sorted_word_count):\n",
    "            if count < 2:\n",
    "                break\n",
    "            else:\n",
    "                self.word_transformer[word] = idx + 1\n",
    "        self.vocab_size = len(self.word_transformer) + 1\n",
    "\n",
    "    def txt2ids(self, txt_list:list):\n",
    "        txt_ids = list()\n",
    "        for txt in txt_list:\n",
    "            ids = list()\n",
    "            for word in txt:\n",
    "                ids.append(self.word_transformer[word])\n",
    "            txt_ids.append(ids)\n",
    "        return txt_ids\n",
    "\n",
    "\n",
    "    def ids2vec(self, txt_ids:list):\n",
    "        txt_vec = list()\n",
    "        identity = np.identity(self.vocab_size)\n",
    "        for ids in txt_ids:\n",
    "            txt_vec.append(identity[ids])\n",
    "        return txt_vec\n",
    "    \n",
    "    \n",
    "def load_data(path):\n",
    "    with open(path, mode='r') as f:\n",
    "        X = list()\n",
    "        Y = list()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            splited_line = line.split('\\t')\n",
    "            X.append(splited_line[0])\n",
    "            Y.append(splited_line[1])\n",
    "        return X, Y\n",
    "\n",
    "def save_file_json(path, data):\n",
    "    with open(path, mode='w') as out_file:\n",
    "        out_file.write(json.dumps(data)+'\\n')\n",
    "        \n",
    "def load_file_json(path):\n",
    "    with open(path, mode='r') as in_file:\n",
    "        data = json.load(in_file)\n",
    "    return data\n",
    "\n",
    "def chr2num(y):\n",
    "    converter = {'b':0, 't':1, 'e':2, 'm':3}\n",
    "    return [converter[article_type] for article_type in y]\n",
    "\n",
    "class MyRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False, rnn_bias=True, PATH=None):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.dw, self.dh = dw, dh\n",
    "        if PATH:\n",
    "            self.embed = nn.from_pretrained(PATH)\n",
    "        else:\n",
    "            m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "            nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "            nn.init.constant_(m.weight[0], 0)\n",
    "            self.embed = m\n",
    "        self.rnn = nn.RNN(dw, dh, bias=rnn_bias, num_layers=num_layers, bidirectional=bidirectional, batch_first=True, nonlinearity='tanh')\n",
    "        if bidirectional:\n",
    "            self.linear = nn.Linear(2 * dh, L, bias=True)\n",
    "        else:\n",
    "            self.linear = nn.Linear(dh, L, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "    '''\n",
    "    x: ids (not one hot vector)\n",
    "    '''\n",
    "    def forward(self, x, x_lengths):\n",
    "        x = self.embed(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, hidden = self.rnn(packed)\n",
    "        hidden = hidden.view(self.num_layers, 2 if self.bidirectional else 1, -1, self.dh)\n",
    "        last_hidden = hidden[-1]\n",
    "        if self.bidirectional:\n",
    "            x = self.linear(torch.cat([last_hidden[0], last_hidden[1]], dim=1))\n",
    "        else:\n",
    "            x = self.linear(last_hidden[0])\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "\n",
    "class MyDataSets(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = [torch.LongTensor(data) for data in x]\n",
    "        self.y = [torch.LongTensor([data]) for data in y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    x = [data[0] for data in batch]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = torch.LongTensor([data[1] for data in batch])\n",
    "    lengths = torch.LongTensor([len(data[0]) for data in batch])\n",
    "    return x, y, lengths\n",
    "    \n",
    "def execution(data_x, data_y, op, criterion, model, batch_size=1, is_train=True, use_gpu=False):\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "    ndata = len(data_x)\n",
    "    dataset = MyDataSets(data_x, data_y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    sum_loss, acc_score = 0, 0\n",
    "    for batch_x, batch_y, batch_lengths in data_loader:\n",
    "        op.zero_grad()\n",
    "        out = model(batch_x, batch_lengths)\n",
    "        loss = criterion(out, batch_y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        acc_score += np.sum((pred == batch_y).cpu().detach().numpy())\n",
    "    return sum_loss / ndata, acc_score / ndata * 100\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess = PreprocessTools('work/vocab.json')\n",
    "    \n",
    "    x_train = load_file_json('work/train_x.json')['data']\n",
    "    y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "    x_valid = load_file_json('work/valid_x.json')['data']\n",
    "    y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "    x_test = load_file_json('work/test_x.json')['data']\n",
    "    y_test = np.asarray(load_file_json('work/test_y.json')['data'])\n",
    "\n",
    "\n",
    "    vocab_size = preprocess.vocab_size\n",
    "    torch.manual_seed(1234)\n",
    "    model = MyRNN(vocab_size, dw=300, dh=50, L=4, num_layers=2, bidirectional=True)\n",
    "    ntrain = len(x_train)\n",
    "    nepoch = 10 \n",
    "    batch_size = 128 \n",
    "    op = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.NLLLoss() \n",
    "\n",
    "    train_writer = SummaryWriter(log_dir='./work/logs/train')\n",
    "    valid_writer = SummaryWriter(log_dir='./work/logs/valid')\n",
    "    logger = list()\n",
    "    for epoch in tqdm.tqdm(range(nepoch)):\n",
    "        train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "        train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "        train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "        with torch.no_grad():\n",
    "            valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "            valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "            valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "        logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "        print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    train_writer.close()\n",
    "    valid_writer.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_size:1\telapsed_time:361.9438319206238 [sec]\n",
    "batch_size:2\telapsed_time:195.748544216156 [sec]\n",
    "batch_size:4\telapsed_time:100.17491579055786 [sec]\n",
    "batch_size:8\telapsed_time:52.92460918426514 [sec]\n",
    "batch_size:16\telapsed_time:28.268762826919556 [sec]\n",
    "batch_size:32\telapsed_time:15.605499982833862 [sec]\n",
    "batch_size:64\telapsed_time:10.040234327316284 [sec]\n",
    "batch_size:128\telapsed_time:6.472395420074463 [sec]\n",
    "batch_size:256\telapsed_time:4.624249458312988 [sec]\n",
    "batch_size:512\telapsed_time:3.524317502975464 [sec]\n",
    "batch_size:1024\telapsed_time:3.101412773132324 [sec]\n",
    "batch_size:2048\telapsed_time:2.749919891357422 [sec]\n",
    "batch_size:4096\telapsed_time:2.6615805625915527 [sec]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_size:1\telapsed_time:1016.4217035770416 [sec]\n",
    "batch_size:2\telapsed_time:527.0541796684265 [sec]\n",
    "batch_size:4\telapsed_time:291.180860042572 [sec]\n",
    "batch_size:8\telapsed_time:147.82439351081848 [sec]\n",
    "batch_size:16\telapsed_time:82.42432832717896 [sec]\n",
    "batch_size:32\telapsed_time:44.86296892166138 [sec]\n",
    "batch_size:64\telapsed_time:28.106043815612793 [sec]\n",
    "batch_size:128\telapsed_time:17.97643756866455 [sec]\n",
    "batch_size:256\telapsed_time:13.771385669708252 [sec]\n",
    "batch_size:512\telapsed_time:11.08540654182434 [sec]\n",
    "batch_size:1024\telapsed_time:9.829963445663452 [sec]\n",
    "batch_size:2048\telapsed_time:15.359896421432495 [sec]\n",
    "batch_size:4096\telapsed_time:12.083300590515137 [sec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 84. 単語ベクトルの導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyRNN(vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False)\n",
    "model.update_from_word2vec(w2v, preprocess.word_transformer)\n",
    "ntrain = len(x_train)\n",
    "nepoch = 10 \n",
    "batch_size = 128 \n",
    "#op = optim.SGD(model.parameters(), lr=0.1)\n",
    "op = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRNN(\n",
      "  (embed): Embedding(9866, 300, padding_idx=0)\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (linear): Linear(in_features=50, out_features=4, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae01c67ef5742b8815efbf8244972b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 1.0841823437687164, 'train_acc': 50.468164794007485, 'valid_loss': 0.8035261861840437, 'valid_acc': 73.85767790262172}\n",
      "{'epoch': 1, 'train_loss': 0.5676666072245394, 'train_acc': 79.05430711610487, 'valid_loss': 0.46076675584253746, 'valid_acc': 83.07116104868913}\n",
      "{'epoch': 2, 'train_loss': 0.3240814438919896, 'train_acc': 87.34082397003745, 'valid_loss': 0.45489098219835805, 'valid_acc': 83.59550561797752}\n",
      "{'epoch': 3, 'train_loss': 0.2447720053490628, 'train_acc': 89.84082397003745, 'valid_loss': 0.41074375164196286, 'valid_acc': 85.0187265917603}\n",
      "{'epoch': 4, 'train_loss': 0.20364386380723354, 'train_acc': 90.53370786516854, 'valid_loss': 0.4525810466946734, 'valid_acc': 85.2434456928839}\n",
      "{'epoch': 5, 'train_loss': 0.16895339485634578, 'train_acc': 91.44194756554307, 'valid_loss': 0.39696553704443943, 'valid_acc': 85.76779026217228}\n",
      "{'epoch': 6, 'train_loss': 0.14188933785488542, 'train_acc': 92.61235955056179, 'valid_loss': 0.4342009868291433, 'valid_acc': 85.76779026217228}\n",
      "{'epoch': 7, 'train_loss': 0.13013480573557737, 'train_acc': 94.20411985018727, 'valid_loss': 0.4601046518440104, 'valid_acc': 86.29213483146067}\n",
      "{'epoch': 8, 'train_loss': 0.10268918731239404, 'train_acc': 96.21722846441948, 'valid_loss': 0.6388556601402912, 'valid_acc': 86.66666666666667}\n",
      "{'epoch': 9, 'train_loss': 0.09355827285890722, 'train_acc': 97.09737827715355, 'valid_loss': 0.6587920381335284, 'valid_acc': 87.11610486891385}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_writer = SummaryWriter(log_dir='./work/logs/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/valid')\n",
    "logger = list()\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 85. 双方向RNN・多層化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False, dropout=0.0, rnn_bias=True, PATH=None):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.dw, self.dh = dw, dh\n",
    "        if PATH:\n",
    "            self.embed = nn.from_pretrained(PATH)\n",
    "        else:\n",
    "            m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "            nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "            nn.init.constant_(m.weight[0], 0)\n",
    "            self.embed = m\n",
    "        self.rnn = nn.RNN(dw, dh, bias=rnn_bias, num_layers=num_layers, bidirectional=bidirectional, batch_first=True, nonlinearity='relu', dropout=dropout)\n",
    "        if bidirectional:\n",
    "            self.linear = nn.Linear(2 * dh, L, bias=True)\n",
    "        else:\n",
    "            self.linear = nn.Linear(dh, L, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "    '''\n",
    "    x: ids (not one hot vector)\n",
    "    '''\n",
    "    def forward(self, x, x_lengths):\n",
    "        x = self.embed(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, hidden = self.rnn(packed)\n",
    "        hidden = hidden.view(self.num_layers, 2 if self.bidirectional else 1, -1, self.dh)\n",
    "        last_hidden = hidden[-1]\n",
    "        if self.bidirectional:\n",
    "            x = self.linear(torch.cat([last_hidden[0], last_hidden[1]], dim=1))\n",
    "        else:\n",
    "            x = self.linear(last_hidden[0])\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "    \n",
    "    def update_from_word2vec(self, w2v, transformer):\n",
    "        for word, idx in transformer.items():\n",
    "            with torch.no_grad():\n",
    "                if word in w2v:\n",
    "                    self.embed.weight[idx].copy_(torch.from_numpy(w2v[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169d89be6b054088bc48ceaaf605a221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 0.5621581544813592, 'train_acc': 78.89513108614233, 'valid_loss': 0.4073159295521425, 'valid_acc': 84.49438202247191}\n",
      "{'epoch': 1, 'train_loss': 0.2761372826965561, 'train_acc': 90.54307116104869, 'valid_loss': 0.32408394719777484, 'valid_acc': 89.21348314606742}\n",
      "{'epoch': 2, 'train_loss': 0.15865980392314968, 'train_acc': 94.85955056179776, 'valid_loss': 0.38206236023134954, 'valid_acc': 89.58801498127342}\n",
      "{'epoch': 3, 'train_loss': 0.09868653422214566, 'train_acc': 96.76029962546816, 'valid_loss': 0.31603187080402945, 'valid_acc': 90.0374531835206}\n",
      "{'epoch': 4, 'train_loss': 0.06191470492352149, 'train_acc': 98.01498127340824, 'valid_loss': 0.354854448614049, 'valid_acc': 90.187265917603}\n",
      "{'epoch': 5, 'train_loss': 0.04207206202970908, 'train_acc': 98.76404494382022, 'valid_loss': 0.423645007476378, 'valid_acc': 90.11235955056179}\n",
      "{'epoch': 6, 'train_loss': 0.029736371623694004, 'train_acc': 99.14794007490637, 'valid_loss': 0.43740582595603744, 'valid_acc': 90.48689138576779}\n",
      "{'epoch': 7, 'train_loss': 0.024143751769253378, 'train_acc': 99.26029962546816, 'valid_loss': 0.47447261707613086, 'valid_acc': 90.2621722846442}\n",
      "{'epoch': 8, 'train_loss': 0.016981003151627514, 'train_acc': 99.50374531835206, 'valid_loss': 0.5276704940679815, 'valid_acc': 90.187265917603}\n",
      "{'epoch': 9, 'train_loss': 0.014470773713300458, 'train_acc': 99.5692883895131, 'valid_loss': 0.578131686792838, 'valid_acc': 90.11235955056179}\n",
      "{'epoch': 10, 'train_loss': 0.012267955881805065, 'train_acc': 99.61610486891385, 'valid_loss': 0.609489129321852, 'valid_acc': 89.9625468164794}\n",
      "{'epoch': 11, 'train_loss': 0.011763852185915025, 'train_acc': 99.6441947565543, 'valid_loss': 0.6221801364354873, 'valid_acc': 90.56179775280899}\n",
      "{'epoch': 12, 'train_loss': 0.009190920337752094, 'train_acc': 99.7565543071161, 'valid_loss': 0.6349943097164569, 'valid_acc': 90.4119850187266}\n",
      "{'epoch': 13, 'train_loss': 0.007475100706369801, 'train_acc': 99.7940074906367, 'valid_loss': 0.6437471562110529, 'valid_acc': 90.56179775280899}\n",
      "{'epoch': 14, 'train_loss': 0.006996589585831772, 'train_acc': 99.76591760299625, 'valid_loss': 0.6853665305434095, 'valid_acc': 90.4119850187266}\n",
      "{'epoch': 15, 'train_loss': 0.006662463872041204, 'train_acc': 99.76591760299625, 'valid_loss': 0.7463206511088525, 'valid_acc': 90.2621722846442}\n",
      "{'epoch': 16, 'train_loss': 0.006095683014465929, 'train_acc': 99.82209737827715, 'valid_loss': 0.7572968491007773, 'valid_acc': 90.187265917603}\n",
      "{'epoch': 17, 'train_loss': 0.005496314409352867, 'train_acc': 99.8501872659176, 'valid_loss': 0.7394630722338787, 'valid_acc': 90.2621722846442}\n",
      "{'epoch': 18, 'train_loss': 0.005264996301163902, 'train_acc': 99.8501872659176, 'valid_loss': 0.7731378175122907, 'valid_acc': 90.187265917603}\n",
      "{'epoch': 19, 'train_loss': 0.005062298988945231, 'train_acc': 99.8689138576779, 'valid_loss': 0.775320291653108, 'valid_acc': 90.48689138576779}\n",
      "{'epoch': 20, 'train_loss': 0.0042176578875826425, 'train_acc': 99.8689138576779, 'valid_loss': 0.8054808324642395, 'valid_acc': 89.9625468164794}\n",
      "{'epoch': 21, 'train_loss': 0.0036168538353995383, 'train_acc': 99.8876404494382, 'valid_loss': 0.8166512896282396, 'valid_acc': 90.3370786516854}\n",
      "{'epoch': 22, 'train_loss': 0.003712566942216354, 'train_acc': 99.87827715355805, 'valid_loss': 0.8189672736639387, 'valid_acc': 90.11235955056179}\n",
      "{'epoch': 23, 'train_loss': 0.004103519654682792, 'train_acc': 99.8689138576779, 'valid_loss': 0.8435167791021897, 'valid_acc': 90.3370786516854}\n",
      "{'epoch': 24, 'train_loss': 0.004758185066966417, 'train_acc': 99.8689138576779, 'valid_loss': 0.8165045719914669, 'valid_acc': 90.4119850187266}\n",
      "{'epoch': 25, 'train_loss': 0.0038886942660910414, 'train_acc': 99.8876404494382, 'valid_loss': 0.7920553676867753, 'valid_acc': 90.2621722846442}\n",
      "{'epoch': 26, 'train_loss': 0.003254001973936341, 'train_acc': 99.91573033707866, 'valid_loss': 0.8587557230995836, 'valid_acc': 90.0374531835206}\n",
      "{'epoch': 27, 'train_loss': 0.0031458503124565723, 'train_acc': 99.8876404494382, 'valid_loss': 0.8222025812788403, 'valid_acc': 90.2621722846442}\n",
      "{'epoch': 28, 'train_loss': 0.003482625587260846, 'train_acc': 99.90636704119851, 'valid_loss': 0.8781661164894533, 'valid_acc': 89.9625468164794}\n",
      "{'epoch': 29, 'train_loss': 0.003453359592571673, 'train_acc': 99.90636704119851, 'valid_loss': 0.8485437124855956, 'valid_acc': 90.3370786516854}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyRNN(vocab_size, dw=300, dh=50, L=4, num_layers=2, bidirectional=True, dropout=0.6)\n",
    "model.update_from_word2vec(w2v, preprocess.word_transformer)\n",
    "ntrain = len(x_train)\n",
    "nepoch = 30 \n",
    "batch_size = 128 \n",
    "op = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0.001)\n",
    "criterion = nn.NLLLoss() \n",
    "train_writer = SummaryWriter(log_dir='./work/logs/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/valid')\n",
    "logger = list()\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 86. 畳み込みニューラルネットワーク (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = load_file_json('work/train_x.json')['data']\n",
    "y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "x_valid = load_file_json('work/valid_x.json')['data']\n",
    "y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "x_test = load_file_json('work/test_x.json')['data']\n",
    "y_test = np.asarray(load_file_json('work/test_y.json')['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "dw, dh, w_sz = 300, 50, 3\n",
    "torch.manual_seed(1234)\n",
    "embed = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "kernel_size = [w_sz, dw]\n",
    "cnn = nn.Conv2d(in_channels=1, out_channels=dh, kernel_size=kernel_size, padding=(w_sz-2, 0), stride=1)\n",
    "g = nn.ReLU()\n",
    "pool = torch.max\n",
    "linear = nn.Linear(dh, 4, bias=True)\n",
    "softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "input = torch.LongTensor([[[0, 1, 2, 3, 4]], [[5, 6, 7, 8, 9]]])\n",
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 3, 300])\n"
     ]
    }
   ],
   "source": [
    "print(cnn.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0314, -1.1941, -1.7095, -1.8355],\n",
      "        [-1.1559, -0.9727, -1.7407, -2.0267]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.FloatTensor([[[1,1,1,0,0]], [[1,1,1,1,0]]])\n",
    "x = embed(input)\n",
    "x = cnn(x)\n",
    "x = x.view(x.shape[:3])\n",
    "x = x * mask\n",
    "x = g(x)\n",
    "x = pool(x, dim=2).values\n",
    "x = linear(x)\n",
    "x = softmax(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 87. 確率的勾配降下法によるCNNの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, w_sz=3, L=4, dropout=0.0):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.dw, self.dh, self.w_sz = dw, dh, w_sz\n",
    "        \n",
    "        m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "        nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "        nn.init.constant_(m.weight[0], 0)\n",
    "        self.embed = m\n",
    "        \n",
    "        self.cnn = nn.Conv2d(in_channels=1, out_channels=dh, kernel_size=[w_sz, dw], padding=(w_sz-2, 0), stride=1)\n",
    "        self.g = nn.ReLU()\n",
    "        self.pool = torch.max\n",
    "        self.linear = nn.Linear(dh, 4, bias=True)\n",
    "        self.dropout = nn.Dropout2d(p=dropout)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, x_lengths):\n",
    "        x = self.embed(x)\n",
    "        x = self.cnn(x)\n",
    "        x = self.g(x)\n",
    "        x = x.view(x.shape[:3])\n",
    "        x = self.pool(x, dim=2).values\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "    \n",
    "    def update_from_word2vec(self, w2v, transformer):\n",
    "        for word, idx in transformer.items():\n",
    "            with torch.no_grad():\n",
    "                if word in w2v:\n",
    "                    self.embed.weight[idx].copy_(torch.from_numpy(w2v[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSets(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = [torch.LongTensor(data) for data in x]\n",
    "        self.y = [torch.LongTensor([data]) for data in y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x = [data[0] for data in batch]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = torch.LongTensor([data[1] for data in batch])\n",
    "    lengths = torch.LongTensor([len(data[0]) for data in batch])\n",
    "    x = x.view(-1, 1, torch.max(lengths))\n",
    "    return x, y, lengths\n",
    "\n",
    "def execution(data_x, data_y, op, criterion, model, batch_size=1, is_train=True, use_gpu=False):\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "    ndata = len(data_x)\n",
    "    dataset = MyDataSets(data_x, data_y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    sum_loss, acc_score = 0, 0\n",
    "    for batch_x, batch_y, batch_lengths in data_loader:\n",
    "        op.zero_grad()\n",
    "        out = model(batch_x, batch_lengths)\n",
    "        loss = criterion(out, batch_y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        acc_score += np.sum((pred == batch_y).cpu().detach().numpy())\n",
    "    return sum_loss / ndata, acc_score / ndata * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d630bd6d01b94efebb8366e4d4767f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 0.9389110220776485, 'train_acc': 64.7565543071161, 'valid_loss': 0.7810873905657651, 'valid_acc': 72.28464419475655}\n",
      "{'epoch': 1, 'train_loss': 0.6317997540631297, 'train_acc': 77.02247191011236, 'valid_loss': 0.5567348639960557, 'valid_acc': 79.02621722846442}\n",
      "{'epoch': 2, 'train_loss': 0.40004950408328493, 'train_acc': 85.40262172284643, 'valid_loss': 0.4443720965145491, 'valid_acc': 82.84644194756554}\n",
      "{'epoch': 3, 'train_loss': 0.2403459146221051, 'train_acc': 91.74157303370787, 'valid_loss': 0.40862589928987436, 'valid_acc': 85.99250936329588}\n",
      "{'epoch': 4, 'train_loss': 0.13932250821607933, 'train_acc': 95.75842696629213, 'valid_loss': 0.38565428323961776, 'valid_acc': 86.59176029962546}\n",
      "{'epoch': 5, 'train_loss': 0.07791281996948243, 'train_acc': 98.22097378277154, 'valid_loss': 0.3847782454515154, 'valid_acc': 88.08988764044943}\n"
     ]
    }
   ],
   "source": [
    "x_train = load_file_json('work/train_x.json')['data']\n",
    "y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "x_valid = load_file_json('work/valid_x.json')['data']\n",
    "y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "x_test = load_file_json('work/test_x.json')['data']\n",
    "y_test = np.asarray(load_file_json('work/test_y.json')['data'])\n",
    "\n",
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyCNN(vocab_size, dw=300, dh=50, w_sz=3, L=4)\n",
    "nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "ntrain = len(x_train)\n",
    "nepoch = 10 \n",
    "batch_size = 1\n",
    "op = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.NLLLoss() \n",
    "\n",
    "train_writer = SummaryWriter(log_dir='./work/logs/cnn/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/cnn/valid')\n",
    "logger = list()\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 88. パラメータチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, w_sz=3, L=4, dropout=0.0):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.dw, self.dh, self.w_sz = dw, dh, w_sz\n",
    "        \n",
    "        m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "        nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "        nn.init.constant_(m.weight[0], 0)\n",
    "        self.embed = m\n",
    "        \n",
    "        self.cnn = nn.Conv2d(in_channels=1, out_channels=dh, kernel_size=[w_sz, dw], padding=(w_sz-2, 0), stride=1)\n",
    "        self.g = nn.ReLU()\n",
    "        self.pool = torch.max\n",
    "        self.linear = nn.Linear(dh, 4, bias=True)\n",
    "        self.dropout = nn.Dropout2d(p=0.2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "    def forward(self, x, x_lengths):\n",
    "        max_len = x_lengths.max()\n",
    "        mask = torch.FloatTensor([[[ 1.0 if i < seq_len else 0.0 for i in range(max_len)]] for seq_len in x_lengths])\n",
    "        x = self.embed(x)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.shape[:3])\n",
    "        x = x * mask\n",
    "        \n",
    "        \n",
    "        x = self.g(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x, dim=2).values\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "    \n",
    "    def update_from_word2vec(self, w2v, transformer):\n",
    "        for word, idx in transformer.items():\n",
    "            with torch.no_grad():\n",
    "                if word in w2v:\n",
    "                    self.embed.weight[idx].copy_(torch.from_numpy(w2v[word]))\n",
    "                    \n",
    "class MyDataSets(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = [torch.LongTensor(data) for data in x]\n",
    "        self.y = [torch.LongTensor([data]) for data in y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x = [data[0] for data in batch]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = torch.LongTensor([data[1] for data in batch])\n",
    "    lengths = torch.LongTensor([len(data[0]) for data in batch])\n",
    "    x = x.view(-1, 1, torch.max(lengths))\n",
    "    return x, y, lengths\n",
    "\n",
    "def execution(data_x, data_y, op, criterion, model, batch_size=1, is_train=True, use_gpu=False):\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "    ndata = len(data_x)\n",
    "    dataset = MyDataSets(data_x, data_y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    sum_loss, acc_score = 0, 0\n",
    "    for batch_x, batch_y, batch_lengths in data_loader:\n",
    "        op.zero_grad()\n",
    "        out = model(batch_x, batch_lengths)\n",
    "        loss = criterion(out, batch_y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        acc_score += np.sum((pred == batch_y).cpu().detach().numpy())\n",
    "    return sum_loss / ndata, acc_score / ndata * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53a42f3a1aa4159b47554430f5b99b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 0.5757558084605785, 'train_acc': 77.11610486891387, 'valid_loss': 0.3003921103834659, 'valid_acc': 91.01123595505618}\n",
      "{'epoch': 1, 'train_loss': 0.3131626569376456, 'train_acc': 86.86329588014982, 'valid_loss': 0.24776309576820346, 'valid_acc': 92.20973782771536}\n",
      "{'epoch': 2, 'train_loss': 0.24898011490200342, 'train_acc': 88.82022471910112, 'valid_loss': 0.2316487668605333, 'valid_acc': 92.35955056179776}\n",
      "{'epoch': 3, 'train_loss': 0.20590350836627047, 'train_acc': 90.25280898876404, 'valid_loss': 0.22737948575269865, 'valid_acc': 92.43445692883896}\n",
      "{'epoch': 4, 'train_loss': 0.18683609850844193, 'train_acc': 90.87078651685393, 'valid_loss': 0.22482745501767384, 'valid_acc': 92.73408239700375}\n",
      "{'epoch': 5, 'train_loss': 0.16657901802536254, 'train_acc': 91.03932584269663, 'valid_loss': 0.22434093769123492, 'valid_acc': 92.58426966292134}\n",
      "{'epoch': 6, 'train_loss': 0.14319769961110662, 'train_acc': 92.43445692883896, 'valid_loss': 0.22556115971522384, 'valid_acc': 92.88389513108615}\n",
      "{'epoch': 7, 'train_loss': 0.1409806786739871, 'train_acc': 92.09737827715357, 'valid_loss': 0.2287044880318731, 'valid_acc': 92.73408239700375}\n",
      "{'epoch': 8, 'train_loss': 0.13260559091407262, 'train_acc': 92.02247191011236, 'valid_loss': 0.2317976978722583, 'valid_acc': 92.73408239700375}\n",
      "{'epoch': 9, 'train_loss': 0.12465392889154985, 'train_acc': 92.64044943820224, 'valid_loss': 0.23459397989712405, 'valid_acc': 92.73408239700375}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train = load_file_json('work/train_x.json')['data']\n",
    "y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "x_valid = load_file_json('work/valid_x.json')['data']\n",
    "y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "x_test = load_file_json('work/test_x.json')['data']\n",
    "y_test = np.asarray(load_file_json('work/test_y.json')['data'])\n",
    "\n",
    "\n",
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyCNN(vocab_size, dw=300, dh=50, w_sz=3, L=4)\n",
    "model.update_from_word2vec(w2v, preprocess.word_transformer)\n",
    "nepoch = 10 \n",
    "batch_size = 256 \n",
    "op = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0.001)\n",
    "criterion = nn.NLLLoss() \n",
    "\n",
    "\n",
    "train_writer = SummaryWriter(log_dir='./work/logs/cnn/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/cnn/valid')\n",
    "logger = list()\n",
    "max_valid = -1\n",
    "max_model_param = None\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "        \n",
    "    if max_valid < valid_acc:\n",
    "        max_valid = valid_acc\n",
    "        max_model_param = model.state_dict()\n",
    "        \n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.53558052434457\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(max_model_param)\n",
    "with torch.no_grad():\n",
    "    test_loss, test_acc = execution(x_test, y_test, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "    print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCl(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, w_sz=3, L=4, num_layers=1, bidirectional=False, dropout=0.2):\n",
    "        super(MyCl, self).__init__()\n",
    "        self.dw, self.dh, self.w_sz = dw, dh, w_sz\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(dw, dh, num_layers=num_layers, bidirectional=bidirectional, batch_first=True, nonlinearity='relu', dropout=dropout)\n",
    "        \n",
    "        m = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "        nn.init.normal_(m.weight, mean=0, std=dw ** -0.5)\n",
    "        nn.init.constant_(m.weight[0], 0)\n",
    "        self.embed = m\n",
    "        \n",
    "        self.cnn = nn.Conv2d(in_channels=1, out_channels=dh, kernel_size=[w_sz, 2 * dh], padding=(w_sz-2, 0), stride=1)\n",
    "        self.g = nn.ReLU()\n",
    "        self.pool = torch.max\n",
    "        self.linear = nn.Linear(dh, 4, bias=True)\n",
    "        self.dropout = nn.Dropout2d(p=dropout)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "        \n",
    "    def forward(self, x, x_lengths):\n",
    "        x = self.embed(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "        x, _ = self.rnn(packed)\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        max_len = x_lengths.max()\n",
    "        mask = torch.FloatTensor([[ [1.0] if i < seq_len else [0.0] for i in range(max_len)] for seq_len in x_lengths])\n",
    "        x = x * mask\n",
    "        x = x.view(-1, 1, max_len, 2 * dh)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.shape[:3])\n",
    "        x = self.g(x)\n",
    "        x = self.dropout(x)\n",
    "        x = pool(x, dim=2).values\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "    \n",
    "    def update_from_word2vec(self, w2v, transformer):\n",
    "        for word, idx in transformer.items():\n",
    "            with torch.no_grad():\n",
    "                if word in w2v:\n",
    "                    self.embed.weight[idx].copy_(torch.from_numpy(w2v[word]))\n",
    "\n",
    "                    \n",
    "class MyDataSets(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = [torch.LongTensor(data) for data in x]\n",
    "        self.y = [torch.LongTensor([data]) for data in y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x = [data[0] for data in batch]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = torch.LongTensor([data[1] for data in batch])\n",
    "    lengths = torch.LongTensor([len(data[0]) for data in batch])\n",
    "    return x, y, lengths\n",
    "\n",
    "def execution(data_x, data_y, op, criterion, model, batch_size=1, is_train=True, use_gpu=False):\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "    ndata = len(data_x)\n",
    "    dataset = MyDataSets(data_x, data_y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    sum_loss, acc_score = 0, 0\n",
    "    for batch_x, batch_y, batch_lengths in data_loader:\n",
    "        op.zero_grad()\n",
    "        out = model(batch_x, batch_lengths)\n",
    "        loss = criterion(out, batch_y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        acc_score += np.sum((pred == batch_y).cpu().detach().numpy())\n",
    "    return sum_loss / ndata, acc_score / ndata * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70c19affbe44b17a631fcbc56609ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 0.7617954755990246, 'train_acc': 64.78464419475655, 'valid_loss': 0.45093154049991224, 'valid_acc': 82.47191011235955}\n",
      "{'epoch': 1, 'train_loss': 0.5248867922731106, 'train_acc': 74.05430711610487, 'valid_loss': 0.386541759141822, 'valid_acc': 84.49438202247191}\n",
      "{'epoch': 2, 'train_loss': 0.43306670436698397, 'train_acc': 76.45131086142321, 'valid_loss': 0.3535459771584929, 'valid_acc': 85.76779026217228}\n",
      "{'epoch': 3, 'train_loss': 0.35876306201634783, 'train_acc': 79.70973782771536, 'valid_loss': 0.29925455965576103, 'valid_acc': 89.9625468164794}\n",
      "{'epoch': 4, 'train_loss': 0.2839015465997132, 'train_acc': 82.7621722846442, 'valid_loss': 0.31662483724315515, 'valid_acc': 90.86142322097378}\n",
      "{'epoch': 5, 'train_loss': 0.23849713913956833, 'train_acc': 84.70973782771536, 'valid_loss': 0.4425045225839043, 'valid_acc': 90.56179775280899}\n",
      "{'epoch': 6, 'train_loss': 0.21623792997683478, 'train_acc': 85.07490636704121, 'valid_loss': 0.41337351468618444, 'valid_acc': 91.16104868913858}\n",
      "{'epoch': 7, 'train_loss': 0.20459032019649107, 'train_acc': 85.62734082397003, 'valid_loss': 0.503964547226938, 'valid_acc': 91.31086142322098}\n",
      "{'epoch': 8, 'train_loss': 0.1931200065974439, 'train_acc': 85.86142322097379, 'valid_loss': 0.49794238486540005, 'valid_acc': 91.08614232209737}\n",
      "{'epoch': 9, 'train_loss': 0.1955018829540367, 'train_acc': 85.77715355805243, 'valid_loss': 0.5288919447513108, 'valid_acc': 91.23595505617978}\n",
      "{'epoch': 10, 'train_loss': 0.18726633752553204, 'train_acc': 85.72097378277154, 'valid_loss': 0.5845789286081264, 'valid_acc': 91.23595505617978}\n",
      "{'epoch': 11, 'train_loss': 0.1802293274063296, 'train_acc': 86.34831460674157, 'valid_loss': 0.67468501410904, 'valid_acc': 90.93632958801498}\n",
      "{'epoch': 12, 'train_loss': 0.18166697998618364, 'train_acc': 86.01123595505618, 'valid_loss': 0.6607835679018542, 'valid_acc': 91.16104868913858}\n",
      "{'epoch': 13, 'train_loss': 0.184954794817203, 'train_acc': 85.73970037453184, 'valid_loss': 0.7500759056891395, 'valid_acc': 91.16104868913858}\n",
      "{'epoch': 14, 'train_loss': 0.1766696358776271, 'train_acc': 86.47003745318352, 'valid_loss': 0.7440189006846496, 'valid_acc': 91.16104868913858}\n",
      "{'epoch': 15, 'train_loss': 0.1744447665491354, 'train_acc': 86.41385767790261, 'valid_loss': 0.7751740882012728, 'valid_acc': 91.01123595505618}\n",
      "{'epoch': 16, 'train_loss': 0.1704878854171167, 'train_acc': 86.72284644194757, 'valid_loss': 0.8303259199478207, 'valid_acc': 90.86142322097378}\n",
      "{'epoch': 17, 'train_loss': 0.16836574651328812, 'train_acc': 86.45131086142322, 'valid_loss': 0.8989630131239301, 'valid_acc': 91.08614232209737}\n",
      "{'epoch': 18, 'train_loss': 0.1731133687139004, 'train_acc': 86.20786516853933, 'valid_loss': 0.9404611070727588, 'valid_acc': 90.93632958801498}\n",
      "{'epoch': 19, 'train_loss': 0.17129373890295457, 'train_acc': 86.12359550561798, 'valid_loss': 1.0660272668809927, 'valid_acc': 91.01123595505618}\n",
      "{'epoch': 20, 'train_loss': 0.16814394048537207, 'train_acc': 86.76966292134831, 'valid_loss': 0.859165332804012, 'valid_acc': 91.16104868913858}\n",
      "{'epoch': 21, 'train_loss': 0.17151649962650256, 'train_acc': 86.17041198501873, 'valid_loss': 0.9326133778032739, 'valid_acc': 91.08614232209737}\n",
      "{'epoch': 22, 'train_loss': 0.17227362345220446, 'train_acc': 86.14232209737828, 'valid_loss': 0.9722493578432204, 'valid_acc': 91.46067415730337}\n",
      "{'epoch': 23, 'train_loss': 0.17383089810051722, 'train_acc': 85.86142322097379, 'valid_loss': 0.9818114571803518, 'valid_acc': 91.08614232209737}\n",
      "{'epoch': 24, 'train_loss': 0.1739653533875719, 'train_acc': 85.87078651685394, 'valid_loss': 1.0070087960596834, 'valid_acc': 91.01123595505618}\n",
      "{'epoch': 25, 'train_loss': 0.16850095216031377, 'train_acc': 86.34831460674157, 'valid_loss': 1.0320392784107937, 'valid_acc': 91.38576779026218}\n",
      "{'epoch': 26, 'train_loss': 0.167805774749888, 'train_acc': 86.17041198501873, 'valid_loss': 1.032887939031651, 'valid_acc': 91.16104868913858}\n",
      "{'epoch': 27, 'train_loss': 0.17548278173927065, 'train_acc': 86.02059925093633, 'valid_loss': 1.0469414906555348, 'valid_acc': 91.16104868913858}\n",
      "{'epoch': 28, 'train_loss': 0.16723449256982698, 'train_acc': 86.52621722846442, 'valid_loss': 1.1431751954477378, 'valid_acc': 91.08614232209737}\n",
      "{'epoch': 29, 'train_loss': 0.16668684780597687, 'train_acc': 86.59176029962546, 'valid_loss': 1.1043180092443687, 'valid_acc': 91.16104868913858}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train = load_file_json('work/train_x.json')['data']\n",
    "y_train = np.asarray(load_file_json('work/train_y.json')['data'])\n",
    "x_valid = load_file_json('work/valid_x.json')['data']\n",
    "y_valid = np.asarray(load_file_json('work/valid_y.json')['data'])\n",
    "x_test = load_file_json('work/test_x.json')['data']\n",
    "y_test = np.asarray(load_file_json('work/test_y.json')['data'])\n",
    "\n",
    "\n",
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyCl(vocab_size, dw=300, dh=50, w_sz=3, L=4, num_layers=2, bidirectional=True, dropout=0.3)\n",
    "model.update_from_word2vec(w2v, preprocess.word_transformer)\n",
    "nepoch = 30 \n",
    "batch_size = 256\n",
    "op = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0.001)\n",
    "criterion = nn.NLLLoss() \n",
    "\n",
    "\n",
    "train_writer = SummaryWriter(log_dir='./work/logs/cnn/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/cnn/valid')\n",
    "logger = list()\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=batch_size)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 89. 事前学習済み言語モデルからの転移学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8MB 3.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages (from transformers) (0.23)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages (from transformers) (1.17.2)\n",
      "Requirement already satisfied: packaging in /Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages (from transformers) (19.2)\n",
      "Requirement already satisfied: filelock in /Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Collecting sacremoses (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 11.6MB/s eta 0:00:01     |███████████████████████████████▉| 880kB 11.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/d5/ae173868b6525c6f18f9a684c8842c0673cfc630430fcb48d8c6eb817f2e/regex-2020.11.13-cp37-cp37m-macosx_10_9_x86_64.whl (284kB)\n",
      "\u001b[K     |████████████████████████████████| 286kB 9.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/81/75ac49a3566f51f85d5fd3a7a7b587fa6cfe37cb9fc19ce3de48763b7295/tokenizers-0.10.1-cp37-cp37m-macosx_10_11_x86_64.whl (2.2MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2MB 9.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages (from transformers) (4.47.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (0.6.0)\n",
      "Requirement already satisfied: six in /Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages (from packaging->transformers) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages (from packaging->transformers) (2.4.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages (from requests->transformers) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages (from requests->transformers) (2019.9.11)\n",
      "Requirement already satisfied: click in /Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages (from sacremoses->transformers) (0.13.2)\n",
      "Requirement already satisfied: more-itertools in /Users/y_nakamura/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->transformers) (7.2.0)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893261 sha256=c1542b1dc91f3f66aa3dfbcb55ac1c2528cb75f4dc24f0c603fac1c90d52ddc4\n",
      "  Stored in directory: /Users/y_nakamura/Library/Caches/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: regex, sacremoses, tokenizers, transformers\n",
      "Successfully installed regex-2020.11.13 sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import cuda\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertModel(torch.nn.Module):\n",
    "    def __init__(self, L=4, dropout=0.2):\n",
    "        super(MyBertModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.linear = nn.Linear(768, L)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = self.bert(inputs['ids'], attention_mask=inputs['mask'])\n",
    "        out = self.linear(self.dropout(out['pooler_output']))\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "                    \n",
    "class MyDataSets(Dataset):\n",
    "    def __init__(self, X, Y, tokenizer, max_len):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.X[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          pad_to_max_length=True,\n",
    "          truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "          'ids': torch.LongTensor(ids),\n",
    "          'mask': torch.LongTensor(mask),\n",
    "          'labels': torch.LongTensor(self.Y[idx])\n",
    "        }\n",
    "\n",
    "def execution(dataset, op, criterion, model, batch_size=1, is_train=True, use_gpu=False):\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "    ndata = len(dataset)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=is_train)\n",
    "    sum_loss, acc_score = 0, 0\n",
    "    for data in data_loader:\n",
    "        op.zero_grad()\n",
    "        labels = data['labels'].reshape(-1)\n",
    "        out = model(data)\n",
    "        loss = criterion(out, labels)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(labels)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        acc_score += np.sum((pred == labels).cpu().detach().numpy())\n",
    "    return sum_loss / ndata, acc_score / ndata * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, _ = load_data('data/train.txt')\n",
    "x_valid, _ = load_data('data/valid.txt')\n",
    "x_test, _ = load_data('data/test.txt')\n",
    "y_train = np.asarray(load_file_json('work/train_y.json')['data']).reshape(-1, 1)\n",
    "y_valid = np.asarray(load_file_json('work/valid_y.json')['data']).reshape(-1, 1)\n",
    "y_test = np.asarray(load_file_json('work/test_y.json')['data']).reshape(-1, 1)\n",
    "\n",
    "max_len = 24\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset_train = MyDataSets(x_train, y_train, tokenizer, max_len)\n",
    "dataset_valid = MyDataSets(x_valid, y_valid, tokenizer, max_len)\n",
    "dataset_test = MyDataSets(x_test, y_test, tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyBertModel(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (softmax): LogSoftmax()\n",
      "  (linear): Linear(in_features=768, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670625a740fd440ba5fb8a5e9bf5bad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-6b510245fd0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmax_model_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m#train_writer.add_scalar(\"loss\", train_loss, epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#train_writer.add_scalar(\"accuracy\", train_acc, epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-517d3b2818cd>\u001b[0m in \u001b[0;36mexecution\u001b[0;34m(dataset, op, criterion, model, batch_size, is_train, use_gpu)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0msum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "model = MyBertModel(L=4, dropout=0.2)\n",
    "nepoch = 10 \n",
    "batch_size = 256 \n",
    "op = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0.001)\n",
    "criterion = nn.NLLLoss() \n",
    "\n",
    "\n",
    "#train_writer = SummaryWriter(log_dir='./work/logs/cnn/train')\n",
    "#valid_writer = SummaryWriter(log_dir='./work/logs/cnn/valid')\n",
    "logger = list()\n",
    "max_valid = -1\n",
    "max_model_param = None\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(dataset_train, op, criterion, model, batch_size=batch_size)\n",
    "    #train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    #train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(dataset_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        #valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        #valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "        \n",
    "    if max_valid < valid_acc:\n",
    "        max_valid = valid_acc\n",
    "        max_model_param = model.state_dict()\n",
    "        \n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "#train_writer.close()\n",
    "#valid_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/gpu89.py\n"
     ]
    }
   ],
   "source": [
    "%%file src/gpu89.py\n",
    "\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import cuda\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path, mode='r') as f:\n",
    "        X = list()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            splited_line = line.split('\\t')\n",
    "            X.append(splited_line[0])\n",
    "        return X\n",
    "\n",
    "def load_file_json(path):\n",
    "    with open(path, mode='r') as in_file:\n",
    "        data = json.load(in_file)\n",
    "    return data\n",
    "\n",
    "class MyBertModel(torch.nn.Module):\n",
    "    def __init__(self, L=4, dropout=0.2):\n",
    "        super(MyBertModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.linear = nn.Linear(768, L)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = self.bert(inputs['ids'], attention_mask=inputs['mask'])\n",
    "        out = self.linear(self.dropout(out['pooler_output']))\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "                    \n",
    "class MyDataSets(Dataset):\n",
    "    def __init__(self, X, Y, tokenizer, max_len):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.X[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          pad_to_max_length=True,\n",
    "          truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "          'ids': torch.LongTensor(ids),\n",
    "          'mask': torch.LongTensor(mask),\n",
    "          'labels': torch.LongTensor(self.Y[idx])\n",
    "        }\n",
    "\n",
    "def execution(dataset, op, criterion, model, batch_size=1, is_train=True, use_gpu=False):\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "    ndata = len(dataset)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=is_train)\n",
    "    sum_loss, acc_score = 0, 0\n",
    "    for data in data_loader:\n",
    "        op.zero_grad()\n",
    "        labels = data['labels'].reshape(-1)\n",
    "        out = model(data)\n",
    "        loss = criterion(out, labels)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(labels)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        acc_score += np.sum((pred == labels).cpu().detach().numpy())\n",
    "    return sum_loss / ndata, acc_score / ndata * 100\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x_train = load_data('data/train.txt')\n",
    "    x_valid = load_data('data/valid.txt')\n",
    "    x_test = load_data('data/test.txt')\n",
    "    y_train = np.asarray(load_file_json('work/train_y.json')['data']).reshape(-1, 1)\n",
    "    y_valid = np.asarray(load_file_json('work/valid_y.json')['data']).reshape(-1, 1)\n",
    "    y_test = np.asarray(load_file_json('work/test_y.json')['data']).reshape(-1, 1)\n",
    "\n",
    "    max_len = 24\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    dataset_train = MyDataSets(x_train, y_train, tokenizer, max_len)\n",
    "    dataset_valid = MyDataSets(x_valid, y_valid, tokenizer, max_len)\n",
    "    dataset_test = MyDataSets(x_test, y_test, tokenizer, max_len)\n",
    "    \n",
    "    torch.manual_seed(1234)\n",
    "    model = MyBertModel(L=4, dropout=0.2)\n",
    "    nepoch = 10 \n",
    "    batch_size = 256 \n",
    "    op = optim.AdamW(model.parameters(), lr=0.00001)\n",
    "    criterion = nn.NLLLoss() \n",
    "\n",
    "\n",
    "    logger = list()\n",
    "    max_valid = -1\n",
    "    max_model_param = None\n",
    "    for epoch in tqdm.tqdm(range(nepoch)):\n",
    "        train_loss, train_acc = execution(dataset_train, op, criterion, model, batch_size=batch_size)\n",
    "        with torch.no_grad():\n",
    "            valid_loss, valid_acc = execution(dataset_valid, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "\n",
    "        if max_valid < valid_acc:\n",
    "            max_valid = valid_acc\n",
    "            max_model_param = model.state_dict()\n",
    "\n",
    "        logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "        print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    \n",
    "    model.load_state_dict(max_model_param)\n",
    "    with torch.no_grad():\n",
    "        test_loss, test_acc = execution(dataset_test, op, criterion, model, batch_size=batch_size, is_train=False)\n",
    "        print(test_acc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{'epoch': 0, 'train_loss': 0.566496663716402, 'train_acc': 79.42883895131087, 'valid_loss': 0.2485000971439626, 'valid_acc': 92.28464419475655}\n",
    "  2%|██▌                                                                                                                                | 1/50 [00:30<25:11, 30.85s/it]{'epoch': 1, 'train_loss': 0.2203922065456262, 'train_acc': 93.19288389513109, 'valid_loss': 0.2163889830478568, 'valid_acc': 92.88389513108615}\n",
    "  4%|█████▏                                                                                                                             | 2/50 [01:02<24:51, 31.08s/it]{'epoch': 2, 'train_loss': 0.14955870329440757, 'train_acc': 95.46816479400749, 'valid_loss': 0.19449176829629178, 'valid_acc': 93.78277153558052}\n",
    "  6%|███████▊                                                                                                                           | 3/50 [01:33<24:24, 31.16s/it]{'epoch': 3, 'train_loss': 0.09833101645558515, 'train_acc': 96.87265917602997, 'valid_loss': 0.19293785230274058, 'valid_acc': 94.15730337078652}\n",
    "  8%|██████████▍                                                                                                                        | 4/50 [02:05<24:00, 31.32s/it]{'epoch': 4, 'train_loss': 0.06399463213394198, 'train_acc': 98.19288389513109, 'valid_loss': 0.19485568438799641, 'valid_acc': 94.68164794007491}\n",
    " 10%|█████████████                                                                                                                      | 5/50 [02:37<23:32, 31.38s/it]{'epoch': 5, 'train_loss': 0.046151753656621955, 'train_acc': 98.75468164794007, 'valid_loss': 0.19580492765715954, 'valid_acc': 94.68164794007491}\n",
    " 12%|███████████████▋                                                                                                                   | 6/50 [03:08<23:02, 31.42s/it]{'epoch': 6, 'train_loss': 0.03472873998129189, 'train_acc': 99.02621722846442, 'valid_loss': 0.2275822892812977, 'valid_acc': 94.08239700374533}\n",
    " 14%|██████████████████▎                                                                                                                | 7/50 [03:40<22:33, 31.47s/it]{'epoch': 7, 'train_loss': 0.026534553090361564, 'train_acc': 99.34456928838952, 'valid_loss': 0.2221802552569002, 'valid_acc': 94.53183520599251}\n",
    " 16%|████████████████████▉                                                                                                              | 8/50 [04:11<22:03, 31.51s/it]{'epoch': 8, 'train_loss': 0.02332154212638122, 'train_acc': 99.46629213483146, 'valid_loss': 0.24210150907241673, 'valid_acc': 94.23220973782772}\n",
    " 18%|███████████████████████▌                                                                                                           | 9/50 [04:43<21:30, 31.48s/it]{'epoch': 9, 'train_loss': 0.01874042779960659, 'train_acc': 99.51310861423221, 'valid_loss': 0.25624266314819066, 'valid_acc': 94.3820224719101}\n",
    " 20%|██████████████████████████                                                                                                        | 10/50 [05:14<21:00, 31.51s/it]{'epoch': 10, 'train_loss': 0.016636172313703578, 'train_acc': 99.52247191011236, 'valid_loss': 0.2640771158067475, 'valid_acc': 94.53183520599251}\n",
    " 22%|████████████████████████████▌                                                                                                     | 11/50 [05:46<20:29, 31.51s/it]{'epoch': 11, 'train_loss': 0.019013032776718675, 'train_acc': 99.46629213483146, 'valid_loss': 0.26177952588441666, 'valid_acc': 94.30711610486891}\n",
    " 24%|███████████████████████████████▏                                                                                                  | 12/50 [06:17<19:59, 31.57s/it]{'epoch': 12, 'train_loss': 0.013313668121475629, 'train_acc': 99.5880149812734, 'valid_loss': 0.27820483599597595, 'valid_acc': 94.23220973782772}\n",
    " 26%|█████████████████████████████████▊                                                                                                | 13/50 [06:49<19:24, 31.48s/it]{'epoch': 13, 'train_loss': 0.012153296597294984, 'train_acc': 99.65355805243445, 'valid_loss': 0.2709267175906383, 'valid_acc': 94.30711610486891}\n",
    " 28%|████████████████████████████████████▍                                                                                             | 14/50 [07:20<18:52, 31.45s/it]{'epoch': 14, 'train_loss': 0.012947318131847078, 'train_acc': 99.6067415730337, 'valid_loss': 0.27626353938146475, 'valid_acc': 94.15730337078652}\n",
    " 30%|███████████████████████████████████████                                                                                           | 15/50 [07:52<18:21, 31.46s/it]{'epoch': 15, 'train_loss': 0.010146667863929483, 'train_acc': 99.7003745318352, 'valid_loss': 0.2839247746023346, 'valid_acc': 94.15730337078652}\n",
    " 32%|█████████████████████████████████████████▌                                                                                        | 16/50 [08:24<18:00, 31.77s/it]{'epoch': 16, 'train_loss': 0.006312958434030539, 'train_acc': 99.8314606741573, 'valid_loss': 0.2760224112578099, 'valid_acc': 94.30711610486891}\n",
    " 34%|████████████████████████████████████████████▏                                                                                     | 17/50 [08:56<17:26, 31.71s/it]{'epoch': 17, 'train_loss': 0.009444951734763713, 'train_acc': 99.67228464419475, 'valid_loss': 0.30023904872521034, 'valid_acc': 93.63295880149812}\n",
    " 36%|██████████████████████████████████████████████▊                                                                                   | 18/50 [09:27<16:54, 31.70s/it]{'epoch': 18, 'train_loss': 0.00981366633893643, 'train_acc': 99.6629213483146, 'valid_loss': 0.28031131172001583, 'valid_acc': 94.53183520599251}\n",
    " 38%|█████████████████████████████████████████████████▍                                                                                | 19/50 [09:59<16:19, 31.59s/it]{'epoch': 19, 'train_loss': 0.012237795338615399, 'train_acc': 99.65355805243445, 'valid_loss': 0.315955847992356, 'valid_acc': 93.55805243445693}\n",
    " 40%|████████████████████████████████████████████████████                                                                              | 20/50 [10:30<15:48, 31.63s/it]{'epoch': 20, 'train_loss': 0.010021786791972697, 'train_acc': 99.6629213483146, 'valid_loss': 0.29351746502161474, 'valid_acc': 94.68164794007491}\n",
    " 42%|██████████████████████████████████████████████████████▌                                                                           | 21/50 [11:02<15:17, 31.65s/it]{'epoch': 21, 'train_loss': 0.006732465795041603, 'train_acc': 99.7752808988764, 'valid_loss': 0.2977933934160721, 'valid_acc': 94.53183520599251}\n",
    " 44%|█████████████████████████████████████████████████████████▏                                                                        | 22/50 [11:33<14:44, 31.58s/it]{'epoch': 22, 'train_loss': 0.004766554499353711, 'train_acc': 99.84082397003745, 'valid_loss': 0.31779430030883477, 'valid_acc': 94.30711610486891}\n",
    " 46%|███████████████████████████████████████████████████████████▊                                                                      | 23/50 [12:05<14:13, 31.62s/it]{'epoch': 23, 'train_loss': 0.003577503356720715, 'train_acc': 99.90636704119851, 'valid_loss': 0.30940003218443207, 'valid_acc': 94.45692883895131}\n",
    " 48%|██████████████████████████████████████████████████████████████▍                                                                   | 24/50 [12:37<13:39, 31.53s/it]{'epoch': 24, 'train_loss': 0.0034252983354004134, 'train_acc': 99.8876404494382, 'valid_loss': 0.31627016506307803, 'valid_acc': 94.83146067415731}\n",
    " 50%|█████████████████████████████████████████████████████████████████                                                                 | 25/50 [13:08<13:08, 31.53s/it]{'epoch': 25, 'train_loss': 0.0034658629680488413, 'train_acc': 99.85955056179775, 'valid_loss': 0.3236635152097052, 'valid_acc': 94.7565543071161}\n",
    " 52%|███████████████████████████████████████████████████████████████████▌                                                              | 26/50 [13:40<12:36, 31.51s/it]{'epoch': 26, 'train_loss': 0.0032907761032571004, 'train_acc': 99.8689138576779, 'valid_loss': 0.3715325372830759, 'valid_acc': 93.70786516853933}\n",
    " 54%|██████████████████████████████████████████████████████████████████████▏                                                           | 27/50 [14:11<12:04, 31.49s/it]{'epoch': 27, 'train_loss': 0.008867851158600538, 'train_acc': 99.72846441947566, 'valid_loss': 0.2849853605645873, 'valid_acc': 94.68164794007491}\n",
    " 56%|████████████████████████████████████████████████████████████████████████▊                                                         | 28/50 [14:42<11:32, 31.48s/it]{'epoch': 28, 'train_loss': 0.00498631942794006, 'train_acc': 99.8314606741573, 'valid_loss': 0.31548379746366867, 'valid_acc': 94.68164794007491}\n",
    " 58%|███████████████████████████████████████████████████████████████████████████▍                                                      | 29/50 [15:14<11:01, 31.49s/it]{'epoch': 29, 'train_loss': 0.002480224859599924, 'train_acc': 99.90636704119851, 'valid_loss': 0.33605022292849274, 'valid_acc': 94.23220973782772}\n",
    " 60%|██████████████████████████████████████████████████████████████████████████████                                                    | 30/50 [15:46<10:32, 31.62s/it]{'epoch': 30, 'train_loss': 0.0045747591321023484, 'train_acc': 99.84082397003745, 'valid_loss': 0.3532768019876174, 'valid_acc': 94.15730337078652}\n",
    " 62%|████████████████████████████████████████████████████████████████████████████████▌                                                 | 31/50 [16:17<09:59, 31.53s/it]{'epoch': 31, 'train_loss': 0.008025072736505153, 'train_acc': 99.78464419475655, 'valid_loss': 0.3113992303888896, 'valid_acc': 94.7565543071161}\n",
    " 64%|███████████████████████████████████████████████████████████████████████████████████▏                                              | 32/50 [16:49<09:27, 31.53s/it]{'epoch': 32, 'train_loss': 0.006113214063508722, 'train_acc': 99.7565543071161, 'valid_loss': 0.3087868998231792, 'valid_acc': 94.53183520599251}\n",
    " 66%|█████████████████████████████████████████████████████████████████████████████████████▊                                            | 33/50 [17:21<08:58, 31.68s/it]{'epoch': 33, 'train_loss': 0.005696265729757249, 'train_acc': 99.8501872659176, 'valid_loss': 0.31668400530762664, 'valid_acc': 94.6067415730337}\n",
    " 68%|████████████████████████████████████████████████████████████████████████████████████████▍                                         | 34/50 [17:52<08:25, 31.59s/it]{'epoch': 34, 'train_loss': 0.0028265674842900008, 'train_acc': 99.8876404494382, 'valid_loss': 0.33643346734010104, 'valid_acc': 94.30711610486891}\n",
    " 70%|███████████████████████████████████████████████████████████████████████████████████████████                                       | 35/50 [18:24<07:54, 31.61s/it]{'epoch': 35, 'train_loss': 0.0071899104465107866, 'train_acc': 99.7940074906367, 'valid_loss': 0.2774727951449234, 'valid_acc': 94.7565543071161}\n",
    " 72%|█████████████████████████████████████████████████████████████████████████████████████████████▌                                    | 36/50 [18:55<07:21, 31.55s/it]{'epoch': 36, 'train_loss': 0.005471882995051046, 'train_acc': 99.80337078651685, 'valid_loss': 0.33625643091618124, 'valid_acc': 94.45692883895131}\n",
    " 74%|████████████████████████████████████████████████████████████████████████████████████████████████▏                                 | 37/50 [19:27<06:49, 31.52s/it]{'epoch': 37, 'train_loss': 0.002144288379600453, 'train_acc': 99.8876404494382, 'valid_loss': 0.31948926945736683, 'valid_acc': 94.6067415730337}\n",
    " 76%|██████████████████████████████████████████████████████████████████████████████████████████████████▊                               | 38/50 [19:58<06:18, 31.57s/it]{'epoch': 38, 'train_loss': 0.001732212972593117, 'train_acc': 99.91573033707866, 'valid_loss': 0.3421260129004605, 'valid_acc': 94.23220973782772}\n",
    " 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████▍                            | 39/50 [20:30<05:46, 31.51s/it]{'epoch': 39, 'train_loss': 0.0012996498759127873, 'train_acc': 99.9438202247191, 'valid_loss': 0.35339767630438734, 'valid_acc': 94.83146067415731}\n",
    " 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████                          | 40/50 [21:01<05:15, 31.54s/it]{'epoch': 40, 'train_loss': 0.0034532172116189357, 'train_acc': 99.8501872659176, 'valid_loss': 0.352416814668697, 'valid_acc': 94.45692883895131}\n",
    " 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▌                       | 41/50 [21:33<04:43, 31.52s/it]{'epoch': 41, 'train_loss': 0.0072976964744974684, 'train_acc': 99.72846441947566, 'valid_loss': 0.33205016802870824, 'valid_acc': 94.68164794007491}\n",
    " 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                    | 42/50 [22:04<04:12, 31.53s/it]{'epoch': 42, 'train_loss': 0.003975374633856187, 'train_acc': 99.84082397003745, 'valid_loss': 0.3482545849591605, 'valid_acc': 94.90636704119851}\n",
    " 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                  | 43/50 [22:36<03:40, 31.48s/it]{'epoch': 43, 'train_loss': 0.0033336599617398584, 'train_acc': 99.84082397003745, 'valid_loss': 0.3632422731714916, 'valid_acc': 94.68164794007491}\n",
    " 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍               | 44/50 [23:07<03:08, 31.41s/it]{'epoch': 44, 'train_loss': 0.005157332198039458, 'train_acc': 99.812734082397, 'valid_loss': 0.3267382009917598, 'valid_acc': 94.23220973782772}\n",
    " 90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████             | 45/50 [23:39<02:37, 31.52s/it]{'epoch': 45, 'train_loss': 0.002259484225255568, 'train_acc': 99.90636704119851, 'valid_loss': 0.37047920886650404, 'valid_acc': 94.6067415730337}\n",
    " 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 46/50 [24:10<02:06, 31.51s/it]{'epoch': 46, 'train_loss': 0.002276028521609238, 'train_acc': 99.90636704119851, 'valid_loss': 0.36404743570004233, 'valid_acc': 94.45692883895131}\n",
    " 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 47/50 [24:42<01:34, 31.60s/it]≤{'epoch': 47, 'train_loss': 0.0032291927420735195, 'train_acc': 99.8876404494382, 'valid_loss': 0.36393148625332317, 'valid_acc': 94.6067415730337}\n",
    " 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 48/50 [25:13<01:03, 31.54s/it]{'epoch': 48, 'train_loss': 0.0038869255913623387, 'train_acc': 99.8689138576779, 'valid_loss': 0.3657738882825058, 'valid_acc': 94.30711610486891}\n",
    " 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 49/50 [25:45<00:31, 31.55s/it]{'epoch': 49, 'train_loss': 0.0013864263491641837, 'train_acc': 99.9250936329588, 'valid_loss': 0.372402172294979, 'valid_acc': 94.08239700374533}\n",
    "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [26:16<00:00, 31.54s/it]\n",
    "test acc: 94.6067415730337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
