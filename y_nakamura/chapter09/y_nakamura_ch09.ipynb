{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80. ID番号への変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from functools import reduce\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_name, file_name):\n",
    "    with open(f'{dir_name}{file_name}') as f:\n",
    "        X = list()\n",
    "        Y = list()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            splited_line = line.split('\\t')\n",
    "            X.append(splited_line[0])\n",
    "            Y.append(splited_line[1])\n",
    "        return np.asarray(X), np.asarray(Y)\n",
    "\n",
    "def save_file_npy(dir_name, file_name, x):\n",
    "    np.save(f'{dir_name}{file_name}', x)\n",
    "        \n",
    "def load_file_npy(dir_name, file_name):\n",
    "    return np.load(f'{dir_name}{file_name}')\n",
    "\n",
    "def chr2num(y):\n",
    "    converter = {'b':0, 't':1, 'e':2, 'm':3}\n",
    "    return np.asarray([converter[article_type] for article_type in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, ytrain = load_data('data/', 'train.txt')\n",
    "xvalid, yvalid = load_data('data/', 'valid.txt')\n",
    "xtest, ytest = load_data('data/', 'test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessTools:\n",
    "    def __init__(self):\n",
    "        self.word_count = defaultdict(int)       \n",
    "        self.word_transformer = defaultdict(int)\n",
    "        self.vocab_size = -1\n",
    "        \n",
    "    def tokenize(self, data):\n",
    "        return [[word for word in word_tokenize(txt)] for txt in data]\n",
    "\n",
    "    def make_word_transformar(self, train_data:list):\n",
    "        for data in train_data:\n",
    "            for word in data:\n",
    "                self.word_count[word] += 1\n",
    "        sorted_word_count = sorted(self.word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "        for idx, (word, count) in enumerate(sorted_word_count):\n",
    "            if count < 2:\n",
    "                break\n",
    "            else:\n",
    "                self.word_transformer[word] = idx + 1\n",
    "        self.vocab_size = len(self.word_transformer) + 1\n",
    "\n",
    "    def txt2ids(self, txt_list:list):\n",
    "        txt_ids = list()\n",
    "        for txt in txt_list:\n",
    "            ids = list()\n",
    "            for word in txt:\n",
    "                ids.append(self.word_transformer[word])\n",
    "            txt_ids.append(ids)\n",
    "        return txt_ids\n",
    "\n",
    "\n",
    "    def ids2vec(self, txt_ids:list):\n",
    "        txt_vec = list()\n",
    "        identity = np.identity(self.vocab_size)\n",
    "        for ids in txt_ids:\n",
    "            txt_vec.append(identity[ids])\n",
    "        return txt_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = PreprocessTools()\n",
    "xtrain, ytrain = load_data('data/', 'train.txt')\n",
    "xvalid, yvalid = load_data('data/', 'valid.txt')\n",
    "xtest, ytest = load_data('data/', 'test.txt')\n",
    "xtrain = preprocess.tokenize(xtrain)\n",
    "xvalid = preprocess.tokenize(xvalid)\n",
    "xtest = preprocess.tokenize(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.make_word_transformar(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_ids = preprocess.txt2ids(xtrain)\n",
    "xvalid_ids = preprocess.txt2ids(xvalid)\n",
    "xtest_ids = preprocess.txt2ids(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kindred', 'Healthcare', 'to', 'buy', 'Gentiva', 'for', 'about', '$', '573', 'mln'] [5065, 3395, 2, 181, 3396, 13, 164, 19, 0, 220]\n",
      "['US', 'to', 'boost', 'ground', ',', 'naval', 'forces', 'in', 'NATO', 'countries'] [15, 2, 586, 3397, 1, 0, 4067, 6, 5066, 5067]\n",
      "['Robert', 'Pattinson', '-', 'Robert', 'Pattinson', 'Brushes', 'Off', 'Kristen', 'Stewart', \"'s\", 'Cheating', '...'] [237, 945, 11, 237, 945, 0, 385, 669, 1259, 4, 3398, 3]\n",
      "['Piers', 'Morgan', 'Delivers', 'One', 'Final', 'Blow', 'To', 'Gun', 'Violence', 'In', 'Last', 'Show'] [5068, 399, 6690, 185, 1074, 5069, 16, 3399, 2225, 20, 785, 161]\n",
      "['Here', 'We', 'Go', ':', \"'Star\", 'Wars', 'Episode', 'VII', \"'\", 'Kicks', 'Off', 'Filming', 'at', 'Pinewood'] [400, 196, 639, 7, 549, 210, 295, 587, 5, 5070, 385, 1371, 22, 0]\n",
      "['Amazon', 'gets', 'in', 'the', 'game', ':', 'Retailer', 'beats', 'Google', 'to', 'buy', 'hit', 'console', 'broadcasting', '...'] [169, 330, 6, 17, 1619, 7, 0, 609, 82, 2, 181, 245, 5071, 0, 3]\n",
      "['FOREX-Euro', 'retreats', 'as', 'ECB', 'steps', 'up', 'verbal', 'campaign'] [331, 5072, 9, 52, 588, 43, 6691, 2518]\n",
      "['UPDATE', '2-US', 'appeals', 'court', 'revives', 'Apple', 'patent', 'lawsuit', 'against', 'Google'] [10, 301, 6692, 332, 0, 109, 1783, 734, 232, 82]\n",
      "['Kendall', 'Jenner', 'shows', 'support', 'for', 'Kanye', 'West', 'in', 'Yeezus', 'tour', 'shirt'] [735, 333, 204, 786, 13, 87, 69, 6, 0, 702, 5073]\n",
      "['UPDATE', '1-S.Korea', \"'s\", 'state', 'health', 'insurer', 'tobacco', 'firms', 'for', 'damages'] [10, 0, 4, 3400, 880, 6693, 3401, 946, 13, 3402]\n"
     ]
    }
   ],
   "source": [
    "for word, ids in zip(xtrain[:10], xtrain_ids[:10]):\n",
    "    print(word, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 81. RNNによる予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hotにはしない\n",
    "#xtrain_vec = preprocess.ids2vec(xtrain_ids) \n",
    "#xvalid_vec = preprocess.ids2vec(xvalid_ids)\n",
    "#xtest_vec = preprocess.ids2vec(xtest_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = chr2num(y_train)\n",
    "y_valid = chr2num(y_valid)\n",
    "y_test = chr2num(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_file_json('work/', 'train_x', xtrain_vec)\n",
    "#save_file_json('work/', 'train_y', ytrain)\n",
    "#save_file_json('work/', 'valid_x', xvalid_vec)\n",
    "#save_file_json('work/', 'valid_y', yvalid)\n",
    "#save_file_json('work/', 'test_x', xtest_vec)\n",
    "#save_file_json('work/', 'test_y', ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs: input, h_0\n",
    "\n",
    "- input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details.\n",
    "\n",
    "- h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.\n",
    "\n",
    "## Outputs: output, h_n\n",
    "\n",
    "- output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.\n",
    "\n",
    "    For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.\n",
    "\n",
    "- h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n",
    "\n",
    "    Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "dw, dh = 300, 50\n",
    "torch.manual_seed(1234)\n",
    "embed = nn.Embedding(vocab_size, dw, padding_idx=0) #idx 0 は 0埋め\n",
    "rnn = nn.RNN(dw, dh, num_layers=2, bidirectional=True, batch_first=True)\n",
    "linear = nn.Linear(50, 4, bias=True)\n",
    "softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "input = torch.LongTensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 50])\n",
      "tensor([[ 0.3656,  0.1161, -0.2724,  0.4356],\n",
      "        [ 0.1910,  0.0066, -0.2242,  0.0248]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.2179, -1.4674, -1.8559, -1.1479],\n",
      "        [-1.2056, -1.3900, -1.6208, -1.3718]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "output, hn = rnn(embed(input))\n",
    "print(hn.shape)\n",
    "x = linear(hn[-1])\n",
    "print(x)\n",
    "print(softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 82. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False, rnn_bias=True, word_vec=None):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.dw, self.dh = dw, dh\n",
    "        if word_vec:\n",
    "            self.embed = nn.from_pretrained(word_vec)\n",
    "        else:\n",
    "            self.embed = nn.Embedding(vocab_size, dw, padding_idx=0)\n",
    "            #self.embed = nn.Embedding.from_pretrained(torch.nn.init.xavier_uniform_(nn.Parameter(torch.FloatTensor(vocab_size, dw))), padding_idx=0)\n",
    "            #self.embed.weight[0] = torch.zeros(dw)\n",
    "        self.rnn = nn.RNN(dw, dh, bias=rnn_bias, num_layers=num_layers, bidirectional=bidirectional, batch_first=True, nonlinearity='tanh')\n",
    "        if bidirectional:\n",
    "            self.linear = nn.Linear(2 * dh, L, bias=True)\n",
    "        else:\n",
    "            self.linear = nn.Linear(dh, L, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # dim=-1 or 1\n",
    "        \n",
    "    '''\n",
    "    x: ids (not one hot vector)\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        _, hidden = self.rnn(x)\n",
    "        hidden = hidden.view(self.num_layers, 2 if self.bidirectional else 1, -1, self.dh)\n",
    "        last_hidden = hidden[-1]\n",
    "        if self.bidirectional:\n",
    "            x = self.linear(torch.cat([last_hidden[0], last_hidden[1]], dim=1))\n",
    "        else:\n",
    "            x = self.linear(last_hidden[0])\n",
    "        x = self.softmax(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSets(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = [torch.LongTensor(data) for data in x]\n",
    "        self.y = [torch.LongTensor([data]) for data in y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    x = [data[0] for data in batch]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = torch.LongTensor([data[1] for data in batch])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution(data_x, data_y, op, criterion, model, batch_size=1, is_train=True, use_gpu=False):\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "    ndata = len(data_x)\n",
    "    dataset = MyDataSets(data_x, data_y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    sum_loss, acc_score = 0, 0\n",
    "    for batch_x, batch_y in data_loader:\n",
    "        op.zero_grad()\n",
    "        out = model(batch_x)\n",
    "        loss = criterion(out, batch_y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        acc_score += np.sum((pred == batch_y).cpu().detach().numpy())\n",
    "    return sum_loss / ndata, acc_score / ndata * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = PreprocessTools()\n",
    "x_train, y_train = load_data('data/', 'train.txt')\n",
    "x_valid, y_valid = load_data('data/', 'valid.txt')\n",
    "x_test, y_test = load_data('data/', 'test.txt')\n",
    "x_train = preprocess.tokenize(x_train)\n",
    "x_valid = preprocess.tokenize(x_valid)\n",
    "x_test = preprocess.tokenize(x_test)\n",
    "preprocess.make_word_transformar(x_train)\n",
    "x_train = preprocess.txt2ids(x_train)\n",
    "x_valid = preprocess.txt2ids(x_valid)\n",
    "x_test = preprocess.txt2ids(x_test)\n",
    "y_train = chr2num(y_train)\n",
    "y_valid = chr2num(y_valid)\n",
    "y_test = chr2num(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = preprocess.vocab_size\n",
    "torch.manual_seed(1234)\n",
    "model = MyRNN(vocab_size, dw=300, dh=50, L=4, num_layers=1, bidirectional=False)\n",
    "ntrain = len(x_train)\n",
    "nepoch = 10 \n",
    "op = optim.Adam(model.parameters(), lr=0.001)\n",
    "#criterion = nn.NLLLoss(ignore_index=0) ignore_indexしちゃダメ...\n",
    "criterion = nn.NLLLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRNN(\n",
      "  (embed): Embedding(9866, 300, padding_idx=0)\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (linear): Linear(in_features=50, out_features=4, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23f24d7d88f4b42849375724265daa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 0.8678550714299027, 'train_acc': 69.76591760299625, 'valid_loss': 0.6977099206982257, 'valid_acc': 76.92883895131087}\n",
      "{'epoch': 1, 'train_loss': 0.5909373537211745, 'train_acc': 79.68164794007491, 'valid_loss': 0.6242007071808092, 'valid_acc': 79.70037453183521}\n",
      "{'epoch': 2, 'train_loss': 0.4399147464107955, 'train_acc': 84.49438202247191, 'valid_loss': 0.6134451217897069, 'valid_acc': 80.0}\n",
      "{'epoch': 3, 'train_loss': 0.34077258804525096, 'train_acc': 88.22097378277154, 'valid_loss': 0.6018027626990414, 'valid_acc': 81.72284644194757}\n",
      "{'epoch': 4, 'train_loss': 0.28248478134771454, 'train_acc': 90.53370786516854, 'valid_loss': 0.6021872756818932, 'valid_acc': 81.42322097378278}\n",
      "{'epoch': 5, 'train_loss': 0.24080713388487376, 'train_acc': 92.20973782771536, 'valid_loss': 0.5551284337183678, 'valid_acc': 82.24719101123596}\n",
      "{'epoch': 6, 'train_loss': 0.2059694091253388, 'train_acc': 93.27715355805243, 'valid_loss': 0.6041555308879872, 'valid_acc': 82.77153558052434}\n",
      "{'epoch': 7, 'train_loss': 0.18229074051853236, 'train_acc': 93.80149812734082, 'valid_loss': 0.6350412422096965, 'valid_acc': 81.57303370786518}\n",
      "{'epoch': 8, 'train_loss': 0.15439442698820358, 'train_acc': 94.99063670411985, 'valid_loss': 0.7252214624000799, 'valid_acc': 81.19850187265918}\n",
      "{'epoch': 9, 'train_loss': 0.14507031757982183, 'train_acc': 95.14044943820224, 'valid_loss': 0.6305916765917089, 'valid_acc': 82.62172284644194}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_writer = SummaryWriter(log_dir='./work/logs/train')\n",
    "valid_writer = SummaryWriter(log_dir='./work/logs/valid')\n",
    "logger = list()\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss, train_acc = execution(x_train, y_train, op, criterion, model, batch_size=1)\n",
    "    train_writer.add_scalar(\"loss\", train_loss, epoch) \n",
    "    train_writer.add_scalar(\"accuracy\", train_acc, epoch)\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = execution(x_valid, y_valid, op, criterion, model, batch_size=1, is_train=False)\n",
    "        valid_writer.add_scalar(\"loss\", valid_loss, epoch)\n",
    "        valid_writer.add_scalar(\"accuracy\", valid_acc, epoch)\n",
    "    logger.append({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "    print({'epoch':epoch, 'train_loss':train_loss, 'train_acc':train_acc, 'valid_loss':valid_loss, 'valid_acc':valid_acc})\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 83. ミニバッチ化・GPU上での学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 84. 単語ベクトルの導入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 85. 双方向RNN・多層化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 86. 畳み込みニューラルネットワーク (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 87. 確率的勾配降下法によるCNNの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 88. パラメータチューニング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 89. 事前学習済み言語モデルからの転移学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
