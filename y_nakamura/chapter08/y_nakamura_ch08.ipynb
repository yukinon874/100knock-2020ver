{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 70. 単語ベクトルの和による特徴量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from functools import reduce\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_name, file_name):\n",
    "    with open(f'{dir_name}{file_name}') as f:\n",
    "        X = list()\n",
    "        Y = list()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            splited_line = line.split('\\t')\n",
    "            X.append(splited_line[0])\n",
    "            Y.append(splited_line[1])\n",
    "        return np.asarray(X), np.asarray(Y)\n",
    "\n",
    "def txt2vec(x):\n",
    "    vec_x_list = list()\n",
    "    #stemmer = PorterStemmer()\n",
    "    for text in x:\n",
    "        words = word_tokenize(text)\n",
    "        #words = list(map(stemmer.stem, words)) stemmingしない方が単語ベクトルに含まれる単語数が多かった\n",
    "        words_vec = [model[word] for word in words if word in model]\n",
    "        if not words_vec:\n",
    "            print(words)\n",
    "            continue\n",
    "        vec_x_list.append(sum(words_vec) / len(words_vec))\n",
    "    return np.asarray(vec_x_list)\n",
    "\n",
    "def save_file_npy(dir_name, file_name, x):\n",
    "    np.save(f'{dir_name}{file_name}', x)\n",
    "        \n",
    "def load_file_npy(dir_name, file_name):\n",
    "    return np.load(f'{dir_name}{file_name}')\n",
    "\n",
    "def chr2num(y):\n",
    "    converter = {'b':0, 't':1, 'e':2, 'm':3}\n",
    "    return np.asarray([converter[article_type] for article_type in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = load_data('data/', 'train.txt')\n",
    "valid_x, valid_y = load_data('data/', 'valid.txt')\n",
    "test_x, test_y = load_data('data/', 'test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10680,) (10680,)\n",
      "(1335,) (1335,)\n",
      "(1335,) (1335,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)\n",
    "print(valid_x.shape, valid_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = txt2vec(train_x)\n",
    "valid_x = txt2vec(valid_x)\n",
    "test_x = txt2vec(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = chr2num(train_y)\n",
    "valid_y = chr2num(valid_y)\n",
    "test_y = chr2num(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_npy('work/', 'train_x', train_x)\n",
    "save_file_npy('work/', 'train_y', train_y)\n",
    "save_file_npy('work/', 'valid_x', valid_x)\n",
    "save_file_npy('work/', 'valid_y', valid_y)\n",
    "save_file_npy('work/', 'test_x', test_x)\n",
    "save_file_npy('work/', 'test_y', test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 71. 単層ニューラルネットワークによる予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = load_file_npy('work/', 'train_x.npy')\n",
    "train_y = load_file_npy('work/', 'train_y.npy')\n",
    "valid_x = load_file_npy('work/', 'valid_x.npy')\n",
    "valid_y = load_file_npy('work/', 'valid_y.npy')\n",
    "test_x = load_file_npy('work/', 'test_x.npy')\n",
    "test_y = load_file_npy('work/', 'test_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10680, 300) (10680,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339],\n",
      "        [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
      "        [ 0.3223, -1.2633,  0.3500,  0.3081],\n",
      "        ...,\n",
      "        [ 1.1469, -0.1733,  0.0637, -1.2699],\n",
      "        [-0.6212, -0.2381,  0.0892,  1.8008],\n",
      "        [-2.0627,  0.3222, -1.1390,  1.2418]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "W = torch.randn(300, 4)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(train_x)\n",
    "train_y = torch.from_numpy(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0073, 0.9393, 0.0504, 0.0030]])\n"
     ]
    }
   ],
   "source": [
    "print(F.softmax(torch.matmul(train_x[:1], W), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0073, 0.9393, 0.0504, 0.0030],\n",
      "        [0.2980, 0.6627, 0.0371, 0.0021],\n",
      "        [0.0568, 0.7394, 0.0224, 0.1814],\n",
      "        [0.1399, 0.5424, 0.0614, 0.2563]])\n"
     ]
    }
   ],
   "source": [
    "print(F.softmax(torch.matmul(train_x[:4], W), dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 72. 損失と勾配の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339],\n",
      "        [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
      "        [ 0.3223, -1.2633,  0.3500,  0.3081],\n",
      "        ...,\n",
      "        [ 1.1469, -0.1733,  0.0637, -1.2699],\n",
      "        [-0.6212, -0.2381,  0.0892,  1.8008],\n",
      "        [-2.0627,  0.3222, -1.1390,  1.2418]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "W = nn.Parameter(torch.randn(300, 4), requires_grad=True)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "v_train_x = train_x[:4]\n",
    "v_train_y = train_y[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0170, -0.0622, -0.0194,  ..., -0.0100,  0.1557, -0.0435],\n",
      "        [-0.0488,  0.0571,  0.1176,  ...,  0.0959,  0.1008,  0.0121],\n",
      "        [ 0.0923,  0.0072, -0.2906,  ...,  0.0138,  0.0388, -0.0379],\n",
      "        [ 0.0472, -0.0069, -0.0391,  ..., -0.0337,  0.0504, -0.1157]])\n"
     ]
    }
   ],
   "source": [
    "print(v_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_train_x = torch.matmul(v_train_x, W)\n",
    "loss = criterion(v_train_x, v_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4065, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0115,  0.0194, -0.0339,  0.0029],\n",
      "        [-0.0103, -0.0047, -0.0004,  0.0154],\n",
      "        [-0.0262, -0.0441,  0.0811, -0.0108],\n",
      "        ...,\n",
      "        [-0.0178,  0.0115,  0.0053,  0.0010],\n",
      "        [-0.0151,  0.0673, -0.0184, -0.0338],\n",
      "        [-0.0068, -0.0309,  0.0360,  0.0017]])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339],\n",
      "        [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
      "        [ 0.3223, -1.2633,  0.3500,  0.3081],\n",
      "        ...,\n",
      "        [ 1.1469, -0.1733,  0.0637, -1.2699],\n",
      "        [-0.6212, -0.2381,  0.0892,  1.8008],\n",
      "        [-2.0627,  0.3222, -1.1390,  1.2418]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "W = nn.Parameter(torch.randn(300, 4), requires_grad=True)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.matmul(train_x[:1], W)\n",
    "loss = criterion(out, train_y[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.8259, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2397e-04,  1.5989e-02,  8.5838e-04, -1.6971e-02],\n",
      "        [-4.5319e-04, -5.8451e-02, -3.1380e-03,  6.2042e-02],\n",
      "        [-1.4114e-04, -1.8203e-02, -9.7726e-04,  1.9321e-02],\n",
      "        ...,\n",
      "        [-7.3013e-05, -9.4169e-03, -5.0556e-04,  9.9955e-03],\n",
      "        [ 1.1338e-03,  1.4623e-01,  7.8504e-03, -1.5521e-01],\n",
      "        [-3.1683e-04, -4.0864e-02, -2.1938e-03,  4.3374e-02]])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 73. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = load_file_npy('work/', 'train_x.npy')\n",
    "train_y = load_file_npy('work/', 'train_y.npy')\n",
    "valid_x = load_file_npy('work/', 'valid_x.npy')\n",
    "valid_y = load_file_npy('work/', 'valid_y.npy')\n",
    "test_x = load_file_npy('work/', 'test_x.npy')\n",
    "test_y = load_file_npy('work/', 'test_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339],\n",
      "        [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
      "        [ 0.3223, -1.2633,  0.3500,  0.3081],\n",
      "        ...,\n",
      "        [ 1.1469, -0.1733,  0.0637, -1.2699],\n",
      "        [-0.6212, -0.2381,  0.0892,  1.8008],\n",
      "        [-2.0627,  0.3222, -1.1390,  1.2418]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "W = nn.Parameter(torch.randn(300, 4), requires_grad=True)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_x, train_y, op, criterion, batch_size=4, nepoch=100):\n",
    "    ntrain = len(train_x)\n",
    "    loss_list = list()\n",
    "    for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "        sum_loss = 0\n",
    "        perm = np.random.permutation(ntrain)\n",
    "        for i in range(0, ntrain, batch_size):\n",
    "            batch_x = torch.from_numpy(train_x[perm[i:i + batch_size]])\n",
    "            batch_y = torch.from_numpy(train_y[perm[i:i + batch_size]])\n",
    "            #batch_x, batch_y = Variable(batch_x), Variable(batch_y)\n",
    "            batch_x = torch.matmul(batch_x, W)\n",
    "            loss = criterion(batch_x, batch_y)\n",
    "            op.zero_grad()\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "            sum_loss += loss.data.item() * len(batch_x)\n",
    "        loss_list.append(sum_loss / ntrain)\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e957d01979724912a25ccb8d6693ac5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ntrain = len(train_x)\n",
    "nepoch = 100\n",
    "op = optim.SGD([W], lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "result = train(train_x, train_y, op, criterion, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-2.0231, -0.1074,  0.4542, -1.2848],\n",
      "        [ 0.4140,  1.0014, -0.1300, -2.1738],\n",
      "        [ 0.8591,  0.7397, -2.5430,  0.6604],\n",
      "        ...,\n",
      "        [ 0.3355, -0.0249, -0.7943,  0.2516],\n",
      "        [ 1.9430, -1.6062, -0.3233,  1.0196],\n",
      "        [-2.2287, -1.0866, -0.4814,  2.1579]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28822114222264317, 0.2729065997679739, 0.26329753994712024, 0.25645157239017474, 0.25113904970215967, 0.24712463183076697, 0.24390258440257187, 0.24111985022297414, 0.2389436593737681, 0.23713426657275935]\n"
     ]
    }
   ],
   "source": [
    "print(result[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 74. 正解率の計測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc=0.9071161048689138\n"
     ]
    }
   ],
   "source": [
    "pred_y = torch.argmax(F.softmax(torch.matmul(torch.from_numpy(test_x), W), dim=-1), dim=1)\n",
    "print(f'test_acc={accuracy_score(test_y, pred_y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.9195692883895131\n"
     ]
    }
   ],
   "source": [
    "pred_y = torch.argmax(F.softmax(torch.matmul(torch.from_numpy(train_x), W), dim=-1), dim=1)\n",
    "print(f'train_loss={accuracy_score(train_y, pred_y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 75. 損失と正解率のプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution(data_x, data_y, op, criterion, batch_size=4, nepoch=100, train=True):\n",
    "    ndata = len(data_x)\n",
    "    perm = np.random.permutation(ndata)\n",
    "    sum_loss = 0\n",
    "    for i in range(0, ndata, batch_size):\n",
    "        op.zero_grad()\n",
    "        batch_x = torch.from_numpy(data_x[perm[i:i + batch_size]])\n",
    "        batch_y = torch.from_numpy(data_y[perm[i:i + batch_size]])\n",
    "        #batch_x, batch_y = Variable(batch_x), Variable(batch_y)\n",
    "        if train:\n",
    "            out = torch.matmul(batch_x, W)\n",
    "            loss = criterion(out, batch_y)\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = torch.matmul(batch_x, W)\n",
    "                loss = criterion(out, batch_y)\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "    return sum_loss / ndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339],\n",
      "        [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
      "        [ 0.3223, -1.2633,  0.3500,  0.3081],\n",
      "        ...,\n",
      "        [ 1.1469, -0.1733,  0.0637, -1.2699],\n",
      "        [-0.6212, -0.2381,  0.0892,  1.8008],\n",
      "        [-2.0627,  0.3222, -1.1390,  1.2418]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "W = nn.Parameter(torch.randn(300, 4), requires_grad=True)\n",
    "print(W)\n",
    "ntrain = len(train_x)\n",
    "nepoch = 100\n",
    "op = optim.SGD([W], lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e190047edf4428ba56a291183c8ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "writer = SummaryWriter(log_dir='./work/logs')\n",
    "for epoch in tqdm.notebook.tqdm(range(nepoch)):\n",
    "    train_loss = execution(train_x, train_y, op, criterion, batch_size=1)\n",
    "    writer.add_scalar(\"train_loss\", train_loss, epoch) \n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    valid_loss = execution(valid_x, valid_y, op, criterion, batch_size=1, nepoch=100, train=False)\n",
    "    writer.add_scalar(\"valid_loss\", valid_loss, epoch)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_y = torch.argmax(F.softmax(torch.matmul(torch.from_numpy(train_x), W), dim=-1), dim=1)\n",
    "        train_acc = accuracy_score(train_y, pred_y)\n",
    "        writer.add_scalar(\"train_acc_score\", train_acc, epoch)\n",
    "\n",
    "        pred_y = torch.argmax(F.softmax(torch.matmul(torch.from_numpy(valid_x), W), dim=-1), dim=1)\n",
    "        valid_acc = accuracy_score(valid_y, pred_y)\n",
    "        writer.add_scalar(\"valid_acc_score\", valid_acc, epoch)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5028027535337544, 0.4530537145898094, 0.4212120260656263, 0.39885072301978136, 0.3817836359072561, 0.36883020925704163, 0.35804143832598584, 0.3490760947505988, 0.3414108556378632, 0.3348980882975398, 0.32918650068763544, 0.3240989345345004, 0.3195082837662921, 0.3153567257739548, 0.3116891590673217, 0.3080970557812238, 0.3048600675543025, 0.302021585000974, 0.2993079356373766, 0.2968831482188698]\n",
      "[0.45987449703108707, 0.4238755054391452, 0.3987418100730527, 0.3810014982517295, 0.3682995367835792, 0.3582362641225979, 0.34990171425655586, 0.3433129606341169, 0.33751936203865257, 0.3334261171332633, 0.3291596764830163, 0.32494030997966145, 0.32244329215217704, 0.31896338794413254, 0.3166717988324684, 0.3146938183171857, 0.312967406040082, 0.3105635773455405, 0.30909733356401886, 0.30747632840379246]\n"
     ]
    }
   ],
   "source": [
    "print(train_loss_list)\n",
    "print(valid_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 76. チェックポイント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 77. ミニバッチ化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_x, train_y, batch_size=4, nepoch=100, op, criterion):\n",
    "    ntrain = len(train_x)\n",
    "    perm = np.random.permutation(ntrain)\n",
    "    loss_list = list()\n",
    "    for epoch in tqdm(range(1, nepoch + 1)):\n",
    "        sum_loss = 0\n",
    "        for i in range(0, ntrain, batch_size):\n",
    "            batch_x = torch.from_numpy(train_x[perm[i:i + batch_size]])\n",
    "            batch_y = torch.from_numpy(train_y[perm[i:i + batch_size]])\n",
    "            batch_x, batch_y = Variable(batch_x), Variable(batch_y)\n",
    "            pred = F.softmax(torch.matmul(batch_x, W), dim=-1)\n",
    "            loss = criterion(pred, batch_y.type(torch.long))\n",
    "            op.zero_grad()\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "            sum_loss += loss.data.item() * len(batch_x)\n",
    "        loss_list.append(sum_loss / ntrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 78. GPU上での学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 79. 多層ニューラルネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
