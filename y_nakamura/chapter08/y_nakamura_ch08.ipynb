{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 70. 単語ベクトルの和による特徴量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 後でワンホットベクトルに書き直そうね"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_name, file_name):\n",
    "    with open(f'{dir_name}{file_name}') as f:\n",
    "        X = list()\n",
    "        Y = list()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            splited_line = line.split('\\t')\n",
    "            X.append(splited_line[0])\n",
    "            Y.append(splited_line[1])\n",
    "        return np.asarray(X), np.asarray(Y)\n",
    "\n",
    "def txt2vec(x):\n",
    "    vec_x_list = list()\n",
    "    for txt in x:\n",
    "        splited_txt = re.split(r'[.,: \\']', txt)\n",
    "        vec_x = np.zeros(300)\n",
    "        word_count = 0\n",
    "        for word in splited_txt:\n",
    "            word = word.strip()\n",
    "            try:\n",
    "                vec_x += model[word]\n",
    "                word_count += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if word_count == 0:\n",
    "            print(txt)\n",
    "            continue\n",
    "        vec_x /= word_count\n",
    "        vec_x_list.append(vec_x)\n",
    "    return np.asarray(vec_x_list)\n",
    "\n",
    "def save_file_npy(dir_name, file_name, x):\n",
    "    np.save(f'{dir_name}{file_name}', x)\n",
    "        \n",
    "def load_file_npy(dir_name, file_name):\n",
    "    return np.load(f'{dir_name}{file_name}').astype(np.float32)\n",
    "\n",
    "def chr2num(y):\n",
    "    converter = dict()\n",
    "    converter['b'] = 0\n",
    "    converter['t'] = 1\n",
    "    converter['e'] = 2\n",
    "    converter['m'] = 3\n",
    "    return np.asarray([converter[article_type] for article_type in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = load_data('data/', 'train.txt')\n",
    "valid_x, valid_y = load_data('data/', 'valid.txt')\n",
    "test_x, test_y = load_data('data/', 'test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10680,) (10680,)\n",
      "(1335,) (1335,)\n",
      "(1335,) (1335,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)\n",
    "print(valid_x.shape, valid_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = txt2vec(train_x)\n",
    "valid_x = txt2vec(valid_x)\n",
    "test_x = txt2vec(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = chr2num(train_y)\n",
    "valid_y = chr2num(valid_y)\n",
    "test_y = chr2num(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_npy('work/', 'train_x', train_x)\n",
    "save_file_npy('work/', 'train_y', train_y)\n",
    "save_file_npy('work/', 'valid_x', valid_x)\n",
    "save_file_npy('work/', 'valid_y', valid_y)\n",
    "save_file_npy('work/', 'test_x', test_x)\n",
    "save_file_npy('work/', 'test_y', test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 71. 単層ニューラルネットワークによる予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = load_file_npy('work/', 'train_x.npy')\n",
    "train_y = load_file_npy('work/', 'train_y.npy')\n",
    "valid_x = load_file_npy('work/', 'valid_x.npy')\n",
    "valid_y = load_file_npy('work/', 'valid_y.npy')\n",
    "test_x = load_file_npy('work/', 'test_x.npy')\n",
    "test_y = load_file_npy('work/', 'test_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10679, 300) (10680,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339],\n",
      "        [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
      "        [ 0.3223, -1.2633,  0.3500,  0.3081],\n",
      "        ...,\n",
      "        [ 1.1469, -0.1733,  0.0637, -1.2699],\n",
      "        [-0.6212, -0.2381,  0.0892,  1.8008],\n",
      "        [-2.0627,  0.3222, -1.1390,  1.2418]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "W = Variable(torch.randn(300, 4), requires_grad = True)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(train_x)\n",
    "train_y = torch.from_numpy(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0044, 0.9502, 0.0428, 0.0026]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.0044, 0.9502, 0.0428, 0.0026],\n",
      "        [0.2980, 0.6627, 0.0371, 0.0021],\n",
      "        [0.0586, 0.7506, 0.0133, 0.1775],\n",
      "        [0.1399, 0.5424, 0.0614, 0.2563]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(F.softmax(torch.matmul(train_x[:1], W), dim = 1))\n",
    "print(F.softmax(torch.matmul(train_x[:4], W), dim = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 72. 損失と勾配の計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://37ma5ras.blogspot.com/2017/09/pytorch-02.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "v_train_x = Variable(train_x[:4])\n",
    "v_train_y = Variable(train_y[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0044, 0.9502, 0.0428, 0.0026],\n",
      "        [0.2980, 0.6627, 0.0371, 0.0021],\n",
      "        [0.0586, 0.7506, 0.0133, 0.1775],\n",
      "        [0.1399, 0.5424, 0.0614, 0.2563]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "v_train_x = torch.matmul(v_train_x, W)\n",
    "pred = F.softmax(v_train_x, dim = -1)\n",
    "print(pred)\n",
    "loss = criterion(pred, v_train_y.type(torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5911, grad_fn=<NllLossBackward>)\n",
      "tensor([[ 2.6124e-03, -1.0254e-03, -1.1798e-03, -4.0727e-04],\n",
      "        [-3.2793e-03,  2.9144e-03,  3.3780e-04,  2.7053e-05],\n",
      "        [-6.1659e-03,  2.7133e-03,  1.9983e-03,  1.4543e-03],\n",
      "        ...,\n",
      "        [-5.5620e-03,  5.0955e-03,  6.1436e-04, -1.4785e-04],\n",
      "        [-5.9280e-03,  7.3155e-03, -1.1759e-03, -2.1165e-04],\n",
      "        [-5.7692e-04, -1.6045e-03,  2.0903e-03,  9.1109e-05]])\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "loss.backward()\n",
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.])\n",
      "tensor([[0.0044, 0.9502, 0.0428, 0.0026]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(train_y[:1])\n",
    "print(F.softmax(torch.matmul(train_x[:1], W), dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0044, 0.9502, 0.0428, 0.0026]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "pred = F.softmax(torch.matmul(train_x[:1], W), dim = -1)\n",
    "print(pred)\n",
    "loss = criterion(pred, train_y[:1].type(torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7267, grad_fn=<NllLossBackward>)\n",
      "tensor([[ 0.0026, -0.0010, -0.0012, -0.0004],\n",
      "        [-0.0032,  0.0021,  0.0009,  0.0002],\n",
      "        [-0.0061,  0.0022,  0.0023,  0.0016],\n",
      "        ...,\n",
      "        [-0.0056,  0.0050,  0.0007, -0.0001],\n",
      "        [-0.0061,  0.0096, -0.0028, -0.0007],\n",
      "        [-0.0005, -0.0020,  0.0024,  0.0002]])\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "loss.backward()\n",
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 73. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メモ\n",
    "```zsh\n",
    "jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "```\n",
    "を実行しようとすると`RuntimeError: npm dependencies failed to install`が出る\n",
    "```zsh\n",
    "conda install nodejs\n",
    "```\n",
    "でnodejsを再インストールすることで解決"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = load_file_npy('work/', 'train_x.npy')\n",
    "train_y = load_file_npy('work/', 'train_y.npy')\n",
    "valid_x = load_file_npy('work/', 'valid_x.npy')\n",
    "valid_y = load_file_npy('work/', 'valid_y.npy')\n",
    "test_x = load_file_npy('work/', 'test_x.npy')\n",
    "test_y = load_file_npy('work/', 'test_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339],\n",
      "        [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
      "        [ 0.3223, -1.2633,  0.3500,  0.3081],\n",
      "        ...,\n",
      "        [ 1.1469, -0.1733,  0.0637, -1.2699],\n",
      "        [-0.6212, -0.2381,  0.0892,  1.8008],\n",
      "        [-2.0627,  0.3222, -1.1390,  1.2418]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "W = Variable(torch.randn(300, 4), requires_grad = True)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "ntrain = len(train_x)\n",
    "#batch_size = 4\n",
    "nepoch = 100\n",
    "op = optim.SGD([W], lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216e65f8ab0b4fea8e1d9edf9e3fa362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_list = list()\n",
    "for epoch in tqdm(range(1, nepoch + 1)):\n",
    "    sum_loss = 0\n",
    "    for i in range(ntrain):\n",
    "        batch_x = torch.from_numpy(train_x[i:i+1])\n",
    "        batch_y = torch.from_numpy(train_y[i:i+1])\n",
    "        batch_x, batch_y = Variable(batch_x), Variable(batch_y)\n",
    "        pred = F.softmax(torch.matmul(batch_x, W), dim = -1)\n",
    "        loss = criterion(pred, batch_y.type(torch.long))\n",
    "        op.zero_grad()\n",
    "        loss.backward()\n",
    "        op.step()\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "    loss_list.append(sum_loss / ntrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0375, -0.7751,  0.5083, -0.6570],\n",
      "        [ 0.8165,  0.5626, -0.1780, -2.0879],\n",
      "        [ 0.1719, -0.3804, -0.9507,  0.8744],\n",
      "        ...,\n",
      "        [ 1.9262, -0.2389, -1.1102, -0.8110],\n",
      "        [ 0.5591, -1.5014, -0.2886,  2.2653],\n",
      "        [-2.6840,  0.1174, -0.7555,  1.6850]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2908205520905804, 1.0574674301602867, 1.0079448441050471, 0.9916939019152288, 0.9832527568277795, 0.9778966107060401, 0.9741127230310708, 0.9712556996986215, 0.9689983874001307, 0.9671549549486753, 0.965610970149326, 0.9642915642440096, 0.9631454401192594, 0.9621360724505861, 0.9612366645244623, 0.9604269673259517, 0.959691395122237, 0.9590176842297508, 0.9583959663796068, 0.9578181952517131, 0.9572776550880532, 0.9567686780896526, 0.9562863448195243, 0.9558263134029921, 0.9553846937440308, 0.9549579104066788, 0.9545426289352138, 0.9541356343566701, 0.9537337906798173, 0.9533339049876406, 0.9529326059175341, 0.9525261842356193, 0.9521102910798587, 0.9516796168316616, 0.9512274755688196, 0.9507452228765809, 0.9502215495493528, 0.9496416275532504, 0.948986126856411, 0.9482306069202637, 0.9473461892553483, 0.9463031243295705, 0.9450781335712373, 0.9436637462748124, 0.9420749689755815, 0.9403491972035236, 0.9385403562239494, 0.9367079648185759, 0.9349018144361981, 0.933153022948499, 0.931476964039749, 0.929879792715503, 0.9283625833159975, 0.9269236521551225, 0.9255597019798301, 0.9242667606037654, 0.9230406003665835, 0.92187711247679, 0.9207723583788907, 0.9197227065427026, 0.9187247263581565, 0.9177751984712337, 0.9168709928400061, 0.9160090616914663, 0.915186352977592, 0.914399789815092, 0.9136462970898392, 0.9129228237528033, 0.9122263139162617, 0.9115537145443623, 0.9109019243594413, 0.91026777107729, 0.9096478892343768, 0.909038538488556, 0.9084354093402959, 0.9078332786368074, 0.9072255341691917, 0.9066033259536443, 0.9059543081716205, 0.9052605597602295, 0.904495297260722, 0.9036180507433549, 0.9025696630558271, 0.901273223188486, 0.8996572758039731, 0.8977213491772891, 0.8956126098719875, 0.8935670098115442, 0.8917223416631588, 0.8900728657339396, 0.888560542229856, 0.8871397556716136, 0.8857876298858431, 0.8844952041457654, 0.8832597265138609, 0.8820806993248311, 0.880958063041003, 0.8798912278498603, 0.8788786413741022, 0.8779177693559436]\n"
     ]
    }
   ],
   "source": [
    "print(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 74. 正解率の計測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8771535580524344"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = torch.argmax(F.softmax(torch.matmul(torch.from_numpy(test_x), W), dim = -1), dim = 1)\n",
    "accuracy_score(test_y, pred_y.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8696629213483146"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = torch.argmax(F.softmax(torch.matmul(torch.from_numpy(valid_x), W), dim = -1), dim = 1)\n",
    "accuracy_score(valid_y, pred_y.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8794943820224719"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = torch.argmax(F.softmax(torch.matmul(torch.from_numpy(train_x), W), dim = -1), dim = 1)\n",
    "accuracy_score(train_y, pred_y.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16310861423220974"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempW = Variable(torch.randn(300, 4), requires_grad = True)\n",
    "pred_y = torch.argmax(F.softmax(torch.matmul(torch.from_numpy(train_x), tempW), dim = -1), dim = 1)\n",
    "accuracy_score(train_y, pred_y.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 75. 損失と正解率のプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution(data_x, data_y, op, criterion, batch_size = 4, nepoch = 100, training_flag = True):\n",
    "    ntrain = len(data_x)\n",
    "    perm = np.random.permutation(ntrain)\n",
    "    sum_loss = 0\n",
    "    for i in range(0, ntrain, batch_size):\n",
    "        batch_x = torch.from_numpy(data_x[perm[i:i + batch_size]])\n",
    "        batch_y = torch.from_numpy(data_y[perm[i:i + batch_size]])\n",
    "        batch_x, batch_y = Variable(batch_x), Variable(batch_y)\n",
    "        pred = F.softmax(torch.matmul(batch_x, W), dim = -1)\n",
    "        loss = criterion(pred, batch_y.type(torch.long))\n",
    "        if training_flag:\n",
    "            op.zero_grad()\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "        sum_loss += loss.data.item() * len(batch_x)\n",
    "    return sum_loss / ntrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082d3817f92c42c4801a9f385bd0fb10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-22f2974c9fc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-172-b4ab77c15375>\u001b[0m in \u001b[0;36mexecution\u001b[0;34m(data_x, data_y, op, criterion, batch_size, nepoch, training_flag)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtraining_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0msum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "writer = SummaryWriter(log_dir='./work/logs')\n",
    "\n",
    "for epoch in tqdm(range(1, nepoch + 1)):\n",
    "    train_loss = execution(train_x, train_y, op, criterion)\n",
    "    writer.add_scalar(\"train_loss\", train_loss, i)\n",
    "    valid_loss = execution(valid_x, valid_y, op, criterion, training_flag = False)\n",
    "    writer.add_scalar(\"valid_loss\", valid_loss, i)\n",
    "    #print(f'train_loss:{train_loss}\\nvalid_loss:{valid_loss}')\n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 76. チェックポイント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 77. ミニバッチ化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_x, train_y, batch_size = 4, nepoch = 100, op, criterion):\n",
    "    ntrain = len(train_x)\n",
    "    perm = np.random.permutation(ntrain)\n",
    "    loss_list = list()\n",
    "    for epoch in tqdm(range(1, nepoch + 1)):\n",
    "        sum_loss = 0\n",
    "        for i in range(0, ntrain, batch_size):\n",
    "            batch_x = torch.from_numpy(train_x[perm[i:i + batch_size]])\n",
    "            batch_y = torch.from_numpy(train_y[perm[i:i + batch_size]])\n",
    "            batch_x, batch_y = Variable(batch_x), Variable(batch_y)\n",
    "            pred = F.softmax(torch.matmul(batch_x, W), dim = -1)\n",
    "            loss = criterion(pred, batch_y.type(torch.long))\n",
    "            op.zero_grad()\n",
    "            loss.backward()\n",
    "            op.step()\n",
    "            sum_loss += loss.data.item() * len(batch_x)\n",
    "        loss_list.append(sum_loss / ntrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 78. GPU上での学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 79. 多層ニューラルネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
